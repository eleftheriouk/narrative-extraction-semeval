{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0d8ce9-d6d9-4e03-82e8-36fe1c5a343a",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa2ebd-27af-46e1-aa7c-d2549cfc988a",
   "metadata": {},
   "source": [
    "## 1. Multi-head per narrative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67788-7872-4608-ba0e-01fd263958cf",
   "metadata": {},
   "source": [
    "### 1.1 Loading pre-saved variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe27a15d-ab42-4768-a155-a65353545112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset.pkl'), 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e056632b-56d7-47ae-929b-cdda00d86a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11550c30-6e80-4c4e-b262-ecec4d942892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Discrediting Ukraine': ['Discrediting Ukrainian nation and society',\n",
       "  'Ukraine is associated with nazism',\n",
       "  'Ukraine is a hub for criminal activities',\n",
       "  'Other',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Discrediting Ukrainian military',\n",
       "  'Rewriting Ukraine’s history',\n",
       "  'Ukraine is a puppet of the West',\n",
       "  'Discrediting Ukrainian government and officials and policies'],\n",
       " 'Discrediting the West, Diplomacy': ['The West is overreacting',\n",
       "  'The West does not care about Ukraine, only about its interests',\n",
       "  'The EU is divided',\n",
       "  'Other',\n",
       "  'Diplomacy does/will not work',\n",
       "  'The West is weak',\n",
       "  'West is tired of Ukraine'],\n",
       " 'Praise of Russia': ['Praise of Russian military might',\n",
       "  'Russia is a guarantor of peace and prosperity',\n",
       "  'Russian invasion has strong national support',\n",
       "  'Other',\n",
       "  'Russia has international support from a number of countries and people',\n",
       "  'Praise of Russian President Vladimir Putin'],\n",
       " 'Russia is the Victim': ['Russia actions in Ukraine are only self-defence',\n",
       "  'Other',\n",
       "  'UA is anti-RU extremists',\n",
       "  'The West is russophobic'],\n",
       " 'Distrust towards Media': ['Western media is an instrument of propaganda',\n",
       "  'Other',\n",
       "  'Ukrainian media cannot be trusted'],\n",
       " 'Amplifying war-related fears': ['Russia will also attack other countries',\n",
       "  'By continuing the war we risk WWIII',\n",
       "  'There is a real possibility that nuclear weapons will be employed',\n",
       "  'Other',\n",
       "  'NATO should/will directly intervene'],\n",
       " 'Overpraising the West': ['NATO will destroy Russia',\n",
       "  'Other',\n",
       "  'The West has the strongest international support',\n",
       "  'The West belongs in the right side of history'],\n",
       " 'Blaming the war on others rather than the invader': ['Ukraine is the aggressor',\n",
       "  'Other',\n",
       "  'The West are the aggressors'],\n",
       " 'Speculating war outcomes': ['Other',\n",
       "  'Russian army is collapsing',\n",
       "  'Russian army will lose all the occupied territories',\n",
       "  'Ukrainian army is collapsing'],\n",
       " 'Hidden plots by secret schemes of powerful groups': ['Blaming global elites',\n",
       "  'Other',\n",
       "  'Climate agenda has hidden motives'],\n",
       " 'Negative Consequences for the West': ['Sanctions imposed by Western countries will backfire',\n",
       "  'Other',\n",
       "  'The conflict will increase the Ukrainian refugee flows to Europe'],\n",
       " 'Amplifying Climate Fears': ['Whatever we do it is already too late',\n",
       "  'Earth will be uninhabitable soon',\n",
       "  'Doomsday scenarios for humans',\n",
       "  'Amplifying existing fears of global warming',\n",
       "  'Other'],\n",
       " 'Other': ['Other'],\n",
       " 'Criticism of institutions and authorities': ['Criticism of international entities',\n",
       "  'Other',\n",
       "  'Criticism of the EU',\n",
       "  'Criticism of political organizations and figures',\n",
       "  'Criticism of national governments'],\n",
       " 'Criticism of climate movement': ['Climate movement is alarmist',\n",
       "  'Climate movement is corrupt',\n",
       "  'Other',\n",
       "  'Ad hominem attacks on key activists'],\n",
       " 'Downplaying climate change': ['Ice is not melting',\n",
       "  'Temperature increase does not have significant impact',\n",
       "  'Climate cycles are natural',\n",
       "  'Other',\n",
       "  'Humans and nature will adapt to the changes',\n",
       "  'Human activities do not impact climate change',\n",
       "  'CO2 concentrations are too small to have an impact',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Sea levels are not rising'],\n",
       " 'Criticism of climate policies': ['Other',\n",
       "  'Climate policies are only for profit',\n",
       "  'Climate policies have negative impact on the economy',\n",
       "  'Climate policies are ineffective'],\n",
       " 'Questioning the measurements and science': ['Scientific community is unreliable',\n",
       "  'Data shows no temperature increase',\n",
       "  'Other',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Methodologies/metrics used are unreliable/faulty'],\n",
       " 'Climate change is beneficial': ['Other',\n",
       "  'Temperature increase is beneficial',\n",
       "  'CO2 is beneficial'],\n",
       " 'Controversy about green technologies': ['Other',\n",
       "  'Renewable energy is costly',\n",
       "  'Renewable energy is dangerous',\n",
       "  'Renewable energy is unreliable'],\n",
       " 'Green policies are geopolitical instruments': ['Green activities are a form of neo-colonialism',\n",
       "  'Climate-related international relations are abusive/exploitative',\n",
       "  'Other']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6912756b-47f2-4a11-b782-71f86ee25137",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082080ec-bfbf-4db2-8363-2b06413a3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/all_embeddings.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "all_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3784e6e1-4100-47d5-9df1-bc1f2c2d8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def stratified_train_val_split_with_embeddings(data, embeddings, labels_column, train_size=0.8, splits=5, shuffle=True, min_instances=2):\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.arange(len(data))\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "        embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    labels = np.array(data[labels_column].tolist())\n",
    "    rare_indices = []\n",
    "    common_indices = []\n",
    "\n",
    "    class_counts = labels.sum(axis=0)\n",
    "    rare_classes = np.where(class_counts <= min_instances)[0]\n",
    "\n",
    "    for idx, label_row in enumerate(labels):\n",
    "        if any(label_row[rare_classes]):\n",
    "            rare_indices.append(idx)\n",
    "        else:\n",
    "            common_indices.append(idx)\n",
    "\n",
    "    rare_data = data.iloc[rare_indices]\n",
    "    rare_labels = labels[rare_indices]\n",
    "    rare_embeddings = embeddings[rare_indices]\n",
    "\n",
    "    train_rare = rare_data.iloc[:len(rare_data) // 2].reset_index(drop=True)\n",
    "    val_rare = rare_data.iloc[len(rare_data) // 2:].reset_index(drop=True)\n",
    "\n",
    "    train_rare_embeddings = rare_embeddings[:len(rare_data) // 2]\n",
    "    val_rare_embeddings = rare_embeddings[len(rare_data) // 2:]\n",
    "\n",
    "    common_data = data.iloc[common_indices].reset_index(drop=True)\n",
    "    common_labels = labels[common_indices]\n",
    "    common_embeddings = embeddings[common_indices]\n",
    "\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=splits)\n",
    "    for train_idx, val_idx in mskf.split(np.zeros(len(common_labels)), common_labels):\n",
    "        train_common = common_data.iloc[train_idx]\n",
    "        val_common = common_data.iloc[val_idx]\n",
    "        train_common_embeddings = common_embeddings[train_idx]\n",
    "        val_common_embeddings = common_embeddings[val_idx]\n",
    "        break\n",
    "\n",
    "    train_data = pd.concat([train_rare, train_common]).reset_index(drop=True)\n",
    "    val_data = pd.concat([val_rare, val_common]).reset_index(drop=True)\n",
    "\n",
    "    train_embeddings = np.concatenate([train_rare_embeddings, train_common_embeddings], axis=0)\n",
    "    val_embeddings = np.concatenate([val_rare_embeddings, val_common_embeddings], axis=0)\n",
    "\n",
    "    return (train_data, train_embeddings), (val_data, val_embeddings)\n",
    "\n",
    "(dataset_train, train_embeddings), (dataset_val, val_embeddings) = stratified_train_val_split_with_embeddings(\n",
    "    dataset,\n",
    "    all_embeddings,\n",
    "    labels_column=\"subnarratives_encoded\",\n",
    "    min_instances=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad404584-76b4-4922-9a08-585d02b3a0fa",
   "metadata": {},
   "source": [
    "### 1.2 Remapping our subnarrative indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae17fbfc-15ac-421b-ad18-2d010c07bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                              ...                        \n",
       "1694    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1695    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "1696    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "1697    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1698    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: subnarratives_encoded, Length: 1699, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['subnarratives_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3d9def-3cf1-43e4-8796-aa0f5c1bc415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: [22, 66, 64, 33, 50, 21, 39, 65, 20], 9: [58, 56, 53, 33, 19, 60, 71], 17: [35, 42, 46, 33, 41, 34], 19: [40, 33, 63, 59], 10: [72, 33, 69], 1: [43, 3, 62, 33, 31], 16: [32, 33, 57, 55], 2: [67, 33, 54], 20: [33, 44, 45, 68], 13: [2, 33, 6], 14: [47, 33, 61], 0: [73, 24, 23, 1, 33], 15: [33], 7: [14, 33, 17, 16, 15], 5: [8, 9, 33, 0], 11: [29, 51, 7, 33, 28, 27, 4, 70, 49], 6: [33, 11, 12, 10], 18: [48, 18, 33, 26, 30], 3: [33, 52, 5], 4: [33, 36, 37, 38], 12: [25, 13, 33]}\n"
     ]
    }
   ],
   "source": [
    "narrative_to_sub_map = {}\n",
    "narrative_classes = list(mlb_narratives.classes_)\n",
    "subnarrative_classes = list(mlb_subnarratives.classes_)\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    narrative_idx = narrative_classes.index(narrative)\n",
    "    subnarrative_indices = [subnarrative_classes.index(sub) for sub in subnarratives]\n",
    "    narrative_to_sub_map[narrative_idx] = subnarrative_indices\n",
    "\n",
    "print(narrative_to_sub_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0839cb3-e83e-4702-8e67-16ee7a7de3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: [22, 66, 64, 33, 50, 21, 39, 65, 20],\n",
       " 9: [58, 56, 53, 33, 19, 60, 71],\n",
       " 17: [35, 42, 46, 33, 41, 34],\n",
       " 19: [40, 33, 63, 59],\n",
       " 10: [72, 33, 69],\n",
       " 1: [43, 3, 62, 33, 31],\n",
       " 16: [32, 33, 57, 55],\n",
       " 2: [67, 33, 54],\n",
       " 20: [33, 44, 45, 68],\n",
       " 13: [2, 33, 6],\n",
       " 14: [47, 33, 61],\n",
       " 0: [73, 24, 23, 1, 33],\n",
       " 15: [33],\n",
       " 7: [14, 33, 17, 16, 15],\n",
       " 5: [8, 9, 33, 0],\n",
       " 11: [29, 51, 7, 33, 28, 27, 4, 70, 49],\n",
       " 6: [33, 11, 12, 10],\n",
       " 18: [48, 18, 33, 26, 30],\n",
       " 3: [33, 52, 5],\n",
       " 4: [33, 36, 37, 38],\n",
       " 12: [25, 13, 33]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_sub_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7878d2ae-83ed-4153-8697-b34f22fb60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_subnarratives(row, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row and encodes the current subnarrative list to the associated hierarchy based on the narr-subnar map\"\"\"\n",
    "    for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "        sub_labels = [row['subnarratives_encoded'][sub_idx] for sub_idx in sub_indices]\n",
    "        col_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "        row[col_name] = sub_labels\n",
    "    return row\n",
    "\n",
    "dataset_train_cpy = dataset_train.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49192dc3-7a83-422a-85ea-dc462ebc8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_cpy = dataset_val.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe99263-8222-40b9-a76c-e4781ffe0e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>narrative_hierarchy_8</th>\n",
       "      <th>narrative_hierarchy_9</th>\n",
       "      <th>narrative_hierarchy_17</th>\n",
       "      <th>...</th>\n",
       "      <th>narrative_hierarchy_0</th>\n",
       "      <th>narrative_hierarchy_15</th>\n",
       "      <th>narrative_hierarchy_7</th>\n",
       "      <th>narrative_hierarchy_5</th>\n",
       "      <th>narrative_hierarchy_11</th>\n",
       "      <th>narrative_hierarchy_6</th>\n",
       "      <th>narrative_hierarchy_18</th>\n",
       "      <th>narrative_hierarchy_3</th>\n",
       "      <th>narrative_hierarchy_4</th>\n",
       "      <th>narrative_hierarchy_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_200022.txt</td>\n",
       "      <td>&lt;PARA&gt;Denmark to Punish Farmers for cow ‘emiss...</td>\n",
       "      <td>[Criticism of institutions and authorities, Cr...</td>\n",
       "      <td>[Criticism of national governments, Other, Met...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 1, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_200221.txt</td>\n",
       "      <td>&lt;PARA&gt;“the hour of decision”&lt;/PARA&gt;\\n\\nshortly...</td>\n",
       "      <td>[Hidden plots by secret schemes of powerful gr...</td>\n",
       "      <td>[Blaming global elites, Other, Other, CO2 conc...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 1, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 1]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_200110.txt</td>\n",
       "      <td>&lt;PARA&gt;if democrats successfully ban gas stoves...</td>\n",
       "      <td>[Criticism of climate policies, Criticism of i...</td>\n",
       "      <td>[Climate policies have negative impact on the ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 1, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HI</td>\n",
       "      <td>HI_173.txt</td>\n",
       "      <td>&lt;PARA&gt;यूक्रेन के बढ़ते हमलों के बीच रूस ने क्य...</td>\n",
       "      <td>[Praise of Russia, Russia is the Victim, Blami...</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 0, 0, 1]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_130.txt</td>\n",
       "      <td>&lt;PARA&gt;hungria mantém veto de fundos da UE a Ky...</td>\n",
       "      <td>[Discrediting Ukraine, Discrediting Ukraine]</td>\n",
       "      <td>[Other, Discrediting Ukrainian government and ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  language        article_id  \\\n",
       "0       EN  EN_CC_200022.txt   \n",
       "1       EN  EN_CC_200221.txt   \n",
       "2       EN  EN_CC_200110.txt   \n",
       "3       HI        HI_173.txt   \n",
       "4       PT        PT_130.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>Denmark to Punish Farmers for cow ‘emiss...   \n",
       "1  <PARA>“the hour of decision”</PARA>\\n\\nshortly...   \n",
       "2  <PARA>if democrats successfully ban gas stoves...   \n",
       "3  <PARA>यूक्रेन के बढ़ते हमलों के बीच रूस ने क्य...   \n",
       "4  <PARA>hungria mantém veto de fundos da UE a Ky...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [Criticism of institutions and authorities, Cr...   \n",
       "1  [Hidden plots by secret schemes of powerful gr...   \n",
       "2  [Criticism of climate policies, Criticism of i...   \n",
       "3  [Praise of Russia, Russia is the Victim, Blami...   \n",
       "4       [Discrediting Ukraine, Discrediting Ukraine]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [Criticism of national governments, Other, Met...   \n",
       "1  [Blaming global elites, Other, Other, CO2 conc...   \n",
       "2  [Climate policies have negative impact on the ...   \n",
       "3  [Russia is a guarantor of peace and prosperity...   \n",
       "4  [Other, Discrediting Ukrainian government and ...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "         narrative_hierarchy_8  narrative_hierarchy_9 narrative_hierarchy_17  \\\n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0, 0, 0]     [0, 0, 0, 1, 0, 0]   \n",
       "1  [0, 0, 0, 1, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0, 0, 0]     [0, 0, 0, 1, 0, 0]   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0, 0, 0]     [0, 0, 0, 1, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0]     [1, 1, 0, 0, 0, 1]   \n",
       "4  [0, 0, 0, 1, 0, 0, 0, 0, 1]  [0, 0, 0, 1, 0, 0, 0]     [0, 0, 0, 1, 0, 0]   \n",
       "\n",
       "   ... narrative_hierarchy_0 narrative_hierarchy_15 narrative_hierarchy_7  \\\n",
       "0  ...       [0, 0, 0, 0, 1]                    [1]       [1, 1, 0, 0, 1]   \n",
       "1  ...       [0, 0, 0, 0, 1]                    [1]       [0, 1, 0, 1, 0]   \n",
       "2  ...       [0, 0, 0, 0, 1]                    [1]       [0, 1, 0, 1, 1]   \n",
       "3  ...       [0, 0, 0, 0, 0]                    [0]       [0, 0, 0, 0, 0]   \n",
       "4  ...       [0, 0, 0, 0, 1]                    [1]       [0, 1, 0, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_5       narrative_hierarchy_11 narrative_hierarchy_6  \\\n",
       "0          [0, 0, 1, 0]  [0, 0, 0, 1, 0, 0, 0, 0, 0]          [1, 0, 0, 0]   \n",
       "1          [0, 0, 1, 0]  [0, 0, 0, 1, 0, 0, 1, 0, 1]          [1, 0, 0, 0]   \n",
       "2          [0, 0, 1, 0]  [0, 0, 0, 1, 0, 0, 0, 0, 0]          [1, 0, 1, 1]   \n",
       "3          [0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0]          [0, 0, 0, 0]   \n",
       "4          [0, 0, 1, 0]  [0, 0, 0, 1, 0, 0, 0, 0, 0]          [1, 0, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_18 narrative_hierarchy_3 narrative_hierarchy_4  \\\n",
       "0        [0, 0, 1, 1, 1]             [1, 0, 0]          [1, 0, 0, 0]   \n",
       "1        [1, 0, 1, 0, 0]             [1, 0, 0]          [1, 0, 0, 0]   \n",
       "2        [0, 0, 1, 0, 0]             [1, 0, 0]          [1, 1, 0, 0]   \n",
       "3        [0, 0, 0, 0, 0]             [0, 0, 0]          [0, 0, 0, 0]   \n",
       "4        [0, 0, 1, 0, 0]             [1, 0, 0]          [1, 0, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_12  \n",
       "0              [0, 0, 1]  \n",
       "1              [0, 0, 1]  \n",
       "2              [0, 0, 1]  \n",
       "3              [0, 0, 0]  \n",
       "4              [0, 0, 1]  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_cpy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed68aa1-856d-4e76-a332-5640670bf9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "2       [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "                   ...             \n",
      "1359    [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "1362    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_8, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 1, 0, 0, 0]\n",
      "2       [0, 0, 0, 1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 1, 0, 0, 0]\n",
      "                ...          \n",
      "1359    [0, 0, 0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 1, 0, 0, 0]\n",
      "1362    [0, 0, 0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 0, 1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_9, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 1, 0, 0]\n",
      "2       [0, 0, 0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 1, 0, 0]\n",
      "               ...        \n",
      "1359    [0, 0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 1, 0, 0]\n",
      "1362    [0, 0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_17, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0]\n",
      "1       [0, 1, 0, 0]\n",
      "2       [0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 1, 0, 0]\n",
      "            ...     \n",
      "1359    [0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0]\n",
      "1361    [0, 1, 0, 0]\n",
      "1362    [0, 0, 0, 0]\n",
      "1363    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_19, Length: 1364, dtype: object\n",
      "0       [0, 0, 0]\n",
      "1       [0, 1, 0]\n",
      "2       [0, 1, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 1, 0]\n",
      "          ...    \n",
      "1359    [0, 0, 0]\n",
      "1360    [1, 0, 0]\n",
      "1361    [0, 1, 0]\n",
      "1362    [0, 0, 0]\n",
      "1363    [0, 1, 0]\n",
      "Name: narrative_hierarchy_10, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 1, 0]\n",
      "2       [0, 0, 0, 1, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 1, 0]\n",
      "             ...       \n",
      "1359    [0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 1, 0]\n",
      "1362    [0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 0, 1, 0]\n",
      "Name: narrative_hierarchy_1, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0]\n",
      "1       [0, 1, 0, 0]\n",
      "2       [0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 1, 0, 0]\n",
      "            ...     \n",
      "1359    [0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0]\n",
      "1361    [0, 1, 0, 0]\n",
      "1362    [0, 0, 0, 0]\n",
      "1363    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_16, Length: 1364, dtype: object\n",
      "0       [0, 0, 0]\n",
      "1       [0, 1, 0]\n",
      "2       [0, 1, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 1, 0]\n",
      "          ...    \n",
      "1359    [0, 0, 0]\n",
      "1360    [0, 0, 0]\n",
      "1361    [0, 1, 0]\n",
      "1362    [0, 0, 0]\n",
      "1363    [0, 1, 0]\n",
      "Name: narrative_hierarchy_2, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0]\n",
      "1       [1, 0, 0, 0]\n",
      "2       [1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [1, 0, 0, 0]\n",
      "            ...     \n",
      "1359    [0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0]\n",
      "1361    [1, 0, 0, 0]\n",
      "1362    [0, 0, 0, 0]\n",
      "1363    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_20, Length: 1364, dtype: object\n",
      "0       [0, 0, 0]\n",
      "1       [0, 1, 1]\n",
      "2       [0, 1, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 1, 0]\n",
      "          ...    \n",
      "1359    [0, 0, 0]\n",
      "1360    [0, 0, 0]\n",
      "1361    [0, 1, 0]\n",
      "1362    [0, 0, 1]\n",
      "1363    [0, 1, 0]\n",
      "Name: narrative_hierarchy_13, Length: 1364, dtype: object\n",
      "0       [0, 0, 0]\n",
      "1       [0, 1, 0]\n",
      "2       [0, 1, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 1, 0]\n",
      "          ...    \n",
      "1359    [0, 0, 0]\n",
      "1360    [0, 0, 0]\n",
      "1361    [0, 1, 0]\n",
      "1362    [0, 0, 0]\n",
      "1363    [0, 1, 0]\n",
      "Name: narrative_hierarchy_14, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 1]\n",
      "2       [0, 0, 0, 0, 1]\n",
      "3       [0, 0, 0, 1, 0]\n",
      "4       [0, 0, 0, 0, 1]\n",
      "             ...       \n",
      "1359    [0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 1]\n",
      "1362    [0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 0, 0, 1]\n",
      "Name: narrative_hierarchy_0, Length: 1364, dtype: object\n",
      "0       [0]\n",
      "1       [1]\n",
      "2       [1]\n",
      "3       [0]\n",
      "4       [1]\n",
      "       ... \n",
      "1359    [0]\n",
      "1360    [0]\n",
      "1361    [1]\n",
      "1362    [0]\n",
      "1363    [1]\n",
      "Name: narrative_hierarchy_15, Length: 1364, dtype: object\n",
      "0       [1, 0, 0, 1, 0]\n",
      "1       [0, 1, 0, 0, 0]\n",
      "2       [0, 1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 1, 0, 0, 0]\n",
      "             ...       \n",
      "1359    [0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0]\n",
      "1361    [0, 1, 0, 0, 0]\n",
      "1362    [0, 0, 0, 0, 0]\n",
      "1363    [0, 1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_7, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0]\n",
      "1       [1, 0, 1, 0]\n",
      "2       [0, 0, 1, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 1, 0]\n",
      "            ...     \n",
      "1359    [0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0]\n",
      "1361    [0, 0, 1, 0]\n",
      "1362    [1, 0, 0, 0]\n",
      "1363    [0, 0, 1, 0]\n",
      "Name: narrative_hierarchy_5, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1       [1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "2       [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "                   ...             \n",
      "1359    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "1362    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_11, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0]\n",
      "1       [1, 0, 0, 0]\n",
      "2       [1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [1, 1, 0, 0]\n",
      "            ...     \n",
      "1359    [0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0]\n",
      "1361    [1, 0, 0, 0]\n",
      "1362    [0, 1, 0, 0]\n",
      "1363    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_6, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 1, 1]\n",
      "1       [1, 0, 1, 0, 0]\n",
      "2       [0, 0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 0, 1, 0, 0]\n",
      "             ...       \n",
      "1359    [0, 0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 1, 0, 0]\n",
      "1362    [0, 0, 0, 0, 0]\n",
      "1363    [0, 0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_18, Length: 1364, dtype: object\n",
      "0       [0, 0, 0]\n",
      "1       [1, 1, 0]\n",
      "2       [1, 0, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [1, 0, 0]\n",
      "          ...    \n",
      "1359    [0, 0, 0]\n",
      "1360    [0, 0, 0]\n",
      "1361    [1, 0, 0]\n",
      "1362    [0, 0, 0]\n",
      "1363    [1, 0, 0]\n",
      "Name: narrative_hierarchy_3, Length: 1364, dtype: object\n",
      "0       [0, 0, 0, 0]\n",
      "1       [1, 0, 0, 0]\n",
      "2       [1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [1, 0, 0, 0]\n",
      "            ...     \n",
      "1359    [0, 0, 0, 0]\n",
      "1360    [0, 0, 0, 0]\n",
      "1361    [1, 0, 0, 0]\n",
      "1362    [0, 1, 1, 0]\n",
      "1363    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_4, Length: 1364, dtype: object\n",
      "0       [0, 0, 0]\n",
      "1       [0, 0, 1]\n",
      "2       [0, 0, 1]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 0, 1]\n",
      "          ...    \n",
      "1359    [0, 0, 0]\n",
      "1360    [0, 0, 0]\n",
      "1361    [0, 0, 1]\n",
      "1362    [0, 0, 0]\n",
      "1363    [0, 0, 1]\n",
      "Name: narrative_hierarchy_12, Length: 1364, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    column_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "    res = dataset_train_cpy[column_name]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed4fea0-adaf-4bce-b35f-aed1a9ce7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort order of narratives to start from hierarchy 0\n",
    "narrative_order = sorted(narrative_to_sub_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28118762-97a6-4dac-8cf0-0a0f7b3c972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d3f065-4abf-422c-b85d-c6879a3c157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subnarratives(row, narrative_order, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row, and aggregates all hierarchy columns to 1 list.\n",
    "    The encoded list will be a list of lists, starting from the first hierarchy\"\"\"\n",
    "    aggregated = []\n",
    "    for narr_idx in narrative_order:\n",
    "        column_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "        sub_labels = row[column_name]\n",
    "        aggregated.append(sub_labels)\n",
    "    return aggregated\n",
    "\n",
    "dataset_train['aggregated_subnarratives'] = dataset_train_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")\n",
    "\n",
    "dataset_val['aggregated_subnarratives'] = dataset_val_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e76fa06-fa05-4b24-ab16-2862f7df659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1       [[0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0], ...\n",
       "2       [[0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0], ...\n",
       "3       [[0, 0, 0, 1, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "4       [[0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0], ...\n",
       "                              ...                        \n",
       "1359    [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1360    [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1361    [[0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0], ...\n",
       "1362    [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1363    [[0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0], ...\n",
       "Name: aggregated_subnarratives, Length: 1364, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['aggregated_subnarratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1968bf35-e93b-465e-b0a2-742953b808bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee760da7-ecad-4690-8c05-46c33b447ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ff6872b-c06e-4450-b232-7d812eff12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c3acef3-fa00-4d9e-bec9-0f9cacd9fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Shared layer\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Top-level narratives: multi-label => Sigmoid\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Subnarrative heads: multi-label => Sigmoid\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71b02d39-8af6-4cae-bde3-b82a663fbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head = MultiTaskClassifierMultiHead(\n",
    "    input_size=input_size,\n",
    "    hidden_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bfde290-d982-49fa-a89a-b48bb81a1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "y_val_nar = dataset_val['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n",
    "y_val_sub_nar = dataset_val['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18b1217b-9e4b-4fcc-8cb1-4274f4bdd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32)\n",
    "\n",
    "y_val_nar = torch.tensor(y_val_nar, dtype=torch.float32)\n",
    "y_val_sub_nar = torch.tensor(y_val_sub_nar, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1b730da-7939-4cdd-b725-a95d84624222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a10fe2bc-989e-40f5-9779-14d70ec69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)\n",
    "\n",
    "class_weights_sub_nar = compute_class_weights(y_val_sub_nar)\n",
    "class_weights_nar = compute_class_weights(y_val_nar)\n",
    "narrative_criterion = WeightedBCELoss(class_weights_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b8e8505-b560-4307-85a3-7ef738cfb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subnarrative head, add a weighted version of BCE based on the indices\n",
    "sub_criterion_dict = {}\n",
    "\n",
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    local_weights = [ class_weights_sub_nar[sub_i] for sub_i in sub_indices ]\n",
    "\n",
    "    sub_criterion = WeightedBCELoss(local_weights)\n",
    "    sub_criterion_dict[str(narr_idx)] = sub_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3863a1f5-eb9e-4247-9e0d-405799431c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_loss(narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "    narr_loss = narrative_criterion(narr_probs, y_narr)\n",
    "\n",
    "    sub_loss = 0.0\n",
    "    count_active = 0\n",
    "    i = 0\n",
    "    for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "        narr_idx = int(narr_idx_str)\n",
    "        # Find the true subnarratives for the batch\n",
    "        y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "        y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32)\n",
    "\n",
    "        sub_loss_func = sub_criterion_dict[narr_idx_str]\n",
    "        ce_loss = sub_loss_func(sub_probs, y_sub_tensor)\n",
    "\n",
    "        sub_loss += ce_loss\n",
    "        count_active += 1\n",
    "        i += 1\n",
    "\n",
    "    if count_active > 0:\n",
    "        sub_loss = sub_loss / count_active\n",
    "    else:\n",
    "        sub_loss = 0.0\n",
    "\n",
    "    total_loss = narr_loss + sub_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f7c1e38-c06b-4285-9d46-0c338ce8c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    narrative_criterion,\n",
    "    train_embeddings=train_embeddings_tensor,\n",
    "    y_train_nar=y_train_nar,\n",
    "    y_train_sub_heads=y_train_sub_heads,\n",
    "    val_embeddings=val_embeddings_tensor,\n",
    "    y_val_nar=y_val_nar,\n",
    "    y_val_sub_heads=y_val_sub_heads,\n",
    "    patience=3,\n",
    "    num_epochs=100,\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "        train_loss = multi_head_loss(train_narr_probs, train_sub_probs_dict, y_train_nar, y_train_sub_heads)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            val_loss = multi_head_loss(val_narr_probs, val_sub_probs_dict, y_val_nar, y_val_sub_heads)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {train_loss.item():.4f} \"\n",
    "              f\"Validation Loss: {val_loss.item():.4f} \")\n",
    "\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec5cf11-220e-48db-be7f-d3ce7aa5e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_multi_head = torch.optim.AdamW(model_multi_head.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a501a7b4-6d0c-4c3f-bf4b-33015a1b73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.4292 Validation Loss: 1.3683 \n",
      "Epoch 2/100, Training Loss: 1.1706 Validation Loss: 1.3606 \n",
      "Epoch 3/100, Training Loss: 1.0477 Validation Loss: 1.3530 \n",
      "Epoch 4/100, Training Loss: 0.9628 Validation Loss: 1.3454 \n",
      "Epoch 5/100, Training Loss: 0.8954 Validation Loss: 1.3376 \n",
      "Epoch 6/100, Training Loss: 0.8417 Validation Loss: 1.3297 \n",
      "Epoch 7/100, Training Loss: 0.7930 Validation Loss: 1.3215 \n",
      "Epoch 8/100, Training Loss: 0.7576 Validation Loss: 1.3129 \n",
      "Epoch 9/100, Training Loss: 0.7215 Validation Loss: 1.3038 \n",
      "Epoch 10/100, Training Loss: 0.6884 Validation Loss: 1.2941 \n",
      "Epoch 11/100, Training Loss: 0.6615 Validation Loss: 1.2841 \n",
      "Epoch 12/100, Training Loss: 0.6369 Validation Loss: 1.2740 \n",
      "Epoch 13/100, Training Loss: 0.6142 Validation Loss: 1.2637 \n",
      "Epoch 14/100, Training Loss: 0.5916 Validation Loss: 1.2533 \n",
      "Epoch 15/100, Training Loss: 0.5720 Validation Loss: 1.2430 \n",
      "Epoch 16/100, Training Loss: 0.5552 Validation Loss: 1.2325 \n",
      "Epoch 17/100, Training Loss: 0.5392 Validation Loss: 1.2218 \n",
      "Epoch 18/100, Training Loss: 0.5244 Validation Loss: 1.2109 \n",
      "Epoch 19/100, Training Loss: 0.5073 Validation Loss: 1.2001 \n",
      "Epoch 20/100, Training Loss: 0.4916 Validation Loss: 1.1896 \n",
      "Epoch 21/100, Training Loss: 0.4793 Validation Loss: 1.1790 \n",
      "Epoch 22/100, Training Loss: 0.4660 Validation Loss: 1.1683 \n",
      "Epoch 23/100, Training Loss: 0.4526 Validation Loss: 1.1578 \n",
      "Epoch 24/100, Training Loss: 0.4405 Validation Loss: 1.1473 \n",
      "Epoch 25/100, Training Loss: 0.4269 Validation Loss: 1.1364 \n",
      "Epoch 26/100, Training Loss: 0.4157 Validation Loss: 1.1254 \n",
      "Epoch 27/100, Training Loss: 0.4025 Validation Loss: 1.1146 \n",
      "Epoch 28/100, Training Loss: 0.3916 Validation Loss: 1.1044 \n",
      "Epoch 29/100, Training Loss: 0.3829 Validation Loss: 1.0942 \n",
      "Epoch 30/100, Training Loss: 0.3683 Validation Loss: 1.0846 \n",
      "Epoch 31/100, Training Loss: 0.3596 Validation Loss: 1.0754 \n",
      "Epoch 32/100, Training Loss: 0.3493 Validation Loss: 1.0660 \n",
      "Epoch 33/100, Training Loss: 0.3386 Validation Loss: 1.0576 \n",
      "Epoch 34/100, Training Loss: 0.3285 Validation Loss: 1.0496 \n",
      "Epoch 35/100, Training Loss: 0.3173 Validation Loss: 1.0433 \n",
      "Epoch 36/100, Training Loss: 0.3086 Validation Loss: 1.0382 \n",
      "Epoch 37/100, Training Loss: 0.3010 Validation Loss: 1.0326 \n",
      "Epoch 38/100, Training Loss: 0.2899 Validation Loss: 1.0264 \n",
      "Epoch 39/100, Training Loss: 0.2807 Validation Loss: 1.0207 \n",
      "Epoch 40/100, Training Loss: 0.2726 Validation Loss: 1.0159 \n",
      "Epoch 41/100, Training Loss: 0.2634 Validation Loss: 1.0137 \n",
      "Epoch 42/100, Training Loss: 0.2551 Validation Loss: 1.0140 \n",
      "Validation loss did not improve for 1 epoch(s).\n",
      "Epoch 43/100, Training Loss: 0.2467 Validation Loss: 1.0131 \n",
      "Epoch 44/100, Training Loss: 0.2383 Validation Loss: 1.0116 \n",
      "Epoch 45/100, Training Loss: 0.2309 Validation Loss: 1.0129 \n",
      "Validation loss did not improve for 1 epoch(s).\n",
      "Epoch 46/100, Training Loss: 0.2234 Validation Loss: 1.0145 \n",
      "Validation loss did not improve for 2 epoch(s).\n",
      "Epoch 47/100, Training Loss: 0.2166 Validation Loss: 1.0175 \n",
      "Validation loss did not improve for 3 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiTaskClassifierMultiHead(\n",
       "  (shared_layer): Sequential(\n",
       "    (0): Linear(in_features=896, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (narrative_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=21, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (subnarrative_heads): ModuleDict(\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=6, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_multihead(\n",
    "    model=model_multi_head,\n",
    "    optimizer=optimizer_multi_head,\n",
    "    narrative_criterion=narrative_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b1ec872-c847-41c3-b2ab-4e1d87d48465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_multihead_model(\n",
    "    model,\n",
    "    embeddings,\n",
    "    y_nar_true,\n",
    "    y_sub_hierarchical,\n",
    "    num_subnarratives = len(mlb_subnarratives.classes_),\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1),\n",
    "    target_names_nar=mlb_narratives.classes_,\n",
    "    target_names_sub=mlb_subnarratives.classes_,\n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    def build_global_sub_array(\n",
    "        y_sub_hierarchical,\n",
    "        num_subnarratives=74,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "    ):\n",
    "        \"\"\"Reconstructs the subnarratives to flatten them (again) to a single array for evaluation\"\"\"\n",
    "        num_samples = len(y_sub_hierarchical)\n",
    "        sub_global_array = np.zeros((num_samples, num_subnarratives), dtype=int)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            for j, narr_idx in enumerate(narrative_order):\n",
    "                sub_label_vec = y_sub_hierarchical[i][j]\n",
    "                narr_idx = int(narr_idx)\n",
    "                sub_indices = narrative_to_sub_map[narr_idx]\n",
    "                for local_sub_i, global_sub_i in enumerate(sub_indices):\n",
    "                    sub_global_array[i, global_sub_i] = sub_label_vec[local_sub_i]\n",
    "\n",
    "        return sub_global_array\n",
    "\n",
    "    embeddings = embeddings.to(device)\n",
    "    y_nar_true_np = y_nar_true.cpu().numpy()\n",
    "\n",
    "    best_threshold = 0\n",
    "    best_f1 = -1\n",
    "    best_report_nar = None\n",
    "    best_report_sub = None\n",
    "    samples = len(embeddings)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # get the predictions for both\n",
    "        narr_probs, sub_probs_dict = model(embeddings)\n",
    "\n",
    "        narr_probs = narr_probs.cpu().numpy()\n",
    "        for k in sub_probs_dict:\n",
    "            sub_probs_dict[k] = sub_probs_dict[k].cpu().numpy()\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        narr_preds = (narr_probs >= threshold).astype(int)\n",
    "\n",
    "        # Need to reconstruct the subnarratives to flatten them (again) to a single array for evaluation\n",
    "        sub_preds_global = np.zeros((samples, num_subnarratives), dtype=int)\n",
    "\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            # Get the predictions for this narrative hierarchy\n",
    "            sub_probs_for_narr = sub_probs_dict[str(narr_idx)]\n",
    "            # If top-level narrative is 1, then threshold subnarratives; otherwise 0.\n",
    "            # Finds for each sample, go to the narr_idx position (the hierarchy we are at)\n",
    "            predicted_narr_mask = narr_preds[:, narr_idx] == 1  # shape (num_samples,)\n",
    "\n",
    "            # For all samples, threshold sub_probs_for_narr:\n",
    "            sub_preds_for_narr = (sub_probs_for_narr >= threshold).astype(int)\n",
    "\n",
    "            # But only keep sub_preds_for_narr if predicted_narr_mask is True:\n",
    "            # If predicted_narr_mask is False for a sample, subnarratives go to 0.\n",
    "            for sample_idx in range(samples):\n",
    "                if predicted_narr_mask[sample_idx] == 1:\n",
    "                    # Construct the flattened pred array\n",
    "                    for local_sub_i, global_sub_i in enumerate(sub_indices):\n",
    "                        sub_preds_global[sample_idx, global_sub_i] = sub_preds_for_narr[sample_idx, local_sub_i]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        f1_nar = f1_score(y_nar_true_np, narr_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # Also flatten the true y_sub to a single array in the same way as we did with the predictions\n",
    "        y_sub_true_np = build_global_sub_array(y_sub_hierarchical, num_subnarratives=num_subnarratives)\n",
    "\n",
    "        f1_sub = f1_score(y_sub_true_np, sub_preds_global, average=\"macro\", zero_division=0)\n",
    "\n",
    "        avg_f1 = (f1_nar + f1_sub) / 2.0\n",
    "\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "            report_nar = classification_report(\n",
    "                y_nar_true_np,\n",
    "                narr_preds,\n",
    "                target_names=target_names_nar,\n",
    "                zero_division=0\n",
    "            )\n",
    "            report_sub = classification_report(\n",
    "                y_sub_true_np,\n",
    "                sub_preds_global,\n",
    "                target_names=target_names_sub,\n",
    "                zero_division=0\n",
    "            )\n",
    "            best_report_nar = report_nar\n",
    "            best_report_sub = report_sub\n",
    "\n",
    "    print(f\"Best threshold = {best_threshold:.2f}, best (avg) F1 = {best_f1:.4f}\")\n",
    "    print(\"Best Narratives classification report:\")\n",
    "    print(best_report_nar)\n",
    "    print(\"Best Subnarratives classification report:\")\n",
    "    print(best_report_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e873134-0fe1-4b60-bdf9-f0eee66ec99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold = 0.50, best (avg) F1 = 0.3827\n",
      "Best Narratives classification report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                         Amplifying Climate Fears       0.81      1.00      0.90        48\n",
      "                     Amplifying war-related fears       0.69      0.69      0.69        49\n",
      "Blaming the war on others rather than the invader       0.27      0.38      0.32        39\n",
      "                     Climate change is beneficial       0.00      0.00      0.00         2\n",
      "             Controversy about green technologies       0.40      1.00      0.57         4\n",
      "                    Criticism of climate movement       0.26      0.70      0.38        10\n",
      "                    Criticism of climate policies       0.50      0.58      0.54        19\n",
      "        Criticism of institutions and authorities       0.60      0.77      0.68        35\n",
      "                             Discrediting Ukraine       0.70      0.79      0.74        85\n",
      "                 Discrediting the West, Diplomacy       0.58      0.75      0.65        68\n",
      "                           Distrust towards Media       0.75      0.27      0.40        11\n",
      "                       Downplaying climate change       0.40      0.80      0.53        10\n",
      "      Green policies are geopolitical instruments       0.00      0.00      0.00         2\n",
      "Hidden plots by secret schemes of powerful groups       0.33      0.75      0.46        12\n",
      "               Negative Consequences for the West       0.37      0.28      0.32        25\n",
      "                                            Other       0.68      0.62      0.65        63\n",
      "                            Overpraising the West       0.22      0.25      0.24         8\n",
      "                                 Praise of Russia       0.53      0.78      0.63        67\n",
      "         Questioning the measurements and science       0.40      0.67      0.50         6\n",
      "                             Russia is the Victim       0.54      0.57      0.56        44\n",
      "                         Speculating war outcomes       0.40      0.77      0.53        22\n",
      "\n",
      "                                        micro avg       0.55      0.68      0.61       629\n",
      "                                        macro avg       0.45      0.59      0.49       629\n",
      "                                     weighted avg       0.57      0.68      0.61       629\n",
      "                                      samples avg       0.59      0.69      0.60       629\n",
      "\n",
      "Best Subnarratives classification report:\n",
      "                                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                   Ad hominem attacks on key activists       0.27      1.00      0.42         4\n",
      "                           Amplifying existing fears of global warming       0.69      0.94      0.80        36\n",
      "                                                 Blaming global elites       0.25      1.00      0.40         4\n",
      "                                   By continuing the war we risk WWIII       0.21      0.50      0.30        12\n",
      "                    CO2 concentrations are too small to have an impact       0.25      0.50      0.33         2\n",
      "                                                     CO2 is beneficial       0.00      0.00      0.00         1\n",
      "                                     Climate agenda has hidden motives       0.24      0.67      0.35         6\n",
      "                                            Climate cycles are natural       0.18      1.00      0.30         3\n",
      "                                          Climate movement is alarmist       0.14      0.60      0.23         5\n",
      "                                           Climate movement is corrupt       0.25      1.00      0.40         2\n",
      "                                      Climate policies are ineffective       0.16      0.43      0.23         7\n",
      "                                  Climate policies are only for profit       0.25      0.67      0.36         3\n",
      "                  Climate policies have negative impact on the economy       0.50      0.67      0.57         9\n",
      "      Climate-related international relations are abusive/exploitative       0.00      0.00      0.00         1\n",
      "                                   Criticism of international entities       0.30      0.43      0.35         7\n",
      "                                     Criticism of national governments       0.35      0.67      0.46        18\n",
      "                      Criticism of political organizations and figures       0.32      0.69      0.44        16\n",
      "                                                   Criticism of the EU       0.50      0.50      0.50         2\n",
      "                                    Data shows no temperature increase       0.00      0.00      0.00         0\n",
      "                                          Diplomacy does/will not work       0.16      0.56      0.25         9\n",
      "          Discrediting Ukrainian government and officials and policies       0.34      0.69      0.46        32\n",
      "                                       Discrediting Ukrainian military       0.24      0.65      0.35        20\n",
      "                             Discrediting Ukrainian nation and society       0.14      0.50      0.22         2\n",
      "                                         Doomsday scenarios for humans       0.24      1.00      0.38         9\n",
      "                                      Earth will be uninhabitable soon       0.20      0.80      0.32         5\n",
      "                        Green activities are a form of neo-colonialism       0.00      0.00      0.00         0\n",
      "          Greenhouse effect/carbon dioxide do not drive climate change       0.00      0.00      0.00         1\n",
      "                         Human activities do not impact climate change       0.08      0.50      0.14         2\n",
      "                           Humans and nature will adapt to the changes       0.00      0.00      0.00         1\n",
      "                                                    Ice is not melting       0.00      0.00      0.00         1\n",
      "                      Methodologies/metrics used are unreliable/faulty       0.25      0.33      0.29         3\n",
      "                                   NATO should/will directly intervene       0.23      0.60      0.33         5\n",
      "                                              NATO will destroy Russia       0.00      0.00      0.00         1\n",
      "                                                                 Other       0.75      0.62      0.68       187\n",
      "                            Praise of Russian President Vladimir Putin       0.33      0.67      0.44         9\n",
      "                                      Praise of Russian military might       0.36      0.69      0.48        29\n",
      "                                            Renewable energy is costly       0.17      0.50      0.25         2\n",
      "                                         Renewable energy is dangerous       0.33      1.00      0.50         1\n",
      "                                        Renewable energy is unreliable       0.14      0.50      0.22         2\n",
      "                                           Rewriting Ukraine’s history       0.00      0.00      0.00         1\n",
      "                       Russia actions in Ukraine are only self-defence       0.30      0.71      0.43        14\n",
      "Russia has international support from a number of countries and people       0.44      0.69      0.54        16\n",
      "                         Russia is a guarantor of peace and prosperity       0.23      0.57      0.33        21\n",
      "                               Russia will also attack other countries       0.32      0.73      0.44        11\n",
      "                                            Russian army is collapsing       0.33      0.67      0.44         3\n",
      "                   Russian army will lose all the occupied territories       0.00      0.00      0.00         1\n",
      "                          Russian invasion has strong national support       0.00      0.00      0.00         2\n",
      "                  Sanctions imposed by Western countries will backfire       0.25      0.14      0.18         7\n",
      "                                    Scientific community is unreliable       0.50      0.60      0.55         5\n",
      "                                             Sea levels are not rising       0.00      0.00      0.00         1\n",
      "                                      Situation in Ukraine is hopeless       0.17      0.55      0.26        11\n",
      "                 Temperature increase does not have significant impact       0.00      0.00      0.00         1\n",
      "                                    Temperature increase is beneficial       0.00      0.00      0.00         1\n",
      "                                                     The EU is divided       0.17      0.38      0.23         8\n",
      "                                           The West are the aggressors       0.14      0.22      0.17        23\n",
      "                         The West belongs in the right side of history       0.00      0.00      0.00         3\n",
      "        The West does not care about Ukraine, only about its interests       0.18      0.59      0.28        17\n",
      "                      The West has the strongest international support       0.33      0.33      0.33         3\n",
      "                                              The West is overreacting       0.00      0.00      0.00         2\n",
      "                                               The West is russophobic       0.09      0.07      0.08        14\n",
      "                                                      The West is weak       0.24      0.67      0.36        12\n",
      "      The conflict will increase the Ukrainian refugee flows to Europe       1.00      1.00      1.00         1\n",
      "     There is a real possibility that nuclear weapons will be employed       0.61      0.70      0.65        20\n",
      "                                              UA is anti-RU extremists       0.00      0.00      0.00         3\n",
      "                              Ukraine is a hub for criminal activities       0.24      0.62      0.34         8\n",
      "                                       Ukraine is a puppet of the West       0.20      0.62      0.30        21\n",
      "                                     Ukraine is associated with nazism       0.14      0.75      0.24         8\n",
      "                                              Ukraine is the aggressor       0.22      0.38      0.28        16\n",
      "                                          Ukrainian army is collapsing       0.13      0.25      0.17         8\n",
      "                                     Ukrainian media cannot be trusted       0.00      0.00      0.00         1\n",
      "                          Weather suggests the trend is global cooling       0.00      0.00      0.00         0\n",
      "                                              West is tired of Ukraine       0.22      0.50      0.31         4\n",
      "                          Western media is an instrument of propaganda       0.75      0.38      0.50         8\n",
      "                                 Whatever we do it is already too late       0.17      0.50      0.25         2\n",
      "\n",
      "                                                             micro avg       0.32      0.59      0.42       736\n",
      "                                                             macro avg       0.22      0.45      0.28       736\n",
      "                                                          weighted avg       0.41      0.59      0.45       736\n",
      "                                                           samples avg       0.40      0.61      0.44       736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_multihead_model(\n",
    "    model=model_multi_head,\n",
    "    embeddings=val_embeddings_tensor,\n",
    "    y_nar_true=y_val_nar,\n",
    "    y_sub_hierarchical=y_val_sub_heads,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
