{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0d8ce9-d6d9-4e03-82e8-36fe1c5a343a",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa2ebd-27af-46e1-aa7c-d2549cfc988a",
   "metadata": {},
   "source": [
    "## 1. Multi-head per narrative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67788-7872-4608-ba0e-01fd263958cf",
   "metadata": {},
   "source": [
    "### 1.1 Loading pre-saved variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d05fc-7c36-43ed-a942-bd6c8daf585d",
   "metadata": {},
   "source": [
    "We start by loading our pre-saved variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe27a15d-ab42-4768-a155-a65353545112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/all_embeddings.npy')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_cleaned.pkl'), 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d58b84c-f126-4f5f-9f4c-883bd5b2741d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[Blaming the war on others rather than the inv...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[Discrediting the West, Diplomacy, Discreditin...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[Discrediting Ukraine, Discrediting Ukraine]</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [Blaming the war on others rather than the inv...   \n",
       "1  [Discrediting the West, Diplomacy, Discreditin...   \n",
       "2                           [Distrust towards Media]   \n",
       "3       [Discrediting Ukraine, Discrediting Ukraine]   \n",
       "4                                 [Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e056632b-56d7-47ae-929b-cdda00d86a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae755a-3967-4790-8405-51b2a99594bd",
   "metadata": {},
   "source": [
    "We'll also need the actual hierarchy of narratives to subnarratives for our new model.  \n",
    "\n",
    "* Each narrative is also mapped to `Other`—this happens because if no subnarrative matches, we assign it to `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11550c30-6e80-4c4e-b262-ecec4d942892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Discrediting Ukraine': ['Other',\n",
       "  'Ukraine is associated with nazism',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Ukraine is a hub for criminal activities',\n",
       "  'Discrediting Ukrainian government and officials and policies',\n",
       "  'Rewriting Ukraine’s history',\n",
       "  'Discrediting Ukrainian nation and society',\n",
       "  'Ukraine is a puppet of the West',\n",
       "  'Discrediting Ukrainian military'],\n",
       " 'Discrediting the West, Diplomacy': ['Other',\n",
       "  'West is tired of Ukraine',\n",
       "  'The West is weak',\n",
       "  'The West does not care about Ukraine, only about its interests',\n",
       "  'The EU is divided',\n",
       "  'Diplomacy does/will not work',\n",
       "  'The West is overreacting'],\n",
       " 'Praise of Russia': ['Other',\n",
       "  'Praise of Russian President Vladimir Putin',\n",
       "  'Praise of Russian military might',\n",
       "  'Russia has international support from a number of countries and people',\n",
       "  'Russian invasion has strong national support',\n",
       "  'Russia is a guarantor of peace and prosperity'],\n",
       " 'Russia is the Victim': ['Russia actions in Ukraine are only self-defence',\n",
       "  'Other',\n",
       "  'UA is anti-RU extremists',\n",
       "  'The West is russophobic'],\n",
       " 'Distrust towards Media': ['Other',\n",
       "  'Ukrainian media cannot be trusted',\n",
       "  'Western media is an instrument of propaganda'],\n",
       " 'Amplifying war-related fears': ['Other',\n",
       "  'Russia will also attack other countries',\n",
       "  'NATO should/will directly intervene',\n",
       "  'By continuing the war we risk WWIII',\n",
       "  'There is a real possibility that nuclear weapons will be employed'],\n",
       " 'Overpraising the West': ['The West belongs in the right side of history',\n",
       "  'Other',\n",
       "  'The West has the strongest international support',\n",
       "  'NATO will destroy Russia'],\n",
       " 'Blaming the war on others rather than the invader': ['The West are the aggressors',\n",
       "  'Ukraine is the aggressor',\n",
       "  'Other'],\n",
       " 'Speculating war outcomes': ['Russian army is collapsing',\n",
       "  'Other',\n",
       "  'Russian army will lose all the occupied territories',\n",
       "  'Ukrainian army is collapsing'],\n",
       " 'Hidden plots by secret schemes of powerful groups': ['Blaming global elites',\n",
       "  'Other',\n",
       "  'Climate agenda has hidden motives'],\n",
       " 'Negative Consequences for the West': ['Sanctions imposed by Western countries will backfire',\n",
       "  'Other',\n",
       "  'The conflict will increase the Ukrainian refugee flows to Europe'],\n",
       " 'Amplifying Climate Fears': ['Other',\n",
       "  'Amplifying existing fears of global warming',\n",
       "  'Earth will be uninhabitable soon',\n",
       "  'Whatever we do it is already too late',\n",
       "  'Doomsday scenarios for humans'],\n",
       " 'Other': ['Other'],\n",
       " 'Criticism of institutions and authorities': ['Other',\n",
       "  'Criticism of international entities',\n",
       "  'Criticism of political organizations and figures',\n",
       "  'Criticism of national governments',\n",
       "  'Criticism of the EU'],\n",
       " 'Criticism of climate movement': ['Climate movement is corrupt',\n",
       "  'Other',\n",
       "  'Climate movement is alarmist',\n",
       "  'Ad hominem attacks on key activists'],\n",
       " 'Downplaying climate change': ['Humans and nature will adapt to the changes',\n",
       "  'Other',\n",
       "  'Climate cycles are natural',\n",
       "  'Temperature increase does not have significant impact',\n",
       "  'Human activities do not impact climate change',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Ice is not melting',\n",
       "  'Sea levels are not rising',\n",
       "  'CO2 concentrations are too small to have an impact'],\n",
       " 'Criticism of climate policies': ['Climate policies have negative impact on the economy',\n",
       "  'Other',\n",
       "  'Climate policies are ineffective',\n",
       "  'Climate policies are only for profit'],\n",
       " 'Questioning the measurements and science': ['Data shows no temperature increase',\n",
       "  'Other',\n",
       "  'Methodologies/metrics used are unreliable/faulty',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Scientific community is unreliable'],\n",
       " 'Climate change is beneficial': ['Other',\n",
       "  'CO2 is beneficial',\n",
       "  'Temperature increase is beneficial'],\n",
       " 'Controversy about green technologies': ['Renewable energy is dangerous',\n",
       "  'Renewable energy is costly',\n",
       "  'Other',\n",
       "  'Renewable energy is unreliable'],\n",
       " 'Green policies are geopolitical instruments': ['Green activities are a form of neo-colonialism',\n",
       "  'Climate-related international relations are abusive/exploitative',\n",
       "  'Other']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6912756b-47f2-4a11-b782-71f86ee25137",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abdb04-673c-4fcd-8282-e1ec57a4b090",
   "metadata": {},
   "source": [
    "Finally, we get our embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082080ec-bfbf-4db2-8363-2b06413a3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/all_embeddings.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "embeddings_kalm = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a160a-e449-4dc6-9943-46aea1ebfe5f",
   "metadata": {},
   "source": [
    "We also need to make sure that the embeddings stay aligned with the dataset split.  \n",
    "* We pass `all_embeddings` so that `dataset_train` and `train_embeddings` match up exactly, keeping everything consistent.\n",
    "\n",
    "We use stratified splitting here to ensure the label distribution stays the same in both the training and validation sets.  \n",
    "* This somewhat maintain the class proportions, even for the rare cases, making sure both sets are roughly representative of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3784e6e1-4100-47d5-9df1-bc1f2c2d8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def stratified_train_val_split_with_embeddings(data, embeddings, labels_column, train_size=0.8, splits=5, shuffle=True, min_instances=2):\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.arange(len(data))\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "        embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    labels = np.array(data[labels_column].tolist())\n",
    "    rare_indices = []\n",
    "    common_indices = []\n",
    "\n",
    "    class_counts = labels.sum(axis=0)\n",
    "    rare_classes = np.where(class_counts <= min_instances)[0]\n",
    "\n",
    "    for idx, label_row in enumerate(labels):\n",
    "        if any(label_row[rare_classes]):\n",
    "            rare_indices.append(idx)\n",
    "        else:\n",
    "            common_indices.append(idx)\n",
    "\n",
    "    rare_data = data.iloc[rare_indices]\n",
    "    rare_labels = labels[rare_indices]\n",
    "    rare_embeddings = embeddings[rare_indices]\n",
    "\n",
    "    train_rare = rare_data.iloc[:len(rare_data) // 2].reset_index(drop=True)\n",
    "    val_rare = rare_data.iloc[len(rare_data) // 2:].reset_index(drop=True)\n",
    "\n",
    "    train_rare_embeddings = rare_embeddings[:len(rare_data) // 2]\n",
    "    val_rare_embeddings = rare_embeddings[len(rare_data) // 2:]\n",
    "\n",
    "    common_data = data.iloc[common_indices].reset_index(drop=True)\n",
    "    common_labels = labels[common_indices]\n",
    "    common_embeddings = embeddings[common_indices]\n",
    "\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=splits)\n",
    "    for train_idx, val_idx in mskf.split(np.zeros(len(common_labels)), common_labels):\n",
    "        train_common = common_data.iloc[train_idx]\n",
    "        val_common = common_data.iloc[val_idx]\n",
    "        train_common_embeddings = common_embeddings[train_idx]\n",
    "        val_common_embeddings = common_embeddings[val_idx]\n",
    "        break\n",
    "\n",
    "    train_data = pd.concat([train_rare, train_common]).reset_index(drop=True)\n",
    "    val_data = pd.concat([val_rare, val_common]).reset_index(drop=True)\n",
    "\n",
    "    train_embeddings = np.concatenate([train_rare_embeddings, train_common_embeddings], axis=0)\n",
    "    val_embeddings = np.concatenate([val_rare_embeddings, val_common_embeddings], axis=0)\n",
    "\n",
    "    return (train_data, train_embeddings), (val_data, val_embeddings)\n",
    "\n",
    "(dataset_train, train_embeddings), (dataset_val, val_embeddings) = stratified_train_val_split_with_embeddings(\n",
    "    dataset,\n",
    "    embeddings_kalm,\n",
    "    labels_column=\"subnarratives_encoded\",\n",
    "    min_instances=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4643ce-609a-4569-be93-0e6d108f2654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d0b422-f30b-4071-935f-8d64285da203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354, 896)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad404584-76b4-4922-9a08-585d02b3a0fa",
   "metadata": {},
   "source": [
    "### 1.2 Remapping our subnarrative indices\n",
    "\n",
    "We know that our articls have many narratives, and each one maps to several subnarratives, creating a hierarchy.  \n",
    "The problem is, our `subnarratives_encoded` currently looks like a flat list of zeros:\n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
    "```\n",
    "\n",
    "But we need it to reflect the hierarchy properly:\n",
    "\n",
    "So, we break it down into a list of lists—each inner list represents the true labels for a specific hierarchy:\n",
    "\n",
    "```\n",
    "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0] , [0, 0, 0, ...\n",
    " ^ hierarchy 0       ^ hierarchy 1          ^ hierarchy 2 ...\n",
    "```\n",
    "\n",
    "This will help us significantly later when we need to know for a specific article, the true subnarrative labels for a specific hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae17fbfc-15ac-421b-ad18-2d010c07bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                              ...                        \n",
       "1694    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1695    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "1696    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "1697    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1698    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: subnarratives_encoded, Length: 1699, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['subnarratives_encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceea6d4-5a51-43f9-b7da-0977b7013a82",
   "metadata": {},
   "source": [
    "We’re using the label encoders to get the indices of narratives and subnarratives, which we’ll use later.  \n",
    "* For each narrative in `narrative_to_subnarratives`, we find the index of the narrative and its corresponding subnarratives using the encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3d9def-3cf1-43e4-8796-aa0f5c1bc415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: [33, 66, 50, 64, 20, 39, 22, 65, 21], 9: [33, 71, 60, 56, 53, 19, 58], 17: [33, 34, 35, 41, 46, 42], 19: [40, 33, 63, 59], 10: [33, 69, 72], 1: [33, 43, 31, 3, 62], 16: [55, 33, 57, 32], 2: [54, 67, 33], 20: [44, 33, 45, 68], 13: [2, 33, 6], 14: [47, 33, 61], 0: [33, 1, 24, 73, 23], 15: [33], 7: [33, 14, 16, 15, 17], 5: [9, 33, 8, 0], 11: [28, 33, 7, 51, 27, 70, 29, 49, 4], 6: [12, 33, 10, 11], 18: [18, 33, 30, 26, 48], 3: [33, 5, 52], 4: [37, 36, 33, 38], 12: [25, 13, 33]}\n"
     ]
    }
   ],
   "source": [
    "narrative_to_sub_map = {}\n",
    "narrative_classes = list(mlb_narratives.classes_)\n",
    "subnarrative_classes = list(mlb_subnarratives.classes_)\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    narrative_idx = narrative_classes.index(narrative)\n",
    "    subnarrative_indices = [subnarrative_classes.index(sub) for sub in subnarratives]\n",
    "    narrative_to_sub_map[narrative_idx] = subnarrative_indices\n",
    "\n",
    "print(narrative_to_sub_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5f15e-2931-47ea-9c91-c663452555f0",
   "metadata": {},
   "source": [
    "Now, we remap the `subnarratives_encoded` list to reflect the correct hierarchy for each article.  \n",
    "* For each narrative, we grab its corresponding subnarrative indices from `narrative_to_sub_map` and assign the sublabels to the appropriate hierarchy column.  \n",
    "\n",
    "This will give us a new set of columns where each one contains the true subnarrative labels for that narrative hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55bd5645-8fdc-44fc-b920-f5015865e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_new_column_name = \"narrative_hierarchy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7878d2ae-83ed-4153-8697-b34f22fb60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_subnarratives(row, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row and encodes the current subnarrative list to the associated hierarchy based on the narr-subnar map\"\"\"\n",
    "    for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "        sub_labels = [row['subnarratives_encoded'][sub_idx] for sub_idx in sub_indices]\n",
    "        col_name = f\"{hierarchy_new_column_name}_{narr_idx}\"\n",
    "        row[col_name] = sub_labels\n",
    "    return row\n",
    "\n",
    "dataset_train_cpy = dataset_train.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1f312-8926-4239-b288-31f081c10262",
   "metadata": {},
   "source": [
    "We do the same for validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49192dc3-7a83-422a-85ea-dc462ebc8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_cpy = dataset_val.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe99263-8222-40b9-a76c-e4781ffe0e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>narrative_hierarchy_8</th>\n",
       "      <th>narrative_hierarchy_9</th>\n",
       "      <th>narrative_hierarchy_17</th>\n",
       "      <th>...</th>\n",
       "      <th>narrative_hierarchy_0</th>\n",
       "      <th>narrative_hierarchy_15</th>\n",
       "      <th>narrative_hierarchy_7</th>\n",
       "      <th>narrative_hierarchy_5</th>\n",
       "      <th>narrative_hierarchy_11</th>\n",
       "      <th>narrative_hierarchy_6</th>\n",
       "      <th>narrative_hierarchy_18</th>\n",
       "      <th>narrative_hierarchy_3</th>\n",
       "      <th>narrative_hierarchy_4</th>\n",
       "      <th>narrative_hierarchy_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_CC_416.txt</td>\n",
       "      <td>&lt;PARA&gt;da patranha do aquecimento global&lt;/PARA&gt;...</td>\n",
       "      <td>[Questioning the measurements and science, Hid...</td>\n",
       "      <td>[Scientific community is unreliable, Climate a...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 1, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 1]</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_200022.txt</td>\n",
       "      <td>&lt;PARA&gt;Denmark to Punish Farmers for cow ‘emiss...</td>\n",
       "      <td>[Criticism of institutions and authorities, Cr...</td>\n",
       "      <td>[Criticism of national governments, Other, Met...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 1, 0, 1, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG</td>\n",
       "      <td>A9_BG_4016.txt</td>\n",
       "      <td>&lt;PARA&gt;британски допломат потресен от политикат...</td>\n",
       "      <td>[Discrediting the West, Diplomacy, Blaming the...</td>\n",
       "      <td>[Other, The West are the aggressors]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HI</td>\n",
       "      <td>HI_112.txt</td>\n",
       "      <td>&lt;PARA&gt;टैंकों और बख्तरबंद वाहनों के साथ रूसी इल...</td>\n",
       "      <td>[Russia is the Victim, Blaming the war on othe...</td>\n",
       "      <td>[Russia actions in Ukraine are only self-defen...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_393.txt</td>\n",
       "      <td>&lt;PARA&gt;estudante que atirou tinta a montenegro ...</td>\n",
       "      <td>[Criticism of institutions and authorities, Am...</td>\n",
       "      <td>[Criticism of political organizations and figu...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 0, 1, 1, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  language        article_id  \\\n",
       "0       PT     PT_CC_416.txt   \n",
       "1       EN  EN_CC_200022.txt   \n",
       "2       BG    A9_BG_4016.txt   \n",
       "3       HI        HI_112.txt   \n",
       "4       PT        PT_393.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>da patranha do aquecimento global</PARA>...   \n",
       "1  <PARA>Denmark to Punish Farmers for cow ‘emiss...   \n",
       "2  <PARA>британски допломат потресен от политикат...   \n",
       "3  <PARA>टैंकों और बख्तरबंद वाहनों के साथ रूसी इल...   \n",
       "4  <PARA>estudante que atirou tinta a montenegro ...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [Questioning the measurements and science, Hid...   \n",
       "1  [Criticism of institutions and authorities, Cr...   \n",
       "2  [Discrediting the West, Diplomacy, Blaming the...   \n",
       "3  [Russia is the Victim, Blaming the war on othe...   \n",
       "4  [Criticism of institutions and authorities, Am...   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [Scientific community is unreliable, Climate a...   \n",
       "1  [Criticism of national governments, Other, Met...   \n",
       "2               [Other, The West are the aggressors]   \n",
       "3  [Russia actions in Ukraine are only self-defen...   \n",
       "4  [Criticism of political organizations and figu...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "         narrative_hierarchy_8  narrative_hierarchy_9 narrative_hierarchy_17  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 0, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 0, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 0, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "3  [1, 0, 0, 0, 1, 0, 0, 0, 0]  [1, 0, 1, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0]  [1, 0, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   ... narrative_hierarchy_0 narrative_hierarchy_15 narrative_hierarchy_7  \\\n",
       "0  ...       [1, 0, 0, 0, 0]                    [1]       [1, 0, 0, 0, 0]   \n",
       "1  ...       [1, 0, 0, 0, 0]                    [1]       [1, 1, 0, 1, 0]   \n",
       "2  ...       [1, 0, 0, 0, 0]                    [1]       [1, 0, 0, 0, 0]   \n",
       "3  ...       [1, 0, 0, 0, 0]                    [1]       [1, 0, 0, 0, 0]   \n",
       "4  ...       [1, 0, 0, 0, 0]                    [1]       [1, 0, 1, 1, 0]   \n",
       "\n",
       "  narrative_hierarchy_5       narrative_hierarchy_11 narrative_hierarchy_6  \\\n",
       "0          [0, 1, 1, 0]  [0, 1, 0, 0, 0, 0, 1, 1, 0]          [0, 1, 0, 0]   \n",
       "1          [0, 1, 0, 0]  [0, 1, 0, 0, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "2          [0, 1, 0, 0]  [0, 1, 0, 0, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "3          [0, 1, 0, 0]  [0, 1, 0, 0, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "4          [0, 1, 0, 0]  [0, 1, 0, 0, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_18 narrative_hierarchy_3 narrative_hierarchy_4  \\\n",
       "0        [0, 1, 0, 0, 1]             [1, 0, 1]          [0, 0, 1, 0]   \n",
       "1        [0, 1, 1, 1, 0]             [1, 0, 0]          [0, 0, 1, 0]   \n",
       "2        [0, 1, 0, 0, 0]             [1, 0, 0]          [0, 0, 1, 0]   \n",
       "3        [0, 1, 0, 0, 0]             [1, 0, 0]          [0, 0, 1, 0]   \n",
       "4        [0, 1, 0, 0, 0]             [1, 0, 0]          [0, 0, 1, 0]   \n",
       "\n",
       "  narrative_hierarchy_12  \n",
       "0              [0, 0, 1]  \n",
       "1              [0, 0, 1]  \n",
       "2              [0, 0, 1]  \n",
       "3              [0, 0, 1]  \n",
       "4              [0, 0, 1]  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_cpy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668fc07-6d29-442f-bf4d-01c2fc595ca0",
   "metadata": {},
   "source": [
    "A sample result looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed68aa1-856d-4e76-a332-5640670bf9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of narrative_hierarchy_8:\n",
      "0    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_8, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_9:\n",
      "0    [0, 0, 0, 0, 0, 0, 0]\n",
      "1    [1, 0, 0, 0, 0, 0, 0]\n",
      "2    [1, 0, 0, 0, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_9, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_17:\n",
      "0    [0, 0, 0, 0, 0, 0]\n",
      "1    [1, 0, 0, 0, 0, 0]\n",
      "2    [1, 0, 0, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_17, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_19:\n",
      "0    [0, 0, 0, 0]\n",
      "1    [0, 1, 0, 0]\n",
      "2    [0, 1, 0, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_19, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_10:\n",
      "0    [0, 0, 0]\n",
      "1    [1, 0, 0]\n",
      "2    [1, 0, 0]\n",
      "3    [0, 0, 0]\n",
      "4    [1, 0, 0]\n",
      "Name: narrative_hierarchy_10, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_1:\n",
      "0    [0, 0, 0, 0, 0]\n",
      "1    [1, 0, 0, 0, 0]\n",
      "2    [1, 0, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_1, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_16:\n",
      "0    [0, 0, 0, 0]\n",
      "1    [0, 1, 0, 0]\n",
      "2    [0, 1, 0, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_16, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_2:\n",
      "0    [0, 0, 0]\n",
      "1    [0, 0, 1]\n",
      "2    [0, 0, 1]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 1]\n",
      "Name: narrative_hierarchy_2, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_20:\n",
      "0    [0, 0, 0, 0]\n",
      "1    [0, 1, 0, 0]\n",
      "2    [0, 1, 0, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_20, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_13:\n",
      "0    [0, 0, 0]\n",
      "1    [1, 1, 0]\n",
      "2    [0, 1, 0]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 1, 0]\n",
      "Name: narrative_hierarchy_13, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_14:\n",
      "0    [0, 0, 0]\n",
      "1    [0, 1, 0]\n",
      "2    [0, 1, 0]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 1, 0]\n",
      "Name: narrative_hierarchy_14, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_0:\n",
      "0    [0, 0, 0, 0, 0]\n",
      "1    [1, 0, 0, 0, 0]\n",
      "2    [1, 0, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_0, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_15:\n",
      "0    [0]\n",
      "1    [1]\n",
      "2    [1]\n",
      "3    [0]\n",
      "4    [1]\n",
      "Name: narrative_hierarchy_15, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_7:\n",
      "0    [0, 1, 1, 0, 0]\n",
      "1    [1, 0, 1, 0, 0]\n",
      "2    [1, 0, 1, 1, 0]\n",
      "3    [0, 1, 1, 1, 0]\n",
      "4    [1, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_7, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_5:\n",
      "0    [0, 0, 0, 0]\n",
      "1    [0, 1, 0, 0]\n",
      "2    [0, 1, 0, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_5, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_11:\n",
      "0    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1    [0, 1, 0, 0, 0, 0, 0, 1, 1]\n",
      "2    [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_11, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_6:\n",
      "0    [0, 0, 0, 0]\n",
      "1    [0, 1, 0, 0]\n",
      "2    [1, 1, 1, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_6, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_18:\n",
      "0    [0, 0, 1, 1, 0]\n",
      "1    [0, 1, 0, 0, 1]\n",
      "2    [0, 1, 0, 0, 0]\n",
      "3    [0, 0, 0, 0, 0]\n",
      "4    [0, 1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_18, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_3:\n",
      "0    [0, 0, 0]\n",
      "1    [1, 0, 0]\n",
      "2    [1, 0, 0]\n",
      "3    [0, 0, 0]\n",
      "4    [1, 0, 0]\n",
      "Name: narrative_hierarchy_3, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_4:\n",
      "0    [0, 0, 0, 0]\n",
      "1    [0, 0, 1, 0]\n",
      "2    [0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 0, 1, 0]\n",
      "Name: narrative_hierarchy_4, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_12:\n",
      "0    [0, 0, 0]\n",
      "1    [0, 0, 1]\n",
      "2    [1, 0, 1]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 1]\n",
      "Name: narrative_hierarchy_12, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    dataset_hierarchy_col_name = f\"{hierarchy_new_column_name}_{narr_idx}\"\n",
    "    res = dataset_train_cpy[dataset_hierarchy_col_name]\n",
    "    print(f\"Sample of {dataset_hierarchy_col_name}:\")\n",
    "    print(res.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed4fea0-adaf-4bce-b35f-aed1a9ce7f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order = sorted(narrative_to_sub_map.keys())\n",
    "narrative_order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7528588-b7d7-4f5c-8962-65ee13741fcb",
   "metadata": {},
   "source": [
    "Now we want to make sure that the true subnarratives for hierarchy 0 are in position 0 of the aggregated list, hierarchy 1 in position 1, and so on.  \n",
    "This ensures the subnarratives are ordered correctly in the final, aggregated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d3f065-4abf-422c-b85d-c6879a3c157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subnarratives(row, narrative_order, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row, and aggregates all hierarchy columns to 1 list.\n",
    "    The encoded list will be a list of lists, starting from the first hierarchy\"\"\"\n",
    "    aggregated = []\n",
    "    for narr_idx in narrative_order:\n",
    "        column_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "        sub_labels = row[column_name]\n",
    "        aggregated.append(sub_labels)\n",
    "    return aggregated\n",
    "\n",
    "dataset_train['aggregated_subnarratives'] = dataset_train_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")\n",
    "\n",
    "dataset_val['aggregated_subnarratives'] = dataset_val_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e76fa06-fa05-4b24-ab16-2862f7df659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1       [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "2       [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "3       [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "4       [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "                              ...                        \n",
       "1349    [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "1350    [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "1351    [[0, 1, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1352    [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "1353    [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "Name: aggregated_subnarratives, Length: 1354, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['aggregated_subnarratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1968bf35-e93b-465e-b0a2-742953b808bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee760da7-ecad-4690-8c05-46c33b447ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff6872b-c06e-4450-b232-7d812eff12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3ee30-c656-4901-b83e-3facd4b0a3ba",
   "metadata": {},
   "source": [
    "Now we have a model with a shared layer that captures the general features of the article.  \n",
    "* The model was finalised after a lot of experimentaions the BatchNorm + ReLU combo significantly improves performance by stabilizing training and speeding up convergence.\n",
    "* Also, it seems like the model overfits very quickly when becoming overly complex.\n",
    "  \n",
    "We make predictions for the top-level narratives, followed by separate subnarrative predictions for each narrative.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c3acef3-fa00-4d9e-bec9-0f9cacd9fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=1024,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20f442d8-dc42-471e-b7b5-6374fb7948be",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 512,\n",
    "    'dropout': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71b02d39-8af6-4cae-bde3-b82a663fbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head = MultiTaskClassifierMultiHead(\n",
    "    input_size=input_size,\n",
    "    hidden_size=network_params['hidden_size'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae2c7c9f-e40e-4594-85ec-116e77958ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiTaskClassifierMultiHead(\n",
      "  (shared_layer): Sequential(\n",
      "    (0): Linear(in_features=896, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (narrative_head): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=21, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (subnarrative_heads): ModuleDict(\n",
      "    (8): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=7, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=6, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_multi_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bfde290-d982-49fa-a89a-b48bb81a1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "y_val_nar = dataset_val['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n",
    "y_val_sub_nar = dataset_val['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3c347-5e6a-4c3a-835d-e1f2cb9a2440",
   "metadata": {},
   "source": [
    "We move everything to a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18b1217b-9e4b-4fcc-8cb1-4274f4bdd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32)\n",
    "\n",
    "y_val_nar = torch.tensor(y_val_nar, dtype=torch.float32)\n",
    "y_val_sub_nar = torch.tensor(y_val_sub_nar, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1b730da-7939-4cdd-b725-a95d84624222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeef867-9a58-4f41-b770-4dc50151a4be",
   "metadata": {},
   "source": [
    "We calculate class weights to handle label imbalance in the training data. \n",
    "* This way, rare labels are given higher importance to ensure the model learns them effectively.\n",
    "* The custom ```WeightedBCELoss``` applies these weights during training to balance the impact of common and rare labels, preventing the model from focusing only on frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a10fe2bc-989e-40f5-9779-14d70ec69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)\n",
    "\n",
    "class_weights_sub_nar = compute_class_weights(y_val_sub_nar)\n",
    "class_weights_nar = compute_class_weights(y_val_nar)\n",
    "narrative_criterion = WeightedBCELoss(class_weights_nar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3da05-7d9a-4486-a4c3-2f899e1af47f",
   "metadata": {},
   "source": [
    "We create a separate loss function for each hierarchy of subnarratives to handle their specific class imbalance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b8e8505-b560-4307-85a3-7ef738cfb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_criterion_dict = {}\n",
    "\n",
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    local_weights = [ class_weights_sub_nar[sub_i] for sub_i in sub_indices ]\n",
    "\n",
    "    sub_criterion = WeightedBCELoss(local_weights)\n",
    "    sub_criterion_dict[str(narr_idx)] = sub_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f1ac5-fd61-441a-81db-f2b960b07427",
   "metadata": {},
   "source": [
    "We define a ```ConditionalLoss``` class to handle the multi-task loss calculation.\n",
    "\n",
    "* In the forward method, we first calculate the loss for the top-level narratives using the narrative criterion.\n",
    "We then loop through each subnarrative head and compute the loss for each one, based on its specific subnarrative labels\n",
    "\n",
    "\n",
    "* We introduce a conditioning term that penalizes inconsistencies between narrative and subnarrative predictions.\n",
    "Finally, we combine the narrative loss, sub-loss, and conditioning loss to get the total loss, which is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3863a1f5-eb9e-4247-9e0d-405799431c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=0.6, sub_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        \n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "        \n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            \n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_loss += sub_loss_func(sub_probs, y_sub_tensor)\n",
    "\n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.mean(\n",
    "                # Penalize high probs of sub, based on first level narr predictinos\n",
    "                torch.abs(sub_probs * (1 - narr_pred)) + \n",
    "                # If a narrative is true, then the subnarrative predictions should match their actual true values.\n",
    "                narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            )\n",
    "            condition_loss += condition_term\n",
    "            \n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        \n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + \\\n",
    "                    self.sub_weight * sub_loss + \\\n",
    "                    self.condition_weight * condition_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2e738b9-7d0b-4816-8307-e575297e4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_loss_fn = MultiHeadLoss(narrative_criterion, sub_criterion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1cd8bb5-2850-447f-a899-e11001466723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weight_norms(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            norm = param.norm(2).item()\n",
    "            print(f\"Layer: {name} | Weight Norm: {norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d07790-5568-40ce-bdc9-38f0282b0598",
   "metadata": {},
   "source": [
    "We define the function for training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f7c1e38-c06b-4285-9d46-0c338ce8c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn=multi_head_loss_fn,\n",
    "    train_embeddings=train_embeddings_tensor,\n",
    "    y_train_nar=y_train_nar,\n",
    "    y_train_sub_heads=y_train_sub_heads,\n",
    "    val_embeddings=val_embeddings_tensor,\n",
    "    y_val_nar=y_val_nar,\n",
    "    y_val_sub_heads=y_val_sub_heads,\n",
    "    patience=5,\n",
    "    num_epochs=100,\n",
    "    scheduler=None,\n",
    "    min_delta=0.001\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "        train_loss = loss_fn(train_narr_probs, train_sub_probs_dict, y_train_nar, y_train_sub_heads)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            val_loss = loss_fn(val_narr_probs, val_sub_probs_dict, y_val_nar, y_val_sub_heads)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {train_loss.item():.4f}, \"\n",
    "              f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        if val_loss.item() < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not significantly improve for {patience_counter} epoch(s).\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec5cf11-220e-48db-be7f-d3ce7aa5e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_multi_head = torch.optim.AdamW(model_multi_head.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a450be-6a3b-4100-9177-bcc2ff8dfda7",
   "metadata": {},
   "source": [
    "We will also initialize a scheduler to adjust the learning rate dynamically during training based on how the model is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7119e042-d647-4491-9e70-f8815ee0134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_multi_head, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a501a7b4-6d0c-4c3f-bf4b-33015a1b73bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.0249, Validation Loss: 0.9849\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Training Loss: 0.8745, Validation Loss: 0.9799\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Training Loss: 0.7871, Validation Loss: 0.9748\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Training Loss: 0.7228, Validation Loss: 0.9698\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Training Loss: 0.6734, Validation Loss: 0.9648\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Training Loss: 0.6298, Validation Loss: 0.9595\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Training Loss: 0.5975, Validation Loss: 0.9537\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Training Loss: 0.5706, Validation Loss: 0.9474\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Training Loss: 0.5436, Validation Loss: 0.9405\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Training Loss: 0.5230, Validation Loss: 0.9331\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Training Loss: 0.5039, Validation Loss: 0.9251\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Training Loss: 0.4853, Validation Loss: 0.9166\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Training Loss: 0.4687, Validation Loss: 0.9077\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Training Loss: 0.4561, Validation Loss: 0.8984\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Training Loss: 0.4424, Validation Loss: 0.8890\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Training Loss: 0.4306, Validation Loss: 0.8795\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Training Loss: 0.4206, Validation Loss: 0.8702\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Training Loss: 0.4090, Validation Loss: 0.8612\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Training Loss: 0.4003, Validation Loss: 0.8520\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Training Loss: 0.3902, Validation Loss: 0.8427\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Training Loss: 0.3828, Validation Loss: 0.8334\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Training Loss: 0.3730, Validation Loss: 0.8239\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Training Loss: 0.3660, Validation Loss: 0.8149\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Training Loss: 0.3589, Validation Loss: 0.8061\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Training Loss: 0.3507, Validation Loss: 0.7967\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Training Loss: 0.3422, Validation Loss: 0.7871\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Training Loss: 0.3378, Validation Loss: 0.7782\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Training Loss: 0.3304, Validation Loss: 0.7698\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Training Loss: 0.3243, Validation Loss: 0.7614\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Training Loss: 0.3195, Validation Loss: 0.7528\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Training Loss: 0.3142, Validation Loss: 0.7443\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Training Loss: 0.3070, Validation Loss: 0.7363\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Training Loss: 0.3021, Validation Loss: 0.7289\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Training Loss: 0.2963, Validation Loss: 0.7220\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Training Loss: 0.2915, Validation Loss: 0.7153\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Training Loss: 0.2869, Validation Loss: 0.7087\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 37/100, Training Loss: 0.2806, Validation Loss: 0.7028\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 38/100, Training Loss: 0.2757, Validation Loss: 0.6982\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 39/100, Training Loss: 0.2720, Validation Loss: 0.6942\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Training Loss: 0.2684, Validation Loss: 0.6906\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 41/100, Training Loss: 0.2614, Validation Loss: 0.6871\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 42/100, Training Loss: 0.2574, Validation Loss: 0.6821\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 43/100, Training Loss: 0.2539, Validation Loss: 0.6780\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 44/100, Training Loss: 0.2500, Validation Loss: 0.6760\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 45/100, Training Loss: 0.2454, Validation Loss: 0.6751\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 46/100, Training Loss: 0.2412, Validation Loss: 0.6751\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 47/100, Training Loss: 0.2379, Validation Loss: 0.6771\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 48/100, Training Loss: 0.2344, Validation Loss: 0.6808\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Epoch 49/100, Training Loss: 0.2295, Validation Loss: 0.6875\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Epoch 50/100, Training Loss: 0.2255, Validation Loss: 0.6880\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Epoch 51/100, Training Loss: 0.2253, Validation Loss: 0.6868\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Epoch 52/100, Training Loss: 0.2210, Validation Loss: 0.6859\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Epoch 53/100, Training Loss: 0.2203, Validation Loss: 0.6872\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Epoch 54/100, Training Loss: 0.2170, Validation Loss: 0.6889\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiTaskClassifierMultiHead(\n",
       "  (shared_layer): Sequential(\n",
       "    (0): Linear(in_features=896, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (narrative_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=21, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (subnarrative_heads): ModuleDict(\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=6, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_multihead(\n",
    "    model=model_multi_head,\n",
    "    optimizer=optimizer_multi_head,\n",
    "    scheduler=scheduler,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f2674d8-56bd-4387-bec9-d87cf92fcead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResults:\n",
    "    best_threshold: float\n",
    "    best_f1: float\n",
    "    narrative_report: str\n",
    "    subnarrative_report: str\n",
    "    narrative_predictions: np.ndarray\n",
    "    subnarrative_predictions: np.ndarray\n",
    "\n",
    "class MultiHeadEvaluator:    \n",
    "    def __init__(\n",
    "        self,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        num_subnarratives=len(mlb_subnarratives.classes_),\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.num_subnarratives = num_subnarratives\n",
    "        self.narrative_classes = narrative_classes\n",
    "        self.subnarrative_classes = subnarrative_classes\n",
    "        self.device = device\n",
    "\n",
    "    def _flatten_subnarratives(self, y_sub_hierarchical):\n",
    "        \"\"\"Reconstruct flattened subnarrative array from hierarchical structure.\"\"\"\n",
    "        num_samples = len(y_sub_hierarchical)\n",
    "        sub_global_array = np.zeros((num_samples, self.num_subnarratives), dtype=int)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            for j, narr_idx in enumerate(self.narrative_order):\n",
    "                sub_label_vec = y_sub_hierarchical[i][j]\n",
    "                narr_idx = int(narr_idx)\n",
    "                sub_indices = self.narrative_to_sub_map[narr_idx]\n",
    "                for local_sub_i, global_sub_i in enumerate(sub_indices):\n",
    "                    sub_global_array[i, global_sub_i] = sub_label_vec[local_sub_i]\n",
    "\n",
    "        return sub_global_array\n",
    "\n",
    "    def visualize_results(self, results):\n",
    "        print(f\"\\nBest Threshold: {results.best_threshold:.2f}\")\n",
    "        print(f\"Best Average F1: {results.best_f1:.4f}\")\n",
    "        print(\"\\nNarrative Classification Report:\")\n",
    "        print(results.narrative_report)\n",
    "        print(\"\\nSubnarrative Classification Report:\")\n",
    "        print(results.subnarrative_report)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings=val_embeddings_tensor,\n",
    "        y_nar_true=y_val_nar,\n",
    "        y_sub_hierarchical=y_val_sub_heads,\n",
    "        thresholds=None,\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "        y_nar_true_np = y_nar_true.cpu().numpy()\n",
    "        \n",
    "        embeddings = embeddings.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "\n",
    "        best_threshold = 0\n",
    "        best_f1 = -1\n",
    "        best_nar_report = \"\"\n",
    "        best_sub_report = \"\"\n",
    "        best_nar_preds = None\n",
    "        best_sub_preds = None\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            narr_preds = (narr_probs >= threshold).astype(int)\n",
    "            samples = len(narr_probs)\n",
    "            sub_preds_flattened = np.zeros((samples, self.num_subnarratives), dtype=int)\n",
    "\n",
    "            for narr_idx, sub_indices in self.narrative_to_sub_map.items():\n",
    "                sub_probs_for_narr = sub_probs_dict[str(narr_idx)]\n",
    "                predicted_narr_mask = narr_preds[:, narr_idx] == 1\n",
    "                sub_preds_for_narr = (sub_probs_for_narr >= threshold).astype(int)\n",
    "\n",
    "                for sample_idx in range(samples):\n",
    "                    if predicted_narr_mask[sample_idx]:\n",
    "                        for local_sub_i, global_sub_i in enumerate(sub_indices):\n",
    "                            sub_preds_flattened[sample_idx, global_sub_i] = sub_preds_for_narr[sample_idx, local_sub_i]\n",
    "\n",
    "            f1_nar = f1_score(y_nar_true_np, narr_preds, average=\"macro\", zero_division=0)\n",
    "            y_sub_true_np = self._flatten_subnarratives(y_sub_hierarchical)\n",
    "            f1_sub = f1_score(y_sub_true_np, sub_preds_flattened, average=\"macro\", zero_division=0)\n",
    "            avg_f1 = (f1_nar + f1_sub) / 2.0\n",
    "\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                best_threshold = threshold\n",
    "                best_nar_preds = narr_preds\n",
    "                best_sub_preds = sub_preds_flattened\n",
    "                best_nar_report = classification_report(\n",
    "                    y_nar_true_np, narr_preds,\n",
    "                    target_names=self.narrative_classes,\n",
    "                    zero_division=0\n",
    "                )\n",
    "                best_sub_report = classification_report(\n",
    "                    y_sub_true_np, sub_preds_flattened,\n",
    "                    target_names=self.subnarrative_classes,\n",
    "                    zero_division=0\n",
    "                )\n",
    "\n",
    "        results = EvaluationResults(\n",
    "            best_threshold=best_threshold,\n",
    "            best_f1=best_f1,\n",
    "            narrative_report=best_nar_report,\n",
    "            subnarrative_report=best_sub_report,\n",
    "            narrative_predictions=best_nar_preds,\n",
    "            subnarrative_predictions=best_sub_preds\n",
    "        )\n",
    "        \n",
    "        self.visualize_results(results)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8491b49d-185e-4ef0-b330-09842c828a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultiHeadEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73460588-6d9f-4392-98d4-a14c9b9f1cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Threshold: 0.50\n",
      "Best Average F1: 0.3699\n",
      "\n",
      "Narrative Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                         Amplifying Climate Fears       0.70      0.96      0.81        46\n",
      "                     Amplifying war-related fears       0.69      0.72      0.71        47\n",
      "Blaming the war on others rather than the invader       0.31      0.62      0.41        32\n",
      "                     Climate change is beneficial       0.50      0.25      0.33         4\n",
      "             Controversy about green technologies       0.36      1.00      0.53         4\n",
      "                    Criticism of climate movement       0.41      0.82      0.55        11\n",
      "                    Criticism of climate policies       0.36      0.57      0.44        21\n",
      "        Criticism of institutions and authorities       0.48      0.86      0.62        28\n",
      "                             Discrediting Ukraine       0.67      0.80      0.73        79\n",
      "                 Discrediting the West, Diplomacy       0.64      0.67      0.65        69\n",
      "                           Distrust towards Media       0.38      0.33      0.35         9\n",
      "                       Downplaying climate change       0.42      0.80      0.55        10\n",
      "      Green policies are geopolitical instruments       0.00      0.00      0.00         2\n",
      "Hidden plots by secret schemes of powerful groups       0.35      0.40      0.38        15\n",
      "               Negative Consequences for the West       0.27      0.41      0.33        17\n",
      "                                            Other       0.61      0.67      0.64        67\n",
      "                            Overpraising the West       0.00      0.00      0.00         9\n",
      "                                 Praise of Russia       0.55      0.66      0.60        67\n",
      "         Questioning the measurements and science       0.57      0.67      0.62         6\n",
      "                             Russia is the Victim       0.38      0.56      0.46        41\n",
      "                         Speculating war outcomes       0.45      0.47      0.46        30\n",
      "\n",
      "                                        micro avg       0.52      0.67      0.59       614\n",
      "                                        macro avg       0.43      0.58      0.48       614\n",
      "                                     weighted avg       0.53      0.67      0.59       614\n",
      "                                      samples avg       0.55      0.68      0.58       614\n",
      "\n",
      "\n",
      "Subnarrative Classification Report:\n",
      "                                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                   Ad hominem attacks on key activists       0.14      0.25      0.18         4\n",
      "                           Amplifying existing fears of global warming       0.67      0.97      0.79        35\n",
      "                                                 Blaming global elites       0.17      0.33      0.22         3\n",
      "                                   By continuing the war we risk WWIII       0.33      0.42      0.37        12\n",
      "                    CO2 concentrations are too small to have an impact       0.00      0.00      0.00         1\n",
      "                                                     CO2 is beneficial       0.00      0.00      0.00         2\n",
      "                                     Climate agenda has hidden motives       0.38      0.43      0.40         7\n",
      "                                            Climate cycles are natural       0.18      0.67      0.29         3\n",
      "                                          Climate movement is alarmist       0.42      0.83      0.56         6\n",
      "                                           Climate movement is corrupt       0.50      0.33      0.40         3\n",
      "                                      Climate policies are ineffective       0.15      0.29      0.20         7\n",
      "                                  Climate policies are only for profit       0.11      0.33      0.17         3\n",
      "                  Climate policies have negative impact on the economy       0.23      0.33      0.27         9\n",
      "      Climate-related international relations are abusive/exploitative       0.00      0.00      0.00         1\n",
      "                                   Criticism of international entities       0.50      0.67      0.57         6\n",
      "                                     Criticism of national governments       0.50      0.75      0.60        20\n",
      "                      Criticism of political organizations and figures       0.38      0.86      0.52        14\n",
      "                                                   Criticism of the EU       0.33      0.50      0.40         2\n",
      "                                    Data shows no temperature increase       0.00      0.00      0.00         1\n",
      "                                          Diplomacy does/will not work       0.29      0.25      0.27         8\n",
      "          Discrediting Ukrainian government and officials and policies       0.45      0.65      0.53        31\n",
      "                                       Discrediting Ukrainian military       0.41      0.65      0.50        20\n",
      "                             Discrediting Ukrainian nation and society       0.00      0.00      0.00         2\n",
      "                                         Doomsday scenarios for humans       0.25      0.62      0.36         8\n",
      "                                      Earth will be uninhabitable soon       0.28      1.00      0.43         5\n",
      "                        Green activities are a form of neo-colonialism       0.00      0.00      0.00         1\n",
      "          Greenhouse effect/carbon dioxide do not drive climate change       0.00      0.00      0.00         1\n",
      "                         Human activities do not impact climate change       0.00      0.00      0.00         2\n",
      "                           Humans and nature will adapt to the changes       0.00      0.00      0.00         1\n",
      "                                                    Ice is not melting       0.00      0.00      0.00         1\n",
      "                      Methodologies/metrics used are unreliable/faulty       1.00      0.67      0.80         3\n",
      "                                   NATO should/will directly intervene       0.00      0.00      0.00         6\n",
      "                                              NATO will destroy Russia       0.00      0.00      0.00         1\n",
      "                                                                 Other       0.77      0.59      0.66       186\n",
      "                            Praise of Russian President Vladimir Putin       0.50      0.50      0.50         8\n",
      "                                      Praise of Russian military might       0.45      0.64      0.53        28\n",
      "                                            Renewable energy is costly       0.33      0.33      0.33         3\n",
      "                                         Renewable energy is dangerous       0.00      0.00      0.00         1\n",
      "                                        Renewable energy is unreliable       0.25      0.33      0.29         3\n",
      "                                           Rewriting Ukraine’s history       1.00      0.50      0.67         2\n",
      "                       Russia actions in Ukraine are only self-defence       0.30      0.50      0.38        14\n",
      "Russia has international support from a number of countries and people       0.50      0.47      0.48        15\n",
      "                         Russia is a guarantor of peace and prosperity       0.38      0.40      0.39        20\n",
      "                               Russia will also attack other countries       0.19      0.27      0.22        11\n",
      "                                            Russian army is collapsing       0.00      0.00      0.00         3\n",
      "                   Russian army will lose all the occupied territories       0.00      0.00      0.00         1\n",
      "                          Russian invasion has strong national support       0.00      0.00      0.00         1\n",
      "                  Sanctions imposed by Western countries will backfire       0.00      0.00      0.00         7\n",
      "                                    Scientific community is unreliable       0.75      0.75      0.75         4\n",
      "                                             Sea levels are not rising       0.00      0.00      0.00         1\n",
      "                                      Situation in Ukraine is hopeless       0.38      0.45      0.42        11\n",
      "                 Temperature increase does not have significant impact       0.00      0.00      0.00         2\n",
      "                                    Temperature increase is beneficial       0.00      0.00      0.00         2\n",
      "                                                     The EU is divided       0.38      0.62      0.48         8\n",
      "                                           The West are the aggressors       0.29      0.36      0.32        22\n",
      "                         The West belongs in the right side of history       0.00      0.00      0.00         4\n",
      "        The West does not care about Ukraine, only about its interests       0.23      0.18      0.20        17\n",
      "                      The West has the strongest international support       0.00      0.00      0.00         3\n",
      "                                              The West is overreacting       0.00      0.00      0.00         2\n",
      "                                               The West is russophobic       0.44      0.29      0.35        14\n",
      "                                                      The West is weak       0.29      0.17      0.21        12\n",
      "      The conflict will increase the Ukrainian refugee flows to Europe       0.00      0.00      0.00         1\n",
      "     There is a real possibility that nuclear weapons will be employed       0.60      0.63      0.62        19\n",
      "                                              UA is anti-RU extremists       0.00      0.00      0.00         3\n",
      "                              Ukraine is a hub for criminal activities       0.45      0.71      0.56         7\n",
      "                                       Ukraine is a puppet of the West       0.32      0.43      0.37        21\n",
      "                                     Ukraine is associated with nazism       0.09      0.12      0.11         8\n",
      "                                              Ukraine is the aggressor       0.37      0.62      0.47        16\n",
      "                                          Ukrainian army is collapsing       0.00      0.00      0.00         7\n",
      "                                     Ukrainian media cannot be trusted       0.00      0.00      0.00         0\n",
      "                          Weather suggests the trend is global cooling       0.00      0.00      0.00         1\n",
      "                                              West is tired of Ukraine       0.00      0.00      0.00         4\n",
      "                          Western media is an instrument of propaganda       0.60      0.38      0.46         8\n",
      "                                 Whatever we do it is already too late       0.33      0.50      0.40         2\n",
      "\n",
      "                                                             micro avg       0.44      0.50      0.47       731\n",
      "                                                             macro avg       0.24      0.30      0.26       731\n",
      "                                                          weighted avg       0.46      0.50      0.46       731\n",
      "                                                           samples avg       0.46      0.54      0.46       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.evaluate(\n",
    "    model=model_multi_head,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50daf14-7af8-4001-8530-a56385f46c8f",
   "metadata": {},
   "source": [
    "### Providing the already predicted narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd803a-e9c3-4e42-b3ef-b3001952784b",
   "metadata": {},
   "source": [
    "Let h(x) be the shared layer output for the embedding x:\n",
    "\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "We compute the probability P(narr_i | x) for each narrative:\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "Previously, we used the following formula for the subnarrative probability P(subnarr_j | x):\n",
    "\n",
    "        P(subnarr_j | x) = σ(h(x))\n",
    "\n",
    "* Where sigma is the sigmoid activation function. This means the subnarrative prediction was based only on the shared layer output.\n",
    "\n",
    "However, the new idea is to consider:\n",
    "\n",
    "        P(subnarr_j | x) = σ(concat(h(x), P(narr_i | x)))\n",
    "\n",
    "\n",
    "Where narr_i is the narrative associated with subnarrative subnarr_j in the hierarchy.\n",
    "\n",
    "* If the probability of the narrative is high, the subnarrative head will be more likely to predict the relevant subnarratives.\n",
    "* If the probability is low, the model will ignore the corresponding subnarratives.\n",
    "* At the same time, the shared output of the shared layer will help determine which subnarrative is most appropriate for the given document (and we can potentially use other techniques like attention to further improve the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00f17fed-9a17-4949-ae38-26b6aad935b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskClassifierMultiHeadConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout']\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layer\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2 + 1, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            conditioned_input = torch.cat((shared_out, narr_probs[:, int(narr_idx)].unsqueeze(1)), dim=1)\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08a1fe75-e8f8-4e34-b4ad-48db7a0f9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def initialize_and_train_model(\n",
    "    model,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=10,\n",
    "    use_scheduler=True,\n",
    "    scheduler_patience=3,\n",
    "    loss_fn=multi_head_loss_fn,\n",
    "    num_subnarratives=len(mlb_subnarratives.classes_),\n",
    "    device='cpu'\n",
    "):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=scheduler_patience)\n",
    "\n",
    "    trained_model = train_with_multihead(\n",
    "                                    model=model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    patience=patience\n",
    "                                )\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc5cb2ce-2cf7-4d50-b444-004198d25653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head_concat= MultiTaskClassifierMultiHeadConcat(\n",
    "    input_size=input_size,\n",
    "    hidden_size=network_params['hidden_size'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d1454ec-f1d3-481b-9f0c-8b37afe40c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.0219, Validation Loss: 0.9845\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Training Loss: 0.8686, Validation Loss: 0.9790\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Training Loss: 0.7851, Validation Loss: 0.9738\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Training Loss: 0.7216, Validation Loss: 0.9688\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Training Loss: 0.6703, Validation Loss: 0.9640\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Training Loss: 0.6304, Validation Loss: 0.9591\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Training Loss: 0.5972, Validation Loss: 0.9539\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Training Loss: 0.5681, Validation Loss: 0.9479\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Training Loss: 0.5427, Validation Loss: 0.9408\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Training Loss: 0.5227, Validation Loss: 0.9327\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Training Loss: 0.5037, Validation Loss: 0.9243\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Training Loss: 0.4848, Validation Loss: 0.9157\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Training Loss: 0.4689, Validation Loss: 0.9069\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Training Loss: 0.4555, Validation Loss: 0.8979\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Training Loss: 0.4414, Validation Loss: 0.8885\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Training Loss: 0.4312, Validation Loss: 0.8791\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Training Loss: 0.4206, Validation Loss: 0.8700\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Training Loss: 0.4087, Validation Loss: 0.8606\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Training Loss: 0.4004, Validation Loss: 0.8511\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Training Loss: 0.3905, Validation Loss: 0.8418\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Training Loss: 0.3827, Validation Loss: 0.8326\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Training Loss: 0.3748, Validation Loss: 0.8233\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Training Loss: 0.3671, Validation Loss: 0.8143\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Training Loss: 0.3594, Validation Loss: 0.8052\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Training Loss: 0.3525, Validation Loss: 0.7959\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Training Loss: 0.3465, Validation Loss: 0.7866\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Training Loss: 0.3389, Validation Loss: 0.7773\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Training Loss: 0.3326, Validation Loss: 0.7684\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Training Loss: 0.3273, Validation Loss: 0.7602\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Training Loss: 0.3208, Validation Loss: 0.7521\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Training Loss: 0.3159, Validation Loss: 0.7445\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Training Loss: 0.3095, Validation Loss: 0.7373\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Training Loss: 0.3046, Validation Loss: 0.7308\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Training Loss: 0.2966, Validation Loss: 0.7249\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Training Loss: 0.2933, Validation Loss: 0.7194\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Training Loss: 0.2880, Validation Loss: 0.7137\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 37/100, Training Loss: 0.2847, Validation Loss: 0.7082\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 38/100, Training Loss: 0.2785, Validation Loss: 0.7027\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 39/100, Training Loss: 0.2743, Validation Loss: 0.6974\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Training Loss: 0.2693, Validation Loss: 0.6923\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 41/100, Training Loss: 0.2660, Validation Loss: 0.6880\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 42/100, Training Loss: 0.2616, Validation Loss: 0.6846\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 43/100, Training Loss: 0.2569, Validation Loss: 0.6817\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 44/100, Training Loss: 0.2523, Validation Loss: 0.6795\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 45/100, Training Loss: 0.2482, Validation Loss: 0.6790\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 46/100, Training Loss: 0.2451, Validation Loss: 0.6798\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 47/100, Training Loss: 0.2401, Validation Loss: 0.6818\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 48/100, Training Loss: 0.2372, Validation Loss: 0.6844\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Epoch 49/100, Training Loss: 0.2321, Validation Loss: 0.6864\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Epoch 50/100, Training Loss: 0.2292, Validation Loss: 0.6860\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Epoch 51/100, Training Loss: 0.2278, Validation Loss: 0.6851\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Epoch 52/100, Training Loss: 0.2258, Validation Loss: 0.6859\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Epoch 53/100, Training Loss: 0.2216, Validation Loss: 0.6876\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Epoch 54/100, Training Loss: 0.2210, Validation Loss: 0.6880\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "trained_model_concat = initialize_and_train_model(\n",
    "    model_multi_head_concat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abd83ba2-86db-499f-b9e1-ac5167e1a89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Threshold: 0.40\n",
      "Best Average F1: 0.3741\n",
      "\n",
      "Narrative Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                         Amplifying Climate Fears       0.69      1.00      0.81        46\n",
      "                     Amplifying war-related fears       0.59      0.83      0.69        47\n",
      "Blaming the war on others rather than the invader       0.25      0.78      0.38        32\n",
      "                     Climate change is beneficial       0.50      0.50      0.50         4\n",
      "             Controversy about green technologies       0.20      1.00      0.33         4\n",
      "                    Criticism of climate movement       0.39      0.82      0.53        11\n",
      "                    Criticism of climate policies       0.36      0.76      0.49        21\n",
      "        Criticism of institutions and authorities       0.40      0.86      0.55        28\n",
      "                             Discrediting Ukraine       0.59      0.92      0.72        79\n",
      "                 Discrediting the West, Diplomacy       0.55      0.72      0.62        69\n",
      "                           Distrust towards Media       0.33      0.33      0.33         9\n",
      "                       Downplaying climate change       0.33      0.90      0.49        10\n",
      "      Green policies are geopolitical instruments       0.00      0.00      0.00         2\n",
      "Hidden plots by secret schemes of powerful groups       0.24      0.40      0.30        15\n",
      "               Negative Consequences for the West       0.24      0.59      0.34        17\n",
      "                                            Other       0.53      0.70      0.60        67\n",
      "                            Overpraising the West       0.08      0.11      0.09         9\n",
      "                                 Praise of Russia       0.45      0.73      0.56        67\n",
      "         Questioning the measurements and science       0.50      1.00      0.67         6\n",
      "                             Russia is the Victim       0.39      0.78      0.52        41\n",
      "                         Speculating war outcomes       0.34      0.57      0.42        30\n",
      "\n",
      "                                        micro avg       0.44      0.76      0.56       614\n",
      "                                        macro avg       0.38      0.68      0.47       614\n",
      "                                     weighted avg       0.47      0.76      0.57       614\n",
      "                                      samples avg       0.50      0.77      0.57       614\n",
      "\n",
      "\n",
      "Subnarrative Classification Report:\n",
      "                                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                   Ad hominem attacks on key activists       0.20      0.50      0.29         4\n",
      "                           Amplifying existing fears of global warming       0.67      1.00      0.80        35\n",
      "                                                 Blaming global elites       0.17      0.33      0.22         3\n",
      "                                   By continuing the war we risk WWIII       0.28      0.75      0.41        12\n",
      "                    CO2 concentrations are too small to have an impact       0.00      0.00      0.00         1\n",
      "                                                     CO2 is beneficial       0.00      0.00      0.00         2\n",
      "                                     Climate agenda has hidden motives       0.33      0.43      0.38         7\n",
      "                                            Climate cycles are natural       0.11      0.67      0.18         3\n",
      "                                          Climate movement is alarmist       0.36      0.83      0.50         6\n",
      "                                           Climate movement is corrupt       0.40      0.67      0.50         3\n",
      "                                      Climate policies are ineffective       0.09      0.29      0.14         7\n",
      "                                  Climate policies are only for profit       0.12      0.33      0.18         3\n",
      "                  Climate policies have negative impact on the economy       0.30      0.78      0.44         9\n",
      "      Climate-related international relations are abusive/exploitative       0.00      0.00      0.00         1\n",
      "                                   Criticism of international entities       0.36      0.67      0.47         6\n",
      "                                     Criticism of national governments       0.43      0.90      0.58        20\n",
      "                      Criticism of political organizations and figures       0.32      0.86      0.47        14\n",
      "                                                   Criticism of the EU       0.33      0.50      0.40         2\n",
      "                                    Data shows no temperature increase       0.00      0.00      0.00         1\n",
      "                                          Diplomacy does/will not work       0.12      0.25      0.16         8\n",
      "          Discrediting Ukrainian government and officials and policies       0.37      0.81      0.51        31\n",
      "                                       Discrediting Ukrainian military       0.30      0.95      0.46        20\n",
      "                             Discrediting Ukrainian nation and society       0.33      0.50      0.40         2\n",
      "                                         Doomsday scenarios for humans       0.21      0.88      0.34         8\n",
      "                                      Earth will be uninhabitable soon       0.25      1.00      0.40         5\n",
      "                        Green activities are a form of neo-colonialism       0.00      0.00      0.00         1\n",
      "          Greenhouse effect/carbon dioxide do not drive climate change       0.00      0.00      0.00         1\n",
      "                         Human activities do not impact climate change       0.00      0.00      0.00         2\n",
      "                           Humans and nature will adapt to the changes       0.00      0.00      0.00         1\n",
      "                                                    Ice is not melting       0.00      0.00      0.00         1\n",
      "                      Methodologies/metrics used are unreliable/faulty       0.50      0.67      0.57         3\n",
      "                                   NATO should/will directly intervene       0.14      0.33      0.20         6\n",
      "                                              NATO will destroy Russia       0.00      0.00      0.00         1\n",
      "                                                                 Other       0.71      0.73      0.72       186\n",
      "                            Praise of Russian President Vladimir Putin       0.29      0.62      0.40         8\n",
      "                                      Praise of Russian military might       0.31      0.64      0.41        28\n",
      "                                            Renewable energy is costly       0.40      0.67      0.50         3\n",
      "                                         Renewable energy is dangerous       0.00      0.00      0.00         1\n",
      "                                        Renewable energy is unreliable       0.33      0.67      0.44         3\n",
      "                                           Rewriting Ukraine’s history       1.00      0.50      0.67         2\n",
      "                       Russia actions in Ukraine are only self-defence       0.20      0.57      0.30        14\n",
      "Russia has international support from a number of countries and people       0.38      0.53      0.44        15\n",
      "                         Russia is a guarantor of peace and prosperity       0.30      0.65      0.41        20\n",
      "                               Russia will also attack other countries       0.24      0.64      0.35        11\n",
      "                                            Russian army is collapsing       0.33      0.67      0.44         3\n",
      "                   Russian army will lose all the occupied territories       0.00      0.00      0.00         1\n",
      "                          Russian invasion has strong national support       0.00      0.00      0.00         1\n",
      "                  Sanctions imposed by Western countries will backfire       0.20      0.14      0.17         7\n",
      "                                    Scientific community is unreliable       0.75      0.75      0.75         4\n",
      "                                             Sea levels are not rising       0.00      0.00      0.00         1\n",
      "                                      Situation in Ukraine is hopeless       0.22      0.73      0.34        11\n",
      "                 Temperature increase does not have significant impact       0.00      0.00      0.00         2\n",
      "                                    Temperature increase is beneficial       0.00      0.00      0.00         2\n",
      "                                                     The EU is divided       0.26      0.75      0.39         8\n",
      "                                           The West are the aggressors       0.34      0.68      0.45        22\n",
      "                         The West belongs in the right side of history       0.00      0.00      0.00         4\n",
      "        The West does not care about Ukraine, only about its interests       0.24      0.47      0.32        17\n",
      "                      The West has the strongest international support       0.00      0.00      0.00         3\n",
      "                                              The West is overreacting       0.00      0.00      0.00         2\n",
      "                                               The West is russophobic       0.31      0.57      0.40        14\n",
      "                                                      The West is weak       0.12      0.17      0.14        12\n",
      "      The conflict will increase the Ukrainian refugee flows to Europe       0.00      0.00      0.00         1\n",
      "     There is a real possibility that nuclear weapons will be employed       0.43      0.63      0.51        19\n",
      "                                              UA is anti-RU extremists       0.00      0.00      0.00         3\n",
      "                              Ukraine is a hub for criminal activities       0.32      1.00      0.48         7\n",
      "                                       Ukraine is a puppet of the West       0.30      0.57      0.39        21\n",
      "                                     Ukraine is associated with nazism       0.17      0.50      0.25         8\n",
      "                                              Ukraine is the aggressor       0.23      0.81      0.36        16\n",
      "                                          Ukrainian army is collapsing       0.10      0.14      0.12         7\n",
      "                                     Ukrainian media cannot be trusted       0.00      0.00      0.00         0\n",
      "                          Weather suggests the trend is global cooling       0.00      0.00      0.00         1\n",
      "                                              West is tired of Ukraine       0.40      0.50      0.44         4\n",
      "                          Western media is an instrument of propaganda       0.43      0.38      0.40         8\n",
      "                                 Whatever we do it is already too late       0.20      0.50      0.29         2\n",
      "\n",
      "                                                             micro avg       0.35      0.65      0.45       731\n",
      "                                                             macro avg       0.22      0.42      0.27       731\n",
      "                                                          weighted avg       0.41      0.65      0.48       731\n",
      "                                                           samples avg       0.43      0.67      0.49       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.evaluate(\n",
    "    model=trained_model_concat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a924dc6-2d24-4835-9812-67346e06cc0a",
   "metadata": {},
   "source": [
    "### Using multiplication instead of concantenation\n",
    "\n",
    "Instead of using concat, we can try an element-wise multiplication. This sounds more logical as multiplication can act as a \"gate\":\n",
    "\n",
    "* If the narrative probability is close to 0, the corresponding subnarrative head’s input will be scaled down, effectively disabling that subnarrative head.\n",
    "* If the narrative probability is close to 1, the shared layer output passes through somewhat unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dee697c4-d795-474f-8a8d-95cf7ffa37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskClassifierMultiHeadMult(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout'],\n",
    "        bias=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared layer output\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        # Narrative probabilities\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            narr_pred = narr_probs[:, int(narr_idx)].unsqueeze(1)\n",
    "\n",
    "            conditioned_input = shared_out * (narr_pred + self.bias)\n",
    "\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a61a372-241e-4bcc-9d81-3b30631ad46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head_mult = MultiTaskClassifierMultiHeadMult(\n",
    "    input_size=input_size,\n",
    "    hidden_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f7f1c-c6a9-498c-88e5-ab7c12ca53f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd95c66a-2a4d-4387-9945-02fe88cd9142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.0104, Validation Loss: 0.9860\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Training Loss: 0.8908, Validation Loss: 0.9823\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Training Loss: 0.8295, Validation Loss: 0.9787\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Training Loss: 0.7878, Validation Loss: 0.9751\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Training Loss: 0.7531, Validation Loss: 0.9715\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Training Loss: 0.7282, Validation Loss: 0.9677\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Training Loss: 0.7062, Validation Loss: 0.9635\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Training Loss: 0.6851, Validation Loss: 0.9591\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Training Loss: 0.6666, Validation Loss: 0.9542\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Training Loss: 0.6513, Validation Loss: 0.9491\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Training Loss: 0.6383, Validation Loss: 0.9439\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Training Loss: 0.6241, Validation Loss: 0.9386\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Training Loss: 0.6097, Validation Loss: 0.9333\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Training Loss: 0.5963, Validation Loss: 0.9280\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Training Loss: 0.5846, Validation Loss: 0.9228\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Training Loss: 0.5726, Validation Loss: 0.9174\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Training Loss: 0.5601, Validation Loss: 0.9117\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Training Loss: 0.5503, Validation Loss: 0.9059\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Training Loss: 0.5407, Validation Loss: 0.8997\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Training Loss: 0.5292, Validation Loss: 0.8930\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Training Loss: 0.5183, Validation Loss: 0.8859\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Training Loss: 0.5102, Validation Loss: 0.8786\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Training Loss: 0.5007, Validation Loss: 0.8712\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Training Loss: 0.4919, Validation Loss: 0.8640\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Training Loss: 0.4839, Validation Loss: 0.8569\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Training Loss: 0.4753, Validation Loss: 0.8498\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Training Loss: 0.4687, Validation Loss: 0.8425\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Training Loss: 0.4607, Validation Loss: 0.8353\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Training Loss: 0.4535, Validation Loss: 0.8280\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Training Loss: 0.4454, Validation Loss: 0.8206\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Training Loss: 0.4385, Validation Loss: 0.8132\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Training Loss: 0.4335, Validation Loss: 0.8058\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Training Loss: 0.4275, Validation Loss: 0.7985\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Training Loss: 0.4208, Validation Loss: 0.7908\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Training Loss: 0.4147, Validation Loss: 0.7838\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Training Loss: 0.4090, Validation Loss: 0.7772\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 37/100, Training Loss: 0.4036, Validation Loss: 0.7710\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 38/100, Training Loss: 0.3984, Validation Loss: 0.7650\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 39/100, Training Loss: 0.3928, Validation Loss: 0.7596\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Training Loss: 0.3879, Validation Loss: 0.7544\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 41/100, Training Loss: 0.3828, Validation Loss: 0.7498\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 42/100, Training Loss: 0.3794, Validation Loss: 0.7452\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 43/100, Training Loss: 0.3748, Validation Loss: 0.7408\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 44/100, Training Loss: 0.3697, Validation Loss: 0.7371\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 45/100, Training Loss: 0.3658, Validation Loss: 0.7336\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 46/100, Training Loss: 0.3611, Validation Loss: 0.7309\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 47/100, Training Loss: 0.3572, Validation Loss: 0.7294\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 48/100, Training Loss: 0.3537, Validation Loss: 0.7284\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 49/100, Training Loss: 0.3510, Validation Loss: 0.7269\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 50/100, Training Loss: 0.3444, Validation Loss: 0.7243\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 51/100, Training Loss: 0.3415, Validation Loss: 0.7218\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 52/100, Training Loss: 0.3387, Validation Loss: 0.7210\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 53/100, Training Loss: 0.3345, Validation Loss: 0.7226\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 54/100, Training Loss: 0.3309, Validation Loss: 0.7259\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 55/100, Training Loss: 0.3277, Validation Loss: 0.7301\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Epoch 56/100, Training Loss: 0.3239, Validation Loss: 0.7338\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Epoch 57/100, Training Loss: 0.3197, Validation Loss: 0.7348\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Epoch 58/100, Training Loss: 0.3173, Validation Loss: 0.7345\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Epoch 59/100, Training Loss: 0.3163, Validation Loss: 0.7345\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Epoch 60/100, Training Loss: 0.3146, Validation Loss: 0.7355\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Epoch 61/100, Training Loss: 0.3124, Validation Loss: 0.7363\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "trained_model_mult = initialize_and_train_model(model_multi_head_mult, loss_fn=multi_head_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4759f6-e4f3-465b-acb3-a31af9331ff0",
   "metadata": {},
   "source": [
    "The results are a bit suprising, but they make sense because concatenation gives the subnarrative heads more \"flexibility\", while multiplication is more restrictive acting as a hard gate.\n",
    "\n",
    "* If our narrative predictions are not confident or most importantly not correct, the subnarrative head will receive very weak input because of the multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4e8b265-8714-4004-adbc-f215ba348052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Threshold: 0.50\n",
      "Best Average F1: 0.3465\n",
      "\n",
      "Narrative Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                         Amplifying Climate Fears       0.64      0.98      0.78        46\n",
      "                     Amplifying war-related fears       0.51      0.87      0.65        47\n",
      "Blaming the war on others rather than the invader       0.23      0.72      0.35        32\n",
      "                     Climate change is beneficial       0.43      0.75      0.55         4\n",
      "             Controversy about green technologies       0.17      1.00      0.30         4\n",
      "                    Criticism of climate movement       0.34      0.91      0.50        11\n",
      "                    Criticism of climate policies       0.33      0.81      0.47        21\n",
      "        Criticism of institutions and authorities       0.36      0.86      0.51        28\n",
      "                             Discrediting Ukraine       0.58      0.91      0.71        79\n",
      "                 Discrediting the West, Diplomacy       0.54      0.77      0.63        69\n",
      "                           Distrust towards Media       0.19      0.44      0.27         9\n",
      "                       Downplaying climate change       0.21      0.90      0.35        10\n",
      "      Green policies are geopolitical instruments       0.25      0.50      0.33         2\n",
      "Hidden plots by secret schemes of powerful groups       0.32      0.60      0.42        15\n",
      "               Negative Consequences for the West       0.20      0.47      0.28        17\n",
      "                                            Other       0.54      0.72      0.62        67\n",
      "                            Overpraising the West       0.12      0.33      0.18         9\n",
      "                                 Praise of Russia       0.46      0.72      0.56        67\n",
      "         Questioning the measurements and science       0.40      1.00      0.57         6\n",
      "                             Russia is the Victim       0.30      0.73      0.43        41\n",
      "                         Speculating war outcomes       0.32      0.57      0.41        30\n",
      "\n",
      "                                        micro avg       0.41      0.77      0.53       614\n",
      "                                        macro avg       0.36      0.74      0.47       614\n",
      "                                     weighted avg       0.44      0.77      0.55       614\n",
      "                                      samples avg       0.46      0.78      0.54       614\n",
      "\n",
      "\n",
      "Subnarrative Classification Report:\n",
      "                                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                   Ad hominem attacks on key activists       0.25      0.25      0.25         4\n",
      "                           Amplifying existing fears of global warming       0.72      0.94      0.81        35\n",
      "                                                 Blaming global elites       0.20      0.33      0.25         3\n",
      "                                   By continuing the war we risk WWIII       0.57      0.33      0.42        12\n",
      "                    CO2 concentrations are too small to have an impact       0.00      0.00      0.00         1\n",
      "                                                     CO2 is beneficial       0.00      0.00      0.00         2\n",
      "                                     Climate agenda has hidden motives       0.40      0.29      0.33         7\n",
      "                                            Climate cycles are natural       0.12      0.33      0.18         3\n",
      "                                          Climate movement is alarmist       0.38      0.50      0.43         6\n",
      "                                           Climate movement is corrupt       0.00      0.00      0.00         3\n",
      "                                      Climate policies are ineffective       0.14      0.14      0.14         7\n",
      "                                  Climate policies are only for profit       0.00      0.00      0.00         3\n",
      "                  Climate policies have negative impact on the economy       0.12      0.11      0.12         9\n",
      "      Climate-related international relations are abusive/exploitative       0.00      0.00      0.00         1\n",
      "                                   Criticism of international entities       0.40      0.33      0.36         6\n",
      "                                     Criticism of national governments       0.45      0.45      0.45        20\n",
      "                      Criticism of political organizations and figures       0.40      0.57      0.47        14\n",
      "                                                   Criticism of the EU       0.00      0.00      0.00         2\n",
      "                                    Data shows no temperature increase       0.00      0.00      0.00         1\n",
      "                                          Diplomacy does/will not work       0.33      0.25      0.29         8\n",
      "          Discrediting Ukrainian government and officials and policies       0.51      0.61      0.56        31\n",
      "                                       Discrediting Ukrainian military       0.43      0.75      0.55        20\n",
      "                             Discrediting Ukrainian nation and society       0.00      0.00      0.00         2\n",
      "                                         Doomsday scenarios for humans       0.31      0.62      0.42         8\n",
      "                                      Earth will be uninhabitable soon       0.36      0.80      0.50         5\n",
      "                        Green activities are a form of neo-colonialism       0.00      0.00      0.00         1\n",
      "          Greenhouse effect/carbon dioxide do not drive climate change       0.00      0.00      0.00         1\n",
      "                         Human activities do not impact climate change       0.00      0.00      0.00         2\n",
      "                           Humans and nature will adapt to the changes       0.00      0.00      0.00         1\n",
      "                                                    Ice is not melting       0.00      0.00      0.00         1\n",
      "                      Methodologies/metrics used are unreliable/faulty       1.00      0.33      0.50         3\n",
      "                                   NATO should/will directly intervene       0.00      0.00      0.00         6\n",
      "                                              NATO will destroy Russia       0.00      0.00      0.00         1\n",
      "                                                                 Other       0.76      0.61      0.68       186\n",
      "                            Praise of Russian President Vladimir Putin       0.43      0.38      0.40         8\n",
      "                                      Praise of Russian military might       0.46      0.64      0.54        28\n",
      "                                            Renewable energy is costly       0.33      0.33      0.33         3\n",
      "                                         Renewable energy is dangerous       0.00      0.00      0.00         1\n",
      "                                        Renewable energy is unreliable       0.00      0.00      0.00         3\n",
      "                                           Rewriting Ukraine’s history       1.00      0.50      0.67         2\n",
      "                       Russia actions in Ukraine are only self-defence       0.43      0.43      0.43        14\n",
      "Russia has international support from a number of countries and people       0.58      0.47      0.52        15\n",
      "                         Russia is a guarantor of peace and prosperity       0.43      0.45      0.44        20\n",
      "                               Russia will also attack other countries       0.00      0.00      0.00        11\n",
      "                                            Russian army is collapsing       0.00      0.00      0.00         3\n",
      "                   Russian army will lose all the occupied territories       0.00      0.00      0.00         1\n",
      "                          Russian invasion has strong national support       0.00      0.00      0.00         1\n",
      "                  Sanctions imposed by Western countries will backfire       0.25      0.14      0.18         7\n",
      "                                    Scientific community is unreliable       1.00      0.75      0.86         4\n",
      "                                             Sea levels are not rising       0.00      0.00      0.00         1\n",
      "                                      Situation in Ukraine is hopeless       0.28      0.45      0.34        11\n",
      "                 Temperature increase does not have significant impact       0.00      0.00      0.00         2\n",
      "                                    Temperature increase is beneficial       0.00      0.00      0.00         2\n",
      "                                                     The EU is divided       0.45      0.62      0.53         8\n",
      "                                           The West are the aggressors       0.28      0.32      0.30        22\n",
      "                         The West belongs in the right side of history       0.00      0.00      0.00         4\n",
      "        The West does not care about Ukraine, only about its interests       0.09      0.06      0.07        17\n",
      "                      The West has the strongest international support       0.00      0.00      0.00         3\n",
      "                                              The West is overreacting       0.00      0.00      0.00         2\n",
      "                                               The West is russophobic       0.43      0.21      0.29        14\n",
      "                                                      The West is weak       0.14      0.08      0.11        12\n",
      "      The conflict will increase the Ukrainian refugee flows to Europe       0.00      0.00      0.00         1\n",
      "     There is a real possibility that nuclear weapons will be employed       0.62      0.42      0.50        19\n",
      "                                              UA is anti-RU extremists       0.00      0.00      0.00         3\n",
      "                              Ukraine is a hub for criminal activities       0.44      0.57      0.50         7\n",
      "                                       Ukraine is a puppet of the West       0.35      0.38      0.36        21\n",
      "                                     Ukraine is associated with nazism       0.17      0.12      0.14         8\n",
      "                                              Ukraine is the aggressor       0.36      0.62      0.45        16\n",
      "                                          Ukrainian army is collapsing       0.00      0.00      0.00         7\n",
      "                                     Ukrainian media cannot be trusted       0.00      0.00      0.00         0\n",
      "                          Weather suggests the trend is global cooling       0.00      0.00      0.00         1\n",
      "                                              West is tired of Ukraine       1.00      0.25      0.40         4\n",
      "                          Western media is an instrument of propaganda       1.00      0.38      0.55         8\n",
      "                                 Whatever we do it is already too late       0.00      0.00      0.00         2\n",
      "\n",
      "                                                             micro avg       0.48      0.45      0.47       731\n",
      "                                                             macro avg       0.25      0.23      0.22       731\n",
      "                                                          weighted avg       0.47      0.45      0.45       731\n",
      "                                                           samples avg       0.47      0.49      0.45       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.evaluate(\n",
    "    model=trained_model_mult,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653a4c6-c42c-47be-b984-1edb68d72602",
   "metadata": {},
   "source": [
    "Because subnarrative heads rely heavily on the narrative probabilities, we will reduce the `sub_weight` increasing the the narrative weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e58c1b7-b743-4fc2-901a-f8cca9f90acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_loss_fn = MultiHeadLoss(\n",
    "    narrative_criterion, \n",
    "    sub_criterion_dict,\n",
    "    condition_weight=0.5,\n",
    "    sub_weight=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbc840d4-2317-47bf-a173-c1ef2754b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head_mult = MultiTaskClassifierMultiHeadMult(\n",
    "    input_size=input_size,\n",
    "    hidden_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd283a9a-cc16-48d3-bf77-71ff1f16790c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.9761, Validation Loss: 0.9359\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Training Loss: 0.8304, Validation Loss: 0.9310\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Training Loss: 0.7645, Validation Loss: 0.9266\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Training Loss: 0.7228, Validation Loss: 0.9229\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Training Loss: 0.6819, Validation Loss: 0.9192\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Training Loss: 0.6584, Validation Loss: 0.9155\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Training Loss: 0.6368, Validation Loss: 0.9115\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Training Loss: 0.6182, Validation Loss: 0.9071\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Training Loss: 0.6035, Validation Loss: 0.9023\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Training Loss: 0.5870, Validation Loss: 0.8968\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Training Loss: 0.5735, Validation Loss: 0.8908\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Training Loss: 0.5597, Validation Loss: 0.8845\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Training Loss: 0.5498, Validation Loss: 0.8780\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Training Loss: 0.5372, Validation Loss: 0.8714\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Training Loss: 0.5263, Validation Loss: 0.8649\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Training Loss: 0.5146, Validation Loss: 0.8586\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Training Loss: 0.5074, Validation Loss: 0.8522\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Training Loss: 0.4984, Validation Loss: 0.8457\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Training Loss: 0.4904, Validation Loss: 0.8386\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Training Loss: 0.4809, Validation Loss: 0.8313\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Training Loss: 0.4737, Validation Loss: 0.8237\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Training Loss: 0.4664, Validation Loss: 0.8160\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Training Loss: 0.4579, Validation Loss: 0.8082\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Training Loss: 0.4517, Validation Loss: 0.8006\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Training Loss: 0.4443, Validation Loss: 0.7931\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Training Loss: 0.4371, Validation Loss: 0.7857\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Training Loss: 0.4313, Validation Loss: 0.7785\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Training Loss: 0.4232, Validation Loss: 0.7715\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Training Loss: 0.4170, Validation Loss: 0.7647\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Training Loss: 0.4117, Validation Loss: 0.7581\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Training Loss: 0.4056, Validation Loss: 0.7513\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Training Loss: 0.3991, Validation Loss: 0.7447\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Training Loss: 0.3936, Validation Loss: 0.7381\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Training Loss: 0.3882, Validation Loss: 0.7319\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Training Loss: 0.3843, Validation Loss: 0.7258\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Training Loss: 0.3778, Validation Loss: 0.7198\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 37/100, Training Loss: 0.3712, Validation Loss: 0.7144\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 38/100, Training Loss: 0.3666, Validation Loss: 0.7093\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 39/100, Training Loss: 0.3622, Validation Loss: 0.7049\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Training Loss: 0.3569, Validation Loss: 0.7000\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 41/100, Training Loss: 0.3525, Validation Loss: 0.6945\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 42/100, Training Loss: 0.3474, Validation Loss: 0.6886\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 43/100, Training Loss: 0.3430, Validation Loss: 0.6831\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 44/100, Training Loss: 0.3380, Validation Loss: 0.6783\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 45/100, Training Loss: 0.3349, Validation Loss: 0.6743\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 46/100, Training Loss: 0.3289, Validation Loss: 0.6702\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 47/100, Training Loss: 0.3260, Validation Loss: 0.6665\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 48/100, Training Loss: 0.3223, Validation Loss: 0.6644\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 49/100, Training Loss: 0.3174, Validation Loss: 0.6637\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 50/100, Training Loss: 0.3145, Validation Loss: 0.6639\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 51/100, Training Loss: 0.3112, Validation Loss: 0.6626\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 52/100, Training Loss: 0.3059, Validation Loss: 0.6604\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 53/100, Training Loss: 0.3026, Validation Loss: 0.6582\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 54/100, Training Loss: 0.2986, Validation Loss: 0.6582\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 55/100, Training Loss: 0.2952, Validation Loss: 0.6586\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 56/100, Training Loss: 0.2914, Validation Loss: 0.6609\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 57/100, Training Loss: 0.2886, Validation Loss: 0.6641\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Epoch 58/100, Training Loss: 0.2861, Validation Loss: 0.6644\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Epoch 59/100, Training Loss: 0.2846, Validation Loss: 0.6649\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Epoch 60/100, Training Loss: 0.2822, Validation Loss: 0.6666\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Epoch 61/100, Training Loss: 0.2800, Validation Loss: 0.6688\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Epoch 62/100, Training Loss: 0.2788, Validation Loss: 0.6707\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Epoch 63/100, Training Loss: 0.2779, Validation Loss: 0.6726\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "trained_model_mult = initialize_and_train_model(model_multi_head_mult,\n",
    "                                                loss_fn=multi_head_loss_fn,\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c36f5470-5bf3-435d-8a01-40c2dc572f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Threshold: 0.40\n",
      "Best Average F1: 0.3496\n",
      "\n",
      "Narrative Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                         Amplifying Climate Fears       0.62      1.00      0.77        46\n",
      "                     Amplifying war-related fears       0.54      0.85      0.66        47\n",
      "Blaming the war on others rather than the invader       0.24      0.69      0.36        32\n",
      "                     Climate change is beneficial       0.20      0.50      0.29         4\n",
      "             Controversy about green technologies       0.19      1.00      0.32         4\n",
      "                    Criticism of climate movement       0.31      1.00      0.48        11\n",
      "                    Criticism of climate policies       0.31      0.71      0.43        21\n",
      "        Criticism of institutions and authorities       0.35      0.89      0.51        28\n",
      "                             Discrediting Ukraine       0.57      0.92      0.71        79\n",
      "                 Discrediting the West, Diplomacy       0.52      0.75      0.62        69\n",
      "                           Distrust towards Media       0.17      0.33      0.22         9\n",
      "                       Downplaying climate change       0.26      0.90      0.40        10\n",
      "      Green policies are geopolitical instruments       0.33      0.50      0.40         2\n",
      "Hidden plots by secret schemes of powerful groups       0.31      0.53      0.39        15\n",
      "               Negative Consequences for the West       0.22      0.47      0.30        17\n",
      "                                            Other       0.49      0.78      0.60        67\n",
      "                            Overpraising the West       0.11      0.33      0.17         9\n",
      "                                 Praise of Russia       0.45      0.79      0.58        67\n",
      "         Questioning the measurements and science       0.35      1.00      0.52         6\n",
      "                             Russia is the Victim       0.29      0.59      0.39        41\n",
      "                         Speculating war outcomes       0.31      0.63      0.42        30\n",
      "\n",
      "                                        micro avg       0.40      0.78      0.53       614\n",
      "                                        macro avg       0.34      0.72      0.45       614\n",
      "                                     weighted avg       0.43      0.78      0.55       614\n",
      "                                      samples avg       0.46      0.79      0.54       614\n",
      "\n",
      "\n",
      "Subnarrative Classification Report:\n",
      "                                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                   Ad hominem attacks on key activists       0.25      0.50      0.33         4\n",
      "                           Amplifying existing fears of global warming       0.69      0.94      0.80        35\n",
      "                                                 Blaming global elites       0.17      0.33      0.22         3\n",
      "                                   By continuing the war we risk WWIII       0.35      0.58      0.44        12\n",
      "                    CO2 concentrations are too small to have an impact       0.00      0.00      0.00         1\n",
      "                                                     CO2 is beneficial       0.00      0.00      0.00         2\n",
      "                                     Climate agenda has hidden motives       0.38      0.43      0.40         7\n",
      "                                            Climate cycles are natural       0.13      0.67      0.22         3\n",
      "                                          Climate movement is alarmist       0.40      0.67      0.50         6\n",
      "                                           Climate movement is corrupt       0.00      0.00      0.00         3\n",
      "                                      Climate policies are ineffective       0.11      0.29      0.15         7\n",
      "                                  Climate policies are only for profit       0.11      0.33      0.17         3\n",
      "                  Climate policies have negative impact on the economy       0.17      0.22      0.19         9\n",
      "      Climate-related international relations are abusive/exploitative       0.00      0.00      0.00         1\n",
      "                                   Criticism of international entities       0.33      0.33      0.33         6\n",
      "                                     Criticism of national governments       0.43      0.60      0.50        20\n",
      "                      Criticism of political organizations and figures       0.39      0.79      0.52        14\n",
      "                                                   Criticism of the EU       0.00      0.00      0.00         2\n",
      "                                    Data shows no temperature increase       0.00      0.00      0.00         1\n",
      "                                          Diplomacy does/will not work       0.15      0.25      0.19         8\n",
      "          Discrediting Ukrainian government and officials and policies       0.43      0.65      0.52        31\n",
      "                                       Discrediting Ukrainian military       0.43      0.90      0.58        20\n",
      "                             Discrediting Ukrainian nation and society       1.00      0.50      0.67         2\n",
      "                                         Doomsday scenarios for humans       0.27      0.75      0.40         8\n",
      "                                      Earth will be uninhabitable soon       0.33      1.00      0.50         5\n",
      "                        Green activities are a form of neo-colonialism       0.00      0.00      0.00         1\n",
      "          Greenhouse effect/carbon dioxide do not drive climate change       0.00      0.00      0.00         1\n",
      "                         Human activities do not impact climate change       0.00      0.00      0.00         2\n",
      "                           Humans and nature will adapt to the changes       0.00      0.00      0.00         1\n",
      "                                                    Ice is not melting       0.00      0.00      0.00         1\n",
      "                      Methodologies/metrics used are unreliable/faulty       1.00      0.33      0.50         3\n",
      "                                   NATO should/will directly intervene       0.00      0.00      0.00         6\n",
      "                                              NATO will destroy Russia       0.00      0.00      0.00         1\n",
      "                                                                 Other       0.75      0.56      0.64       186\n",
      "                            Praise of Russian President Vladimir Putin       0.31      0.50      0.38         8\n",
      "                                      Praise of Russian military might       0.37      0.68      0.48        28\n",
      "                                            Renewable energy is costly       0.33      0.33      0.33         3\n",
      "                                         Renewable energy is dangerous       0.00      0.00      0.00         1\n",
      "                                        Renewable energy is unreliable       0.25      0.33      0.29         3\n",
      "                                           Rewriting Ukraine’s history       1.00      0.50      0.67         2\n",
      "                       Russia actions in Ukraine are only self-defence       0.31      0.57      0.40        14\n",
      "Russia has international support from a number of countries and people       0.47      0.47      0.47        15\n",
      "                         Russia is a guarantor of peace and prosperity       0.39      0.60      0.47        20\n",
      "                               Russia will also attack other countries       0.07      0.09      0.08        11\n",
      "                                            Russian army is collapsing       0.00      0.00      0.00         3\n",
      "                   Russian army will lose all the occupied territories       0.00      0.00      0.00         1\n",
      "                          Russian invasion has strong national support       0.00      0.00      0.00         1\n",
      "                  Sanctions imposed by Western countries will backfire       0.00      0.00      0.00         7\n",
      "                                    Scientific community is unreliable       1.00      0.75      0.86         4\n",
      "                                             Sea levels are not rising       0.00      0.00      0.00         1\n",
      "                                      Situation in Ukraine is hopeless       0.21      0.55      0.31        11\n",
      "                 Temperature increase does not have significant impact       0.00      0.00      0.00         2\n",
      "                                    Temperature increase is beneficial       0.00      0.00      0.00         2\n",
      "                                                     The EU is divided       0.42      0.62      0.50         8\n",
      "                                           The West are the aggressors       0.26      0.36      0.30        22\n",
      "                         The West belongs in the right side of history       0.00      0.00      0.00         4\n",
      "        The West does not care about Ukraine, only about its interests       0.19      0.24      0.21        17\n",
      "                      The West has the strongest international support       0.00      0.00      0.00         3\n",
      "                                              The West is overreacting       0.00      0.00      0.00         2\n",
      "                                               The West is russophobic       0.31      0.36      0.33        14\n",
      "                                                      The West is weak       0.18      0.17      0.17        12\n",
      "      The conflict will increase the Ukrainian refugee flows to Europe       0.00      0.00      0.00         1\n",
      "     There is a real possibility that nuclear weapons will be employed       0.53      0.47      0.50        19\n",
      "                                              UA is anti-RU extremists       0.00      0.00      0.00         3\n",
      "                              Ukraine is a hub for criminal activities       0.42      0.71      0.53         7\n",
      "                                       Ukraine is a puppet of the West       0.32      0.43      0.37        21\n",
      "                                     Ukraine is associated with nazism       0.19      0.38      0.25         8\n",
      "                                              Ukraine is the aggressor       0.33      0.62      0.43        16\n",
      "                                          Ukrainian army is collapsing       0.00      0.00      0.00         7\n",
      "                                     Ukrainian media cannot be trusted       0.00      0.00      0.00         0\n",
      "                          Weather suggests the trend is global cooling       0.00      0.00      0.00         1\n",
      "                                              West is tired of Ukraine       0.25      0.25      0.25         4\n",
      "                          Western media is an instrument of propaganda       0.60      0.38      0.46         8\n",
      "                                 Whatever we do it is already too late       0.33      0.50      0.40         2\n",
      "\n",
      "                                                             micro avg       0.40      0.50      0.44       731\n",
      "                                                             macro avg       0.23      0.30      0.25       731\n",
      "                                                          weighted avg       0.44      0.50      0.45       731\n",
      "                                                           samples avg       0.43      0.52      0.44       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.evaluate(\n",
    "    model=trained_model_mult,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
