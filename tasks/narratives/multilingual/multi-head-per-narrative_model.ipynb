{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0d8ce9-d6d9-4e03-82e8-36fe1c5a343a",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa2ebd-27af-46e1-aa7c-d2549cfc988a",
   "metadata": {},
   "source": [
    "## 1. Multi-head per narrative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67788-7872-4608-ba0e-01fd263958cf",
   "metadata": {},
   "source": [
    "### 1.1 Loading pre-saved variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe27a15d-ab42-4768-a155-a65353545112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_cleaned.pkl'), 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d58b84c-f126-4f5f-9f4c-883bd5b2741d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[Blaming the war on others rather than the inv...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[Discrediting the West, Diplomacy, Discreditin...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[Discrediting Ukraine, Discrediting Ukraine]</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [Blaming the war on others rather than the inv...   \n",
       "1  [Discrediting the West, Diplomacy, Discreditin...   \n",
       "2                           [Distrust towards Media]   \n",
       "3       [Discrediting Ukraine, Discrediting Ukraine]   \n",
       "4                                 [Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e056632b-56d7-47ae-929b-cdda00d86a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11550c30-6e80-4c4e-b262-ecec4d942892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Discrediting Ukraine': ['Other',\n",
       "  'Rewriting Ukraine’s history',\n",
       "  'Discrediting Ukrainian military',\n",
       "  'Ukraine is a hub for criminal activities',\n",
       "  'Ukraine is associated with nazism',\n",
       "  'Discrediting Ukrainian nation and society',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Ukraine is a puppet of the West',\n",
       "  'Discrediting Ukrainian government and officials and policies'],\n",
       " 'Discrediting the West, Diplomacy': ['Diplomacy does/will not work',\n",
       "  'Other',\n",
       "  'The EU is divided',\n",
       "  'The West is weak',\n",
       "  'The West does not care about Ukraine, only about its interests',\n",
       "  'The West is overreacting',\n",
       "  'West is tired of Ukraine'],\n",
       " 'Praise of Russia': ['Other',\n",
       "  'Praise of Russian President Vladimir Putin',\n",
       "  'Russia is a guarantor of peace and prosperity',\n",
       "  'Russian invasion has strong national support',\n",
       "  'Russia has international support from a number of countries and people',\n",
       "  'Praise of Russian military might'],\n",
       " 'Russia is the Victim': ['Other',\n",
       "  'The West is russophobic',\n",
       "  'UA is anti-RU extremists',\n",
       "  'Russia actions in Ukraine are only self-defence'],\n",
       " 'Distrust towards Media': ['Other',\n",
       "  'Western media is an instrument of propaganda',\n",
       "  'Ukrainian media cannot be trusted'],\n",
       " 'Amplifying war-related fears': ['Other',\n",
       "  'NATO should/will directly intervene',\n",
       "  'Russia will also attack other countries',\n",
       "  'There is a real possibility that nuclear weapons will be employed',\n",
       "  'By continuing the war we risk WWIII'],\n",
       " 'Overpraising the West': ['Other',\n",
       "  'NATO will destroy Russia',\n",
       "  'The West has the strongest international support',\n",
       "  'The West belongs in the right side of history'],\n",
       " 'Blaming the war on others rather than the invader': ['Ukraine is the aggressor',\n",
       "  'The West are the aggressors',\n",
       "  'Other'],\n",
       " 'Speculating war outcomes': ['Other',\n",
       "  'Ukrainian army is collapsing',\n",
       "  'Russian army will lose all the occupied territories',\n",
       "  'Russian army is collapsing'],\n",
       " 'Hidden plots by secret schemes of powerful groups': ['Other',\n",
       "  'Blaming global elites',\n",
       "  'Climate agenda has hidden motives'],\n",
       " 'Negative Consequences for the West': ['Sanctions imposed by Western countries will backfire',\n",
       "  'Other',\n",
       "  'The conflict will increase the Ukrainian refugee flows to Europe'],\n",
       " 'Amplifying Climate Fears': ['Other',\n",
       "  'Amplifying existing fears of global warming',\n",
       "  'Whatever we do it is already too late',\n",
       "  'Doomsday scenarios for humans',\n",
       "  'Earth will be uninhabitable soon'],\n",
       " 'Other': ['Other'],\n",
       " 'Criticism of institutions and authorities': ['Other',\n",
       "  'Criticism of national governments',\n",
       "  'Criticism of international entities',\n",
       "  'Criticism of the EU',\n",
       "  'Criticism of political organizations and figures'],\n",
       " 'Criticism of climate movement': ['Climate movement is alarmist',\n",
       "  'Other',\n",
       "  'Climate movement is corrupt',\n",
       "  'Ad hominem attacks on key activists'],\n",
       " 'Downplaying climate change': ['Other',\n",
       "  'Sea levels are not rising',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Ice is not melting',\n",
       "  'Climate cycles are natural',\n",
       "  'CO2 concentrations are too small to have an impact',\n",
       "  'Human activities do not impact climate change',\n",
       "  'Humans and nature will adapt to the changes',\n",
       "  'Temperature increase does not have significant impact'],\n",
       " 'Criticism of climate policies': ['Climate policies are only for profit',\n",
       "  'Other',\n",
       "  'Climate policies have negative impact on the economy',\n",
       "  'Climate policies are ineffective'],\n",
       " 'Questioning the measurements and science': ['Methodologies/metrics used are unreliable/faulty',\n",
       "  'Other',\n",
       "  'Scientific community is unreliable',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Data shows no temperature increase'],\n",
       " 'Climate change is beneficial': ['Other',\n",
       "  'CO2 is beneficial',\n",
       "  'Temperature increase is beneficial'],\n",
       " 'Controversy about green technologies': ['Renewable energy is dangerous',\n",
       "  'Other',\n",
       "  'Renewable energy is costly',\n",
       "  'Renewable energy is unreliable'],\n",
       " 'Green policies are geopolitical instruments': ['Green activities are a form of neo-colonialism',\n",
       "  'Other',\n",
       "  'Climate-related international relations are abusive/exploitative']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6912756b-47f2-4a11-b782-71f86ee25137",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082080ec-bfbf-4db2-8363-2b06413a3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/all_embeddings.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "all_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3784e6e1-4100-47d5-9df1-bc1f2c2d8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def stratified_train_val_split_with_embeddings(data, embeddings, labels_column, train_size=0.8, splits=5, shuffle=True, min_instances=2):\n",
    "    if shuffle:\n",
    "        shuffled_indices = np.arange(len(data))\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "        embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    labels = np.array(data[labels_column].tolist())\n",
    "    rare_indices = []\n",
    "    common_indices = []\n",
    "\n",
    "    class_counts = labels.sum(axis=0)\n",
    "    rare_classes = np.where(class_counts <= min_instances)[0]\n",
    "\n",
    "    for idx, label_row in enumerate(labels):\n",
    "        if any(label_row[rare_classes]):\n",
    "            rare_indices.append(idx)\n",
    "        else:\n",
    "            common_indices.append(idx)\n",
    "\n",
    "    rare_data = data.iloc[rare_indices]\n",
    "    rare_labels = labels[rare_indices]\n",
    "    rare_embeddings = embeddings[rare_indices]\n",
    "\n",
    "    train_rare = rare_data.iloc[:len(rare_data) // 2].reset_index(drop=True)\n",
    "    val_rare = rare_data.iloc[len(rare_data) // 2:].reset_index(drop=True)\n",
    "\n",
    "    train_rare_embeddings = rare_embeddings[:len(rare_data) // 2]\n",
    "    val_rare_embeddings = rare_embeddings[len(rare_data) // 2:]\n",
    "\n",
    "    common_data = data.iloc[common_indices].reset_index(drop=True)\n",
    "    common_labels = labels[common_indices]\n",
    "    common_embeddings = embeddings[common_indices]\n",
    "\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=splits)\n",
    "    for train_idx, val_idx in mskf.split(np.zeros(len(common_labels)), common_labels):\n",
    "        train_common = common_data.iloc[train_idx]\n",
    "        val_common = common_data.iloc[val_idx]\n",
    "        train_common_embeddings = common_embeddings[train_idx]\n",
    "        val_common_embeddings = common_embeddings[val_idx]\n",
    "        break\n",
    "\n",
    "    train_data = pd.concat([train_rare, train_common]).reset_index(drop=True)\n",
    "    val_data = pd.concat([val_rare, val_common]).reset_index(drop=True)\n",
    "\n",
    "    train_embeddings = np.concatenate([train_rare_embeddings, train_common_embeddings], axis=0)\n",
    "    val_embeddings = np.concatenate([val_rare_embeddings, val_common_embeddings], axis=0)\n",
    "\n",
    "    return (train_data, train_embeddings), (val_data, val_embeddings)\n",
    "\n",
    "(dataset_train, train_embeddings), (dataset_val, val_embeddings) = stratified_train_val_split_with_embeddings(\n",
    "    dataset,\n",
    "    all_embeddings,\n",
    "    labels_column=\"subnarratives_encoded\",\n",
    "    min_instances=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad404584-76b4-4922-9a08-585d02b3a0fa",
   "metadata": {},
   "source": [
    "### 1.2 Remapping our subnarrative indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae17fbfc-15ac-421b-ad18-2d010c07bda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                              ...                        \n",
       "1694    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1695    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "1696    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "1697    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1698    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: subnarratives_encoded, Length: 1699, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['subnarratives_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3d9def-3cf1-43e4-8796-aa0f5c1bc415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: [33, 39, 21, 64, 66, 22, 50, 65, 20], 9: [19, 33, 53, 60, 56, 58, 71], 17: [33, 34, 42, 46, 41, 35], 19: [33, 59, 63, 40], 10: [33, 72, 69], 1: [33, 31, 43, 62, 3], 16: [33, 32, 57, 55], 2: [67, 54, 33], 20: [33, 68, 45, 44], 13: [33, 2, 6], 14: [47, 33, 61], 0: [33, 1, 73, 23, 24], 15: [33], 7: [33, 15, 14, 17, 16], 5: [8, 33, 9, 0], 11: [33, 49, 70, 29, 7, 4, 27, 28, 51], 6: [11, 33, 12, 10], 18: [30, 33, 48, 26, 18], 3: [33, 5, 52], 4: [37, 33, 36, 38], 12: [25, 33, 13]}\n"
     ]
    }
   ],
   "source": [
    "narrative_to_sub_map = {}\n",
    "narrative_classes = list(mlb_narratives.classes_)\n",
    "subnarrative_classes = list(mlb_subnarratives.classes_)\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    narrative_idx = narrative_classes.index(narrative)\n",
    "    subnarrative_indices = [subnarrative_classes.index(sub) for sub in subnarratives]\n",
    "    narrative_to_sub_map[narrative_idx] = subnarrative_indices\n",
    "\n",
    "print(narrative_to_sub_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0839cb3-e83e-4702-8e67-16ee7a7de3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: [33, 39, 21, 64, 66, 22, 50, 65, 20],\n",
       " 9: [19, 33, 53, 60, 56, 58, 71],\n",
       " 17: [33, 34, 42, 46, 41, 35],\n",
       " 19: [33, 59, 63, 40],\n",
       " 10: [33, 72, 69],\n",
       " 1: [33, 31, 43, 62, 3],\n",
       " 16: [33, 32, 57, 55],\n",
       " 2: [67, 54, 33],\n",
       " 20: [33, 68, 45, 44],\n",
       " 13: [33, 2, 6],\n",
       " 14: [47, 33, 61],\n",
       " 0: [33, 1, 73, 23, 24],\n",
       " 15: [33],\n",
       " 7: [33, 15, 14, 17, 16],\n",
       " 5: [8, 33, 9, 0],\n",
       " 11: [33, 49, 70, 29, 7, 4, 27, 28, 51],\n",
       " 6: [11, 33, 12, 10],\n",
       " 18: [30, 33, 48, 26, 18],\n",
       " 3: [33, 5, 52],\n",
       " 4: [37, 33, 36, 38],\n",
       " 12: [25, 33, 13]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_sub_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7878d2ae-83ed-4153-8697-b34f22fb60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_subnarratives(row, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row and encodes the current subnarrative list to the associated hierarchy based on the narr-subnar map\"\"\"\n",
    "    for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "        sub_labels = [row['subnarratives_encoded'][sub_idx] for sub_idx in sub_indices]\n",
    "        col_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "        row[col_name] = sub_labels\n",
    "    return row\n",
    "\n",
    "dataset_train_cpy = dataset_train.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49192dc3-7a83-422a-85ea-dc462ebc8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_cpy = dataset_val.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe99263-8222-40b9-a76c-e4781ffe0e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>narrative_hierarchy_8</th>\n",
       "      <th>narrative_hierarchy_9</th>\n",
       "      <th>narrative_hierarchy_17</th>\n",
       "      <th>...</th>\n",
       "      <th>narrative_hierarchy_0</th>\n",
       "      <th>narrative_hierarchy_15</th>\n",
       "      <th>narrative_hierarchy_7</th>\n",
       "      <th>narrative_hierarchy_5</th>\n",
       "      <th>narrative_hierarchy_11</th>\n",
       "      <th>narrative_hierarchy_6</th>\n",
       "      <th>narrative_hierarchy_18</th>\n",
       "      <th>narrative_hierarchy_3</th>\n",
       "      <th>narrative_hierarchy_4</th>\n",
       "      <th>narrative_hierarchy_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_CC_416.txt</td>\n",
       "      <td>&lt;PARA&gt;da patranha do aquecimento global&lt;/PARA&gt;...</td>\n",
       "      <td>[Questioning the measurements and science, Hid...</td>\n",
       "      <td>[Scientific community is unreliable, Climate a...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_200022.txt</td>\n",
       "      <td>&lt;PARA&gt;Denmark to Punish Farmers for cow ‘emiss...</td>\n",
       "      <td>[Criticism of institutions and authorities, Cr...</td>\n",
       "      <td>[Criticism of national governments, Other, Met...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 1, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_300179.txt</td>\n",
       "      <td>&lt;PARA&gt;‘disastrous’ flood warning in California...</td>\n",
       "      <td>[Amplifying Climate Fears]</td>\n",
       "      <td>[Amplifying existing fears of global warming]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_URW_410.txt</td>\n",
       "      <td>&lt;PARA&gt;rússia acusa ucrânia de preparar ataque ...</td>\n",
       "      <td>[Discrediting the West, Diplomacy, Discreditin...</td>\n",
       "      <td>[Other, Other, Discrediting Ukrainian governme...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HI</td>\n",
       "      <td>HI_116.txt</td>\n",
       "      <td>&lt;PARA&gt;russia-ukraine conflict: क्या यूक्रेन सं...</td>\n",
       "      <td>[Amplifying war-related fears, Russia is the V...</td>\n",
       "      <td>[By continuing the war we risk WWIII, The West...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  language        article_id  \\\n",
       "0       PT     PT_CC_416.txt   \n",
       "1       EN  EN_CC_200022.txt   \n",
       "2       EN  EN_CC_300179.txt   \n",
       "3       PT    PT_URW_410.txt   \n",
       "4       HI        HI_116.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>da patranha do aquecimento global</PARA>...   \n",
       "1  <PARA>Denmark to Punish Farmers for cow ‘emiss...   \n",
       "2  <PARA>‘disastrous’ flood warning in California...   \n",
       "3  <PARA>rússia acusa ucrânia de preparar ataque ...   \n",
       "4  <PARA>russia-ukraine conflict: क्या यूक्रेन सं...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [Questioning the measurements and science, Hid...   \n",
       "1  [Criticism of institutions and authorities, Cr...   \n",
       "2                         [Amplifying Climate Fears]   \n",
       "3  [Discrediting the West, Diplomacy, Discreditin...   \n",
       "4  [Amplifying war-related fears, Russia is the V...   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [Scientific community is unreliable, Climate a...   \n",
       "1  [Criticism of national governments, Other, Met...   \n",
       "2      [Amplifying existing fears of global warming]   \n",
       "3  [Other, Other, Discrediting Ukrainian governme...   \n",
       "4  [By continuing the war we risk WWIII, The West...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "         narrative_hierarchy_8  narrative_hierarchy_9 narrative_hierarchy_17  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 1, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 1, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0]     [0, 0, 0, 0, 0, 0]   \n",
       "3  [1, 0, 0, 1, 0, 0, 0, 1, 1]  [0, 1, 0, 0, 0, 0, 0]     [1, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0]     [0, 1, 0, 0, 0, 0]   \n",
       "\n",
       "   ... narrative_hierarchy_0 narrative_hierarchy_15 narrative_hierarchy_7  \\\n",
       "0  ...       [1, 0, 0, 0, 0]                    [1]       [1, 0, 0, 0, 0]   \n",
       "1  ...       [1, 0, 0, 0, 0]                    [1]       [1, 1, 1, 0, 0]   \n",
       "2  ...       [0, 1, 0, 0, 0]                    [0]       [0, 0, 0, 0, 0]   \n",
       "3  ...       [1, 0, 0, 0, 0]                    [1]       [1, 0, 0, 0, 0]   \n",
       "4  ...       [0, 0, 0, 0, 0]                    [0]       [0, 0, 0, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_5       narrative_hierarchy_11 narrative_hierarchy_6  \\\n",
       "0          [1, 1, 0, 0]  [1, 1, 0, 1, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "1          [0, 1, 0, 0]  [1, 0, 0, 0, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "2          [0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0]          [0, 0, 0, 0]   \n",
       "3          [0, 1, 0, 0]  [1, 0, 0, 0, 0, 0, 0, 0, 0]          [0, 1, 0, 0]   \n",
       "4          [0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0]          [0, 0, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_18 narrative_hierarchy_3 narrative_hierarchy_4  \\\n",
       "0        [0, 1, 1, 0, 0]             [1, 0, 1]          [0, 1, 0, 0]   \n",
       "1        [1, 1, 0, 1, 0]             [1, 0, 0]          [0, 1, 0, 0]   \n",
       "2        [0, 0, 0, 0, 0]             [0, 0, 0]          [0, 0, 0, 0]   \n",
       "3        [0, 1, 0, 0, 0]             [1, 0, 0]          [0, 1, 0, 0]   \n",
       "4        [0, 0, 0, 0, 0]             [0, 0, 0]          [0, 0, 0, 0]   \n",
       "\n",
       "  narrative_hierarchy_12  \n",
       "0              [0, 1, 0]  \n",
       "1              [0, 1, 0]  \n",
       "2              [0, 0, 0]  \n",
       "3              [0, 1, 0]  \n",
       "4              [0, 0, 0]  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_cpy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed68aa1-856d-4e76-a332-5640670bf9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "4       [0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "                   ...             \n",
      "1358    [1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1362    [1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_8, Length: 1363, dtype: object\n",
      "0       [0, 1, 0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 0, 0, 0]\n",
      "2       [0, 1, 0, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0, 0, 0]\n",
      "                ...          \n",
      "1358    [0, 1, 0, 0, 0, 0, 0]\n",
      "1359    [0, 1, 0, 0, 0, 0, 0]\n",
      "1360    [0, 1, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0, 0, 0]\n",
      "1362    [0, 1, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_9, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0, 0]\n",
      "               ...        \n",
      "1358    [1, 0, 1, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0, 0]\n",
      "1362    [1, 0, 1, 0, 0, 1]\n",
      "Name: narrative_hierarchy_17, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0]\n",
      "1       [0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 1, 1]\n",
      "            ...     \n",
      "1358    [1, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0]\n",
      "1362    [1, 0, 0, 1]\n",
      "Name: narrative_hierarchy_19, Length: 1363, dtype: object\n",
      "0       [1, 0, 0]\n",
      "1       [0, 0, 0]\n",
      "2       [1, 0, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 0, 0]\n",
      "          ...    \n",
      "1358    [1, 0, 0]\n",
      "1359    [1, 0, 0]\n",
      "1360    [1, 0, 0]\n",
      "1361    [0, 0, 0]\n",
      "1362    [1, 0, 0]\n",
      "Name: narrative_hierarchy_10, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0]\n",
      "             ...       \n",
      "1358    [1, 0, 1, 0, 0]\n",
      "1359    [1, 0, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0]\n",
      "1362    [1, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_1, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0]\n",
      "1       [0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0]\n",
      "            ...     \n",
      "1358    [1, 0, 0, 1]\n",
      "1359    [1, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0]\n",
      "1362    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_16, Length: 1363, dtype: object\n",
      "0       [0, 0, 1]\n",
      "1       [0, 0, 0]\n",
      "2       [0, 0, 1]\n",
      "3       [1, 0, 0]\n",
      "4       [0, 1, 0]\n",
      "          ...    \n",
      "1358    [0, 0, 1]\n",
      "1359    [0, 1, 1]\n",
      "1360    [0, 0, 1]\n",
      "1361    [0, 0, 0]\n",
      "1362    [0, 0, 1]\n",
      "Name: narrative_hierarchy_2, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0]\n",
      "1       [0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0]\n",
      "            ...     \n",
      "1358    [1, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0]\n",
      "1362    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_20, Length: 1363, dtype: object\n",
      "0       [1, 1, 0]\n",
      "1       [0, 0, 0]\n",
      "2       [1, 0, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 0, 0]\n",
      "          ...    \n",
      "1358    [1, 0, 0]\n",
      "1359    [1, 0, 0]\n",
      "1360    [1, 0, 0]\n",
      "1361    [0, 1, 1]\n",
      "1362    [1, 0, 0]\n",
      "Name: narrative_hierarchy_13, Length: 1363, dtype: object\n",
      "0       [0, 1, 0]\n",
      "1       [0, 0, 0]\n",
      "2       [0, 1, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 0, 0]\n",
      "          ...    \n",
      "1358    [0, 1, 0]\n",
      "1359    [0, 1, 0]\n",
      "1360    [0, 1, 0]\n",
      "1361    [0, 0, 0]\n",
      "1362    [0, 1, 0]\n",
      "Name: narrative_hierarchy_14, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0]\n",
      "             ...       \n",
      "1358    [1, 0, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0]\n",
      "1362    [1, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_0, Length: 1363, dtype: object\n",
      "0       [1]\n",
      "1       [0]\n",
      "2       [1]\n",
      "3       [0]\n",
      "4       [0]\n",
      "       ... \n",
      "1358    [1]\n",
      "1359    [1]\n",
      "1360    [1]\n",
      "1361    [0]\n",
      "1362    [1]\n",
      "Name: narrative_hierarchy_15, Length: 1363, dtype: object\n",
      "0       [1, 0, 0, 0, 1]\n",
      "1       [0, 0, 1, 0, 1]\n",
      "2       [1, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0]\n",
      "             ...       \n",
      "1358    [1, 0, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0, 0]\n",
      "1361    [0, 0, 1, 0, 1]\n",
      "1362    [1, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_7, Length: 1363, dtype: object\n",
      "0       [0, 1, 0, 0]\n",
      "1       [0, 0, 0, 0]\n",
      "2       [0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0]\n",
      "            ...     \n",
      "1358    [0, 1, 0, 0]\n",
      "1359    [0, 1, 0, 0]\n",
      "1360    [0, 1, 0, 0]\n",
      "1361    [0, 0, 0, 0]\n",
      "1362    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_5, Length: 1363, dtype: object\n",
      "0       [1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "1       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2       [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "                   ...             \n",
      "1358    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1359    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1360    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1362    [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_11, Length: 1363, dtype: object\n",
      "0       [0, 1, 0, 0]\n",
      "1       [0, 0, 0, 0]\n",
      "2       [0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0]\n",
      "            ...     \n",
      "1358    [0, 1, 0, 0]\n",
      "1359    [0, 1, 0, 0]\n",
      "1360    [0, 1, 0, 0]\n",
      "1361    [0, 0, 0, 0]\n",
      "1362    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_6, Length: 1363, dtype: object\n",
      "0       [0, 1, 1, 0, 0]\n",
      "1       [1, 0, 0, 1, 0]\n",
      "2       [0, 1, 0, 0, 0]\n",
      "3       [0, 0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0, 0]\n",
      "             ...       \n",
      "1358    [0, 1, 0, 0, 0]\n",
      "1359    [0, 1, 0, 0, 0]\n",
      "1360    [0, 1, 0, 0, 0]\n",
      "1361    [0, 0, 0, 0, 0]\n",
      "1362    [0, 1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_18, Length: 1363, dtype: object\n",
      "0       [1, 0, 0]\n",
      "1       [0, 0, 0]\n",
      "2       [1, 0, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 0, 0]\n",
      "          ...    \n",
      "1358    [1, 0, 0]\n",
      "1359    [1, 0, 0]\n",
      "1360    [1, 0, 0]\n",
      "1361    [0, 0, 0]\n",
      "1362    [1, 0, 0]\n",
      "Name: narrative_hierarchy_3, Length: 1363, dtype: object\n",
      "0       [0, 1, 0, 0]\n",
      "1       [0, 0, 0, 0]\n",
      "2       [0, 1, 0, 0]\n",
      "3       [0, 0, 0, 0]\n",
      "4       [0, 0, 0, 0]\n",
      "            ...     \n",
      "1358    [0, 1, 0, 0]\n",
      "1359    [0, 1, 0, 0]\n",
      "1360    [0, 1, 0, 0]\n",
      "1361    [0, 0, 0, 0]\n",
      "1362    [0, 1, 0, 0]\n",
      "Name: narrative_hierarchy_4, Length: 1363, dtype: object\n",
      "0       [0, 1, 0]\n",
      "1       [0, 0, 0]\n",
      "2       [0, 1, 0]\n",
      "3       [0, 0, 0]\n",
      "4       [0, 0, 0]\n",
      "          ...    \n",
      "1358    [0, 1, 0]\n",
      "1359    [0, 1, 0]\n",
      "1360    [0, 1, 0]\n",
      "1361    [0, 0, 0]\n",
      "1362    [0, 1, 0]\n",
      "Name: narrative_hierarchy_12, Length: 1363, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    column_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "    res = dataset_train_cpy[column_name]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed4fea0-adaf-4bce-b35f-aed1a9ce7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort order of narratives to start from hierarchy 0\n",
    "narrative_order = sorted(narrative_to_sub_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28118762-97a6-4dac-8cf0-0a0f7b3c972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03d3f065-4abf-422c-b85d-c6879a3c157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subnarratives(row, narrative_order, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row, and aggregates all hierarchy columns to 1 list.\n",
    "    The encoded list will be a list of lists, starting from the first hierarchy\"\"\"\n",
    "    aggregated = []\n",
    "    for narr_idx in narrative_order:\n",
    "        column_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "        sub_labels = row[column_name]\n",
    "        aggregated.append(sub_labels)\n",
    "    return aggregated\n",
    "\n",
    "dataset_train['aggregated_subnarratives'] = dataset_train_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")\n",
    "\n",
    "dataset_val['aggregated_subnarratives'] = dataset_val_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e76fa06-fa05-4b24-ab16-2862f7df659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "1       [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "2       [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "3       [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 0, 0], ...\n",
       "4       [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 1, 0], ...\n",
       "                              ...                        \n",
       "1358    [[1, 0, 0, 0, 0], [1, 0, 1, 0, 0], [0, 0, 1], ...\n",
       "1359    [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 1, 1], ...\n",
       "1360    [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "1361    [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0], ...\n",
       "1362    [[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1], ...\n",
       "Name: aggregated_subnarratives, Length: 1363, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['aggregated_subnarratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1968bf35-e93b-465e-b0a2-742953b808bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee760da7-ecad-4690-8c05-46c33b447ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff6872b-c06e-4450-b232-7d812eff12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3acef3-fa00-4d9e-bec9-0f9cacd9fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Shared layer\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Top-level narratives: multi-label => Sigmoid\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Subnarrative heads: multi-label => Sigmoid\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71b02d39-8af6-4cae-bde3-b82a663fbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head = MultiTaskClassifierMultiHead(\n",
    "    input_size=input_size,\n",
    "    hidden_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bfde290-d982-49fa-a89a-b48bb81a1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "y_val_nar = dataset_val['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n",
    "y_val_sub_nar = dataset_val['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18b1217b-9e4b-4fcc-8cb1-4274f4bdd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32)\n",
    "\n",
    "y_val_nar = torch.tensor(y_val_nar, dtype=torch.float32)\n",
    "y_val_sub_nar = torch.tensor(y_val_sub_nar, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b730da-7939-4cdd-b725-a95d84624222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a10fe2bc-989e-40f5-9779-14d70ec69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)\n",
    "\n",
    "class_weights_sub_nar = compute_class_weights(y_val_sub_nar)\n",
    "class_weights_nar = compute_class_weights(y_val_nar)\n",
    "narrative_criterion = WeightedBCELoss(class_weights_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b8e8505-b560-4307-85a3-7ef738cfb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subnarrative head, add a weighted version of BCE based on the indices\n",
    "sub_criterion_dict = {}\n",
    "\n",
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    local_weights = [ class_weights_sub_nar[sub_i] for sub_i in sub_indices ]\n",
    "\n",
    "    sub_criterion = WeightedBCELoss(local_weights)\n",
    "    sub_criterion_dict[str(narr_idx)] = sub_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3863a1f5-eb9e-4247-9e0d-405799431c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_loss(narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "    narr_loss = narrative_criterion(narr_probs, y_narr)\n",
    "\n",
    "    sub_loss = 0.0\n",
    "    count_active = 0\n",
    "    i = 0\n",
    "    for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "        narr_idx = int(narr_idx_str)\n",
    "        # Find the true subnarratives for the batch\n",
    "        y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "        y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32)\n",
    "\n",
    "        sub_loss_func = sub_criterion_dict[narr_idx_str]\n",
    "        ce_loss = sub_loss_func(sub_probs, y_sub_tensor)\n",
    "\n",
    "        sub_loss += ce_loss\n",
    "        count_active += 1\n",
    "        i += 1\n",
    "\n",
    "    if count_active > 0:\n",
    "        sub_loss = sub_loss / count_active\n",
    "    else:\n",
    "        sub_loss = 0.0\n",
    "\n",
    "    total_loss = narr_loss + sub_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f7c1e38-c06b-4285-9d46-0c338ce8c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    narrative_criterion,\n",
    "    train_embeddings=train_embeddings_tensor,\n",
    "    y_train_nar=y_train_nar,\n",
    "    y_train_sub_heads=y_train_sub_heads,\n",
    "    val_embeddings=val_embeddings_tensor,\n",
    "    y_val_nar=y_val_nar,\n",
    "    y_val_sub_heads=y_val_sub_heads,\n",
    "    patience=3,\n",
    "    num_epochs=100,\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "        train_loss = multi_head_loss(train_narr_probs, train_sub_probs_dict, y_train_nar, y_train_sub_heads)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            val_loss = multi_head_loss(val_narr_probs, val_sub_probs_dict, y_val_nar, y_val_sub_heads)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {train_loss.item():.4f} \"\n",
    "              f\"Validation Loss: {val_loss.item():.4f} \")\n",
    "\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ec5cf11-220e-48db-be7f-d3ce7aa5e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_multi_head = torch.optim.AdamW(model_multi_head.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a501a7b4-6d0c-4c3f-bf4b-33015a1b73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.4464 Validation Loss: 1.3690 \n",
      "Epoch 2/100, Training Loss: 1.1742 Validation Loss: 1.3619 \n",
      "Epoch 3/100, Training Loss: 1.0581 Validation Loss: 1.3547 \n",
      "Epoch 4/100, Training Loss: 0.9719 Validation Loss: 1.3474 \n",
      "Epoch 5/100, Training Loss: 0.9001 Validation Loss: 1.3400 \n",
      "Epoch 6/100, Training Loss: 0.8471 Validation Loss: 1.3323 \n",
      "Epoch 7/100, Training Loss: 0.7993 Validation Loss: 1.3243 \n",
      "Epoch 8/100, Training Loss: 0.7585 Validation Loss: 1.3159 \n",
      "Epoch 9/100, Training Loss: 0.7267 Validation Loss: 1.3071 \n",
      "Epoch 10/100, Training Loss: 0.6961 Validation Loss: 1.2979 \n",
      "Epoch 11/100, Training Loss: 0.6702 Validation Loss: 1.2884 \n",
      "Epoch 12/100, Training Loss: 0.6452 Validation Loss: 1.2787 \n",
      "Epoch 13/100, Training Loss: 0.6211 Validation Loss: 1.2684 \n",
      "Epoch 14/100, Training Loss: 0.5989 Validation Loss: 1.2578 \n",
      "Epoch 15/100, Training Loss: 0.5814 Validation Loss: 1.2469 \n",
      "Epoch 16/100, Training Loss: 0.5603 Validation Loss: 1.2360 \n",
      "Epoch 17/100, Training Loss: 0.5436 Validation Loss: 1.2251 \n",
      "Epoch 18/100, Training Loss: 0.5298 Validation Loss: 1.2140 \n",
      "Epoch 19/100, Training Loss: 0.5119 Validation Loss: 1.2030 \n",
      "Epoch 20/100, Training Loss: 0.4969 Validation Loss: 1.1923 \n",
      "Epoch 21/100, Training Loss: 0.4831 Validation Loss: 1.1819 \n",
      "Epoch 22/100, Training Loss: 0.4715 Validation Loss: 1.1714 \n",
      "Epoch 23/100, Training Loss: 0.4595 Validation Loss: 1.1610 \n",
      "Epoch 24/100, Training Loss: 0.4460 Validation Loss: 1.1504 \n",
      "Epoch 25/100, Training Loss: 0.4329 Validation Loss: 1.1399 \n",
      "Epoch 26/100, Training Loss: 0.4220 Validation Loss: 1.1294 \n",
      "Epoch 27/100, Training Loss: 0.4111 Validation Loss: 1.1191 \n",
      "Epoch 28/100, Training Loss: 0.4001 Validation Loss: 1.1091 \n",
      "Epoch 29/100, Training Loss: 0.3846 Validation Loss: 1.0990 \n",
      "Epoch 30/100, Training Loss: 0.3751 Validation Loss: 1.0897 \n",
      "Epoch 31/100, Training Loss: 0.3652 Validation Loss: 1.0807 \n",
      "Epoch 32/100, Training Loss: 0.3570 Validation Loss: 1.0723 \n",
      "Epoch 33/100, Training Loss: 0.3456 Validation Loss: 1.0640 \n",
      "Epoch 34/100, Training Loss: 0.3339 Validation Loss: 1.0560 \n",
      "Epoch 35/100, Training Loss: 0.3256 Validation Loss: 1.0487 \n",
      "Epoch 36/100, Training Loss: 0.3157 Validation Loss: 1.0425 \n",
      "Epoch 37/100, Training Loss: 0.3077 Validation Loss: 1.0373 \n",
      "Epoch 38/100, Training Loss: 0.2993 Validation Loss: 1.0323 \n",
      "Epoch 39/100, Training Loss: 0.2881 Validation Loss: 1.0280 \n",
      "Epoch 40/100, Training Loss: 0.2793 Validation Loss: 1.0232 \n",
      "Epoch 41/100, Training Loss: 0.2732 Validation Loss: 1.0192 \n",
      "Epoch 42/100, Training Loss: 0.2645 Validation Loss: 1.0151 \n",
      "Epoch 43/100, Training Loss: 0.2540 Validation Loss: 1.0135 \n",
      "Epoch 44/100, Training Loss: 0.2475 Validation Loss: 1.0158 \n",
      "Validation loss did not improve for 1 epoch(s).\n",
      "Epoch 45/100, Training Loss: 0.2387 Validation Loss: 1.0170 \n",
      "Validation loss did not improve for 2 epoch(s).\n",
      "Epoch 46/100, Training Loss: 0.2315 Validation Loss: 1.0209 \n",
      "Validation loss did not improve for 3 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiTaskClassifierMultiHead(\n",
       "  (shared_layer): Sequential(\n",
       "    (0): Linear(in_features=896, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (narrative_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=21, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (subnarrative_heads): ModuleDict(\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=6, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_multihead(\n",
    "    model=model_multi_head,\n",
    "    optimizer=optimizer_multi_head,\n",
    "    narrative_criterion=narrative_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b1ec872-c847-41c3-b2ab-4e1d87d48465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_multihead_model(\n",
    "    model,\n",
    "    embeddings,\n",
    "    y_nar_true,\n",
    "    y_sub_hierarchical,\n",
    "    num_subnarratives = len(mlb_subnarratives.classes_),\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1),\n",
    "    target_names_nar=mlb_narratives.classes_,\n",
    "    target_names_sub=mlb_subnarratives.classes_,\n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    def build_global_sub_array(\n",
    "        y_sub_hierarchical,\n",
    "        num_subnarratives=74,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "    ):\n",
    "        \"\"\"Reconstructs the subnarratives to flatten them (again) to a single array for evaluation\"\"\"\n",
    "        num_samples = len(y_sub_hierarchical)\n",
    "        sub_global_array = np.zeros((num_samples, num_subnarratives), dtype=int)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            for j, narr_idx in enumerate(narrative_order):\n",
    "                sub_label_vec = y_sub_hierarchical[i][j]\n",
    "                narr_idx = int(narr_idx)\n",
    "                sub_indices = narrative_to_sub_map[narr_idx]\n",
    "                for local_sub_i, global_sub_i in enumerate(sub_indices):\n",
    "                    sub_global_array[i, global_sub_i] = sub_label_vec[local_sub_i]\n",
    "\n",
    "        return sub_global_array\n",
    "\n",
    "    embeddings = embeddings.to(device)\n",
    "    y_nar_true_np = y_nar_true.cpu().numpy()\n",
    "\n",
    "    best_threshold = 0\n",
    "    best_f1 = -1\n",
    "    best_report_nar = None\n",
    "    best_report_sub = None\n",
    "    samples = len(embeddings)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # get the predictions for both\n",
    "        narr_probs, sub_probs_dict = model(embeddings)\n",
    "\n",
    "        narr_probs = narr_probs.cpu().numpy()\n",
    "        for k in sub_probs_dict:\n",
    "            sub_probs_dict[k] = sub_probs_dict[k].cpu().numpy()\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        narr_preds = (narr_probs >= threshold).astype(int)\n",
    "\n",
    "        # Need to reconstruct the subnarratives to flatten them (again) to a single array for evaluation\n",
    "        sub_preds_global = np.zeros((samples, num_subnarratives), dtype=int)\n",
    "\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            # Get the predictions for this narrative hierarchy\n",
    "            sub_probs_for_narr = sub_probs_dict[str(narr_idx)]\n",
    "            # If top-level narrative is 1, then threshold subnarratives; otherwise 0.\n",
    "            # Finds for each sample, go to the narr_idx position (the hierarchy we are at)\n",
    "            predicted_narr_mask = narr_preds[:, narr_idx] == 1  # shape (num_samples,)\n",
    "\n",
    "            # For all samples, threshold sub_probs_for_narr:\n",
    "            sub_preds_for_narr = (sub_probs_for_narr >= threshold).astype(int)\n",
    "\n",
    "            # But only keep sub_preds_for_narr if predicted_narr_mask is True:\n",
    "            # If predicted_narr_mask is False for a sample, subnarratives go to 0.\n",
    "            for sample_idx in range(samples):\n",
    "                if predicted_narr_mask[sample_idx] == 1:\n",
    "                    # Construct the flattened pred array\n",
    "                    for local_sub_i, global_sub_i in enumerate(sub_indices):\n",
    "                        sub_preds_global[sample_idx, global_sub_i] = sub_preds_for_narr[sample_idx, local_sub_i]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        f1_nar = f1_score(y_nar_true_np, narr_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # Also flatten the true y_sub to a single array in the same way as we did with the predictions\n",
    "        y_sub_true_np = build_global_sub_array(y_sub_hierarchical, num_subnarratives=num_subnarratives)\n",
    "\n",
    "        f1_sub = f1_score(y_sub_true_np, sub_preds_global, average=\"macro\", zero_division=0)\n",
    "\n",
    "        avg_f1 = (f1_nar + f1_sub) / 2.0\n",
    "\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "            report_nar = classification_report(\n",
    "                y_nar_true_np,\n",
    "                narr_preds,\n",
    "                target_names=target_names_nar,\n",
    "                zero_division=0\n",
    "            )\n",
    "            report_sub = classification_report(\n",
    "                y_sub_true_np,\n",
    "                sub_preds_global,\n",
    "                target_names=target_names_sub,\n",
    "                zero_division=0\n",
    "            )\n",
    "            best_report_nar = report_nar\n",
    "            best_report_sub = report_sub\n",
    "\n",
    "    print(f\"Best threshold = {best_threshold:.2f}, best (avg) F1 = {best_f1:.4f}\")\n",
    "    print(\"Best Narratives classification report:\")\n",
    "    print(best_report_nar)\n",
    "    print(\"Best Subnarratives classification report:\")\n",
    "    print(best_report_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e873134-0fe1-4b60-bdf9-f0eee66ec99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold = 0.50, best (avg) F1 = 0.4102\n",
      "Best Narratives classification report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                         Amplifying Climate Fears       0.66      0.94      0.77        49\n",
      "                     Amplifying war-related fears       0.58      0.76      0.66        42\n",
      "Blaming the war on others rather than the invader       0.28      0.61      0.39        36\n",
      "                     Climate change is beneficial       1.00      0.33      0.50         3\n",
      "             Controversy about green technologies       0.29      0.40      0.33         5\n",
      "                    Criticism of climate movement       0.39      0.75      0.51        12\n",
      "                    Criticism of climate policies       0.55      0.90      0.68        20\n",
      "        Criticism of institutions and authorities       0.74      0.88      0.81        33\n",
      "                             Discrediting Ukraine       0.65      0.78      0.71        81\n",
      "                 Discrediting the West, Diplomacy       0.66      0.60      0.63        70\n",
      "                           Distrust towards Media       0.67      0.36      0.47        11\n",
      "                       Downplaying climate change       0.64      0.58      0.61        12\n",
      "      Green policies are geopolitical instruments       0.00      0.00      0.00         2\n",
      "Hidden plots by secret schemes of powerful groups       0.56      0.60      0.58        15\n",
      "               Negative Consequences for the West       0.36      0.36      0.36        22\n",
      "                                            Other       0.64      0.60      0.62        65\n",
      "                            Overpraising the West       0.14      0.25      0.18         8\n",
      "                                 Praise of Russia       0.54      0.74      0.63        66\n",
      "         Questioning the measurements and science       1.00      0.60      0.75         5\n",
      "                             Russia is the Victim       0.46      0.63      0.54        41\n",
      "                         Speculating war outcomes       0.21      0.61      0.31        18\n",
      "\n",
      "                                        micro avg       0.53      0.69      0.60       616\n",
      "                                        macro avg       0.52      0.59      0.53       616\n",
      "                                     weighted avg       0.56      0.69      0.61       616\n",
      "                                      samples avg       0.54      0.68      0.57       616\n",
      "\n",
      "Best Subnarratives classification report:\n",
      "                                                                        precision    recall  f1-score   support\n",
      "\n",
      "                                   Ad hominem attacks on key activists       0.30      0.75      0.43         4\n",
      "                           Amplifying existing fears of global warming       0.70      0.89      0.78        36\n",
      "                                                 Blaming global elites       0.22      0.50      0.31         4\n",
      "                                   By continuing the war we risk WWIII       0.30      0.58      0.40        12\n",
      "                    CO2 concentrations are too small to have an impact       0.50      1.00      0.67         1\n",
      "                                                     CO2 is beneficial       0.00      0.00      0.00         1\n",
      "                                     Climate agenda has hidden motives       0.38      0.71      0.50         7\n",
      "                                            Climate cycles are natural       0.22      0.67      0.33         3\n",
      "                                          Climate movement is alarmist       0.16      0.60      0.25         5\n",
      "                                           Climate movement is corrupt       0.09      0.50      0.15         2\n",
      "                                      Climate policies are ineffective       0.16      0.83      0.27         6\n",
      "                                  Climate policies are only for profit       0.20      0.50      0.29         4\n",
      "                  Climate policies have negative impact on the economy       0.33      0.89      0.48         9\n",
      "      Climate-related international relations are abusive/exploitative       0.00      0.00      0.00         0\n",
      "                                   Criticism of international entities       0.20      0.67      0.31         6\n",
      "                                     Criticism of national governments       0.39      0.78      0.52        18\n",
      "                      Criticism of political organizations and figures       0.31      0.79      0.45        14\n",
      "                                                   Criticism of the EU       0.33      0.50      0.40         2\n",
      "                                    Data shows no temperature increase       0.00      0.00      0.00         0\n",
      "                                          Diplomacy does/will not work       0.18      0.33      0.23         9\n",
      "          Discrediting Ukrainian government and officials and policies       0.40      0.66      0.50        32\n",
      "                                       Discrediting Ukrainian military       0.27      0.80      0.41        20\n",
      "                             Discrediting Ukrainian nation and society       0.00      0.00      0.00         2\n",
      "                                         Doomsday scenarios for humans       0.21      0.75      0.33         8\n",
      "                                      Earth will be uninhabitable soon       0.22      1.00      0.36         4\n",
      "                        Green activities are a form of neo-colonialism       0.00      0.00      0.00         1\n",
      "          Greenhouse effect/carbon dioxide do not drive climate change       0.00      0.00      0.00         1\n",
      "                         Human activities do not impact climate change       0.00      0.00      0.00         2\n",
      "                           Humans and nature will adapt to the changes       0.00      0.00      0.00         1\n",
      "                                                    Ice is not melting       0.00      0.00      0.00         2\n",
      "                      Methodologies/metrics used are unreliable/faulty       0.50      0.50      0.50         2\n",
      "                                   NATO should/will directly intervene       0.14      0.60      0.23         5\n",
      "                                              NATO will destroy Russia       0.00      0.00      0.00         1\n",
      "                                                                 Other       0.70      0.68      0.69       187\n",
      "                            Praise of Russian President Vladimir Putin       0.17      0.22      0.19         9\n",
      "                                      Praise of Russian military might       0.38      0.69      0.49        29\n",
      "                                            Renewable energy is costly       0.67      1.00      0.80         2\n",
      "                                         Renewable energy is dangerous       0.50      0.50      0.50         2\n",
      "                                        Renewable energy is unreliable       1.00      0.67      0.80         3\n",
      "                                           Rewriting Ukraine’s history       0.33      1.00      0.50         1\n",
      "                       Russia actions in Ukraine are only self-defence       0.29      0.79      0.42        14\n",
      "Russia has international support from a number of countries and people       0.44      0.75      0.56        16\n",
      "                         Russia is a guarantor of peace and prosperity       0.35      0.62      0.45        21\n",
      "                               Russia will also attack other countries       0.23      0.64      0.34        11\n",
      "                                            Russian army is collapsing       0.20      0.33      0.25         3\n",
      "                   Russian army will lose all the occupied territories       0.00      0.00      0.00         1\n",
      "                          Russian invasion has strong national support       0.50      1.00      0.67         1\n",
      "                  Sanctions imposed by Western countries will backfire       0.33      0.14      0.20         7\n",
      "                                    Scientific community is unreliable       1.00      0.50      0.67         4\n",
      "                                             Sea levels are not rising       0.00      0.00      0.00         1\n",
      "                                      Situation in Ukraine is hopeless       0.12      0.25      0.17        12\n",
      "                 Temperature increase does not have significant impact       0.00      0.00      0.00         1\n",
      "                                    Temperature increase is beneficial       0.00      0.00      0.00         2\n",
      "                                                     The EU is divided       0.12      0.25      0.17         8\n",
      "                                           The West are the aggressors       0.28      0.48      0.35        23\n",
      "                         The West belongs in the right side of history       0.17      0.33      0.22         3\n",
      "        The West does not care about Ukraine, only about its interests       0.29      0.41      0.34        17\n",
      "                      The West has the strongest international support       0.00      0.00      0.00         4\n",
      "                                              The West is overreacting       0.00      0.00      0.00         2\n",
      "                                               The West is russophobic       0.26      0.36      0.30        14\n",
      "                                                      The West is weak       0.13      0.17      0.15        12\n",
      "      The conflict will increase the Ukrainian refugee flows to Europe       0.00      0.00      0.00         1\n",
      "     There is a real possibility that nuclear weapons will be employed       0.72      0.65      0.68        20\n",
      "                                              UA is anti-RU extremists       0.00      0.00      0.00         3\n",
      "                              Ukraine is a hub for criminal activities       0.19      0.75      0.31         8\n",
      "                                       Ukraine is a puppet of the West       0.40      0.67      0.50        21\n",
      "                                     Ukraine is associated with nazism       0.22      0.62      0.32         8\n",
      "                                              Ukraine is the aggressor       0.23      0.69      0.35        16\n",
      "                                          Ukrainian army is collapsing       0.10      0.29      0.14         7\n",
      "                                     Ukrainian media cannot be trusted       0.00      0.00      0.00         1\n",
      "                          Weather suggests the trend is global cooling       0.00      0.00      0.00         0\n",
      "                                              West is tired of Ukraine       0.43      0.75      0.55         4\n",
      "                          Western media is an instrument of propaganda       0.50      0.38      0.43         8\n",
      "                                 Whatever we do it is already too late       0.14      0.50      0.22         2\n",
      "\n",
      "                                                             micro avg       0.36      0.61      0.45       733\n",
      "                                                             macro avg       0.25      0.44      0.30       733\n",
      "                                                          weighted avg       0.43      0.61      0.48       733\n",
      "                                                           samples avg       0.40      0.64      0.45       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_multihead_model(\n",
    "    model=model_multi_head,\n",
    "    embeddings=val_embeddings_tensor,\n",
    "    y_nar_true=y_val_nar,\n",
    "    y_sub_hierarchical=y_val_sub_heads,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
