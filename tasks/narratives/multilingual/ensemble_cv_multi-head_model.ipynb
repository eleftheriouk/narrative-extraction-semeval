{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc359a13-e78a-40a6-8481-56d671a82533",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f31eb2-7e66-43ca-b9a9-d89784d5e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de79452-a38d-4014-b0db-b256ef3b826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "if random_state:\n",
    "    print('[WARNING] Setting random state')\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state) \n",
    "    random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267d496-fc3d-49e0-8082-5b005af2c867",
   "metadata": {},
   "source": [
    "## Ensemble Model Using Cross-Validation\n",
    "\n",
    "As of now, we’ve been training on a combined dataset of all languages to handle limited data. Our final submission will focus on a single test set, but we want to leverage as much training data as possible.\n",
    "\n",
    "One practical approach is to perform n-fold cross-validation on our combined set, which creates n different models—each learning slightly different patterns. \n",
    "* Then, at prediction time, we can load each fold model and average their outputs.\n",
    "  By combining multiple models it is said that, we smooth out any biases or quirks of individual folds and often get more robust predictions.\n",
    "* This ensemble method will help us get the most out of our data while still producing a single set of final predictions for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0798ffdf-4d77-4061-bda1-9628da7ee37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "root_dir = \"../../\"\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d279dc8-b4c5-4e6d-acd3-dc5f2006cf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "1  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6765b0-232d-4f50-bbf4-adf35b52dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf092afa-741c-402d-8198-f0cf016afe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives_map.pkl'), 'rb') as f:\n",
    "    narrative_to_sub_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf22a7b-911d-4ad7-a282-670ba92959c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'coarse_classes.pkl'), 'rb') as f:\n",
    "    coarse_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'fine_classes.pkl'), 'rb') as f:\n",
    "    fine_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_order.pkl'), 'rb') as f:\n",
    "    narrative_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76727fac-2b2a-4908-82f5-a1209c1bb3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ce6eef-209f-433f-b993-bbe9ecac98d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URW: Discrediting Ukraine': ['Ukraine is a puppet of the West',\n",
       "  'Rewriting Ukraine’s history',\n",
       "  'Ukraine is a hub for criminal activities',\n",
       "  'Discrediting Ukrainian nation and society',\n",
       "  'Discrediting Ukrainian government and officials and policies',\n",
       "  'Discrediting Ukrainian military',\n",
       "  'Other',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Ukraine is associated with nazism'],\n",
       " 'URW: Discrediting the West, Diplomacy': ['West is tired of Ukraine',\n",
       "  'Diplomacy does/will not work',\n",
       "  'The West is weak',\n",
       "  'The EU is divided',\n",
       "  'The West does not care about Ukraine, only about its interests',\n",
       "  'Other',\n",
       "  'The West is overreacting'],\n",
       " 'URW: Praise of Russia': ['Russia has international support from a number of countries and people',\n",
       "  'Praise of Russian military might',\n",
       "  'Russia is a guarantor of peace and prosperity',\n",
       "  'Other',\n",
       "  'Praise of Russian President Vladimir Putin',\n",
       "  'Russian invasion has strong national support'],\n",
       " 'URW: Russia is the Victim': ['The West is russophobic',\n",
       "  'UA is anti-RU extremists',\n",
       "  'Other',\n",
       "  'Russia actions in Ukraine are only self-defence'],\n",
       " 'URW: Distrust towards Media': ['Western media is an instrument of propaganda',\n",
       "  'Ukrainian media cannot be trusted',\n",
       "  'Other'],\n",
       " 'URW: Amplifying war-related fears': ['By continuing the war we risk WWIII',\n",
       "  'There is a real possibility that nuclear weapons will be employed',\n",
       "  'Russia will also attack other countries',\n",
       "  'Other',\n",
       "  'NATO should/will directly intervene'],\n",
       " 'URW: Blaming the war on others rather than the invader': ['Ukraine is the aggressor',\n",
       "  'The West are the aggressors',\n",
       "  'Other'],\n",
       " 'URW: Overpraising the West': ['The West belongs in the right side of history',\n",
       "  'NATO will destroy Russia',\n",
       "  'Other',\n",
       "  'The West has the strongest international support'],\n",
       " 'URW: Speculating war outcomes': ['Other',\n",
       "  'Ukrainian army is collapsing',\n",
       "  'Russian army is collapsing',\n",
       "  'Russian army will lose all the occupied territories'],\n",
       " 'URW: Hidden plots by secret schemes of powerful groups': ['Other'],\n",
       " 'Other': ['Other'],\n",
       " 'URW: Negative Consequences for the West': ['The conflict will increase the Ukrainian refugee flows to Europe',\n",
       "  'Sanctions imposed by Western countries will backfire',\n",
       "  'Other'],\n",
       " 'CC: Amplifying Climate Fears': ['Doomsday scenarios for humans',\n",
       "  'Whatever we do it is already too late',\n",
       "  'Amplifying existing fears of global warming',\n",
       "  'Other',\n",
       "  'Earth will be uninhabitable soon'],\n",
       " 'CC: Criticism of institutions and authorities': ['Criticism of national governments',\n",
       "  'Other',\n",
       "  'Criticism of political organizations and figures',\n",
       "  'Criticism of international entities',\n",
       "  'Criticism of the EU'],\n",
       " 'CC: Criticism of climate movement': ['Ad hominem attacks on key activists',\n",
       "  'Climate movement is corrupt',\n",
       "  'Other',\n",
       "  'Climate movement is alarmist'],\n",
       " 'CC: Downplaying climate change': ['Ice is not melting',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Temperature increase does not have significant impact',\n",
       "  'Humans and nature will adapt to the changes',\n",
       "  'Sea levels are not rising',\n",
       "  'Other',\n",
       "  'CO2 concentrations are too small to have an impact',\n",
       "  'Human activities do not impact climate change',\n",
       "  'Climate cycles are natural'],\n",
       " 'CC: Criticism of climate policies': ['Climate policies are only for profit',\n",
       "  'Climate policies are ineffective',\n",
       "  'Other',\n",
       "  'Climate policies have negative impact on the economy'],\n",
       " 'CC: Questioning the measurements and science': ['Data shows no temperature increase',\n",
       "  'Scientific community is unreliable',\n",
       "  'Methodologies/metrics used are unreliable/faulty',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Other'],\n",
       " 'CC: Hidden plots by secret schemes of powerful groups': ['Blaming global elites',\n",
       "  'Climate agenda has hidden motives',\n",
       "  'Other'],\n",
       " 'CC: Climate change is beneficial': ['Temperature increase is beneficial',\n",
       "  'Other',\n",
       "  'CO2 is beneficial'],\n",
       " 'CC: Controversy about green technologies': ['Renewable energy is costly',\n",
       "  'Renewable energy is dangerous',\n",
       "  'Renewable energy is unreliable',\n",
       "  'Other'],\n",
       " 'CC: Green policies are geopolitical instruments': ['Climate-related international relations are abusive/exploitative',\n",
       "  'Green activities are a form of neo-colonialism',\n",
       "  'Other']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cfa15c7-687f-4871-8a3d-b872cd632124",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77339ce-78a0-4344-92a0-8f36c514314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_train_stella.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "train_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ecc917-221b-416d-add9-38357edb6443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337138fd-e00b-48c3-90f3-d6060fc9fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_and_embeddings(dataset, embeddings, condition_fn):\n",
    "    filtered_indices = dataset.index[dataset.apply(condition_fn, axis=1)].tolist()\n",
    "    \n",
    "    filtered_dataset = dataset.loc[filtered_indices]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "\n",
    "    return filtered_dataset, filtered_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f6c8f89-b82a-42d6-bf57-1af05734720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aee4215-a420-4261-a249-e2127f3ea099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc5b75f-faa3-44e1-afe9-7e4a54cc221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_dev_stella.npy')\n",
    "\n",
    "val_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4f10a-b96c-43c6-b842-3cc9fdc187a7",
   "metadata": {},
   "source": [
    "We keep as target language for final evaluation the English dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ccd7f07-5d5c-49ff-8d50-a84ae9437628",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lang = \"EN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffa92fb3-a44c-4392-810e-c03fcaab16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_target, val_embeddings_target = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == target_lang\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dc365d4-7971-43f8-8bde-c9a90a9a498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_non_target, val_embeddings_non_target = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] != target_lang\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4e333-9238-43e0-89a9-1397c55ac21a",
   "metadata": {},
   "source": [
    "We combine both train and val datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f4854f3-9ba0-426a-bee7-9bca8b73a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combined = pd.concat([dataset_train, dataset_val_non_target], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82031f4a-1bcf-43e2-904a-79942ef74e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "prefer_cpu=True\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() and not prefer_cpu\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777afd4c-b9ca-44a7-a84a-2c315e746ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings_non_target, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37519854-a8eb-4377-874f-766c2d515fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_target_tensor = torch.tensor(val_embeddings_target, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f1fb0-7013-4184-a4a2-6ee21b9bc57c",
   "metadata": {},
   "source": [
    "We also do the same for the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be890e76-ea7d-4ad4-af58-16be91b08e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_combined = torch.cat([train_embeddings_tensor, val_embeddings_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3571206-3413-4868-b07a-6bdca8cc003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_shuffling(data, embeddings):\n",
    "    shuffled_indices = np.arange(len(data))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "    embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    return data, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "991d6642-28c8-4cee-86ea-7b1e55386f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, embeddings = custom_shuffling(dataset_combined, embeddings_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cdf2c20-f46e-4007-98ea-1ea2602888ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_UA_019640.txt</td>\n",
       "      <td>&lt;PARA&gt;after North Korea’s Kim Jong Un, Putin a...</td>\n",
       "      <td>[URW: Praise of Russia, URW: Russia is the Vic...</td>\n",
       "      <td>[Russia has international support from a numbe...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_81.txt</td>\n",
       "      <td>&lt;PARA&gt;deputada do chega em cascais não poupa o...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is associated with nazism, Ukraine is...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_UA_300050.txt</td>\n",
       "      <td>&lt;PARA&gt;is EU kowtowing to Germany's interests?&lt;...</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HI</td>\n",
       "      <td>HI_244.txt</td>\n",
       "      <td>&lt;PARA&gt;रूस ने यूक्रेन के ऊर्जा संयंत्रों को बना...</td>\n",
       "      <td>[URW: Speculating war outcomes]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_699.txt</td>\n",
       "      <td>&lt;PARA&gt;«държавите от НАТО създадоха за милиарди...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Blaming the w...</td>\n",
       "      <td>[Ukraine is a puppet of the West, The West are...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language        article_id  \\\n",
       "0       EN  EN_UA_019640.txt   \n",
       "1       PT         PT_81.txt   \n",
       "2       EN  EN_UA_300050.txt   \n",
       "3       HI        HI_244.txt   \n",
       "4       BG        BG_699.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>after North Korea’s Kim Jong Un, Putin a...   \n",
       "1  <PARA>deputada do chega em cascais não poupa o...   \n",
       "2  <PARA>is EU kowtowing to Germany's interests?<...   \n",
       "3  <PARA>रूस ने यूक्रेन के ऊर्जा संयंत्रों को बना...   \n",
       "4  <PARA>«държавите от НАТО създадоха за милиарди...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Praise of Russia, URW: Russia is the Vic...   \n",
       "1  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "2                                            [Other]   \n",
       "3                    [URW: Speculating war outcomes]   \n",
       "4  [URW: Discrediting Ukraine, URW: Blaming the w...   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [Russia has international support from a numbe...   \n",
       "1  [Ukraine is associated with nazism, Ukraine is...   \n",
       "2                                            [Other]   \n",
       "3                                            [Other]   \n",
       "4  [Ukraine is a puppet of the West, The West are...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "1  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "2  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "3  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63265bcf-6a01-4dbc-98d4-3ad555b4a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub_heads = dataset['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e40ce54-7b7f-438c-b69d-ad340c0e44f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "input_size = embeddings.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ded6266-8004-45bf-8110-102378630944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=1024,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db5cc04c-8223-4956-a196-be29352c87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 1024,\n",
    "    'dropout': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca024dba-3271-42d3-8699-4ab35a3f2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nar = dataset['narratives_encoded'].tolist()\n",
    "\n",
    "y_sub_nar = dataset['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f40bfd3f-4350-47e4-b327-8eab082eb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nar = torch.tensor(y_nar, dtype=torch.float32).to(device)\n",
    "y_sub_nar = torch.tensor(y_sub_nar, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4284e389-b29a-4aad-a339-5ac2b263d83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears',\n",
       " 'CC: Climate change is beneficial',\n",
       " 'CC: Controversy about green technologies',\n",
       " 'CC: Criticism of climate movement',\n",
       " 'CC: Criticism of climate policies']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "630377d4-c001-41aa-bed0-9352cb18dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7eeb49a-a41e-450f-96f4-ace246e788d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears: Amplifying existing fears of global warming',\n",
       " 'CC: Amplifying Climate Fears: Doomsday scenarios for humans',\n",
       " 'CC: Amplifying Climate Fears: Earth will be uninhabitable soon',\n",
       " 'CC: Amplifying Climate Fears: Other',\n",
       " 'CC: Amplifying Climate Fears: Whatever we do it is already too late']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67076b-b2ab-4711-899c-81db6b57fa78",
   "metadata": {},
   "source": [
    "We define the same evaluator we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1e6d91c-39d0-406c-8aad-19ad26ab709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "class MultiHeadEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes_coarse=coarse_classes,\n",
    "        classes_fine=fine_classes,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu',\n",
    "        output_dir='../../../submissions',\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.narrative_classes = list(narrative_classes)\n",
    "        self.subnarrative_classes = list(subnarrative_classes)\n",
    "        \n",
    "        self.classes_coarse = classes_coarse\n",
    "        self.classes_fine = classes_fine\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings=val_embeddings_tensor,\n",
    "        dataset=dataset_val_non_target,\n",
    "        thresholds=None,\n",
    "        save=False,\n",
    "        std_weight=0.4,\n",
    "        lower_thres=0.1,\n",
    "        upper_thres=0.60\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(lower_thres, upper_thres, 0.05)\n",
    "\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        best_results = {\n",
    "            \"best_coarse_f1\": -1,\n",
    "            \"best_coarse_std\": float(\"inf\"),\n",
    "            \"best_fine_f1\": -1,\n",
    "            \"best_fine_std\": float(\"inf\"),\n",
    "            \"narr_threshold\": 0,\n",
    "            \"sub_threshold\": 0,\n",
    "            \"predictions\": None,\n",
    "            \"best_combined_score\": -float(\"inf\"),\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "\n",
    "        for narr_threshold in thresholds:\n",
    "            for sub_threshold in thresholds:\n",
    "                predictions = []\n",
    "                for sample_idx, row in dataset.iterrows():\n",
    "                    pred = self._make_prediction(\n",
    "                        row[\"article_id\"],\n",
    "                        sample_idx,\n",
    "                        narr_probs,\n",
    "                        sub_probs_dict,\n",
    "                        narr_threshold,\n",
    "                        sub_threshold,\n",
    "                    )\n",
    "                    predictions.append(pred)\n",
    "\n",
    "                f1_coarse_mean, coarse_std, f1_fine_mean, fine_std = self._compute_metrics_coarse_fine(\n",
    "                    predictions, dataset\n",
    "                )\n",
    "                combined_score = f1_fine_mean - (std_weight * coarse_std)\n",
    "\n",
    "                if combined_score > best_results[\"best_combined_score\"]:\n",
    "                    best_results.update(\n",
    "                        {\n",
    "                            \"best_coarse_f1\": f1_coarse_mean,\n",
    "                            \"best_coarse_std\": coarse_std,\n",
    "                            \"best_fine_f1\": f1_fine_mean,\n",
    "                            \"best_fine_std\": fine_std,\n",
    "                            \"narr_threshold\": narr_threshold,\n",
    "                            \"sub_threshold\": sub_threshold,\n",
    "                            \"predictions\": predictions,\n",
    "                            \"best_combined_score\": combined_score,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        print(\"\\nBest thresholds found:\")\n",
    "        print(f\"Narrative threshold: {best_results['narr_threshold']:.2f}\")\n",
    "        print(f\"Subnarrative threshold: {best_results['sub_threshold']:.2f}\")\n",
    "        print(\"\\nCompetition Values\")\n",
    "        print(f\"Coarse-F1: {best_results['best_coarse_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. coarse: {best_results['best_coarse_std']:.3f}\")\n",
    "        print(f\"Fine-F1: {best_results['best_fine_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. fine: {best_results['best_fine_std']:.3f}\")\n",
    "\n",
    "        if save:\n",
    "            self._save_predictions(best_results, os.path.join(self.output_dir, \"submission.txt\"))\n",
    "\n",
    "        return best_results\n",
    "\n",
    "    def _make_prediction(self, article_id, sample_idx, narr_probs, sub_probs_dict, narr_threshold, sub_threshold):\n",
    "        other_idx = self.narrative_classes.index(\"Other\")\n",
    "        active_narratives = [\n",
    "            (n_idx, prob)\n",
    "            for n_idx, prob in enumerate(narr_probs[sample_idx])\n",
    "            if n_idx != other_idx and prob >= narr_threshold\n",
    "        ]\n",
    "        # Fallback, If no active narrartive, output \"Other\" for both\n",
    "        # narrative and subnarratives.\n",
    "        if not active_narratives:\n",
    "            return {\n",
    "                'article_id': article_id,\n",
    "                'narratives': [\"Other\"],\n",
    "                'pairs': [\"Other\"]\n",
    "            }\n",
    "        \n",
    "        narratives = []\n",
    "        pairs = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        active_narratives.sort(key=lambda x: x[1], reverse=True)\n",
    "        for narr_idx, _ in active_narratives:\n",
    "            narr_name = self.narrative_classes[narr_idx]\n",
    "            \n",
    "            sub_probs = sub_probs_dict[str(narr_idx)][sample_idx]\n",
    "            # FInd active subnarratives based on the cur threshold\n",
    "            active_subnarratives = [\n",
    "                (local_idx, s_prob)\n",
    "                for local_idx, s_prob in enumerate(sub_probs)\n",
    "                if s_prob >= sub_threshold\n",
    "            ]\n",
    "            # If no active subnarrative, output the predicted Narrative, with Other\n",
    "            # as a pair.\n",
    "            active_subnarratives.sort(key=lambda x: x[1], reverse=True)\n",
    "            if not active_subnarratives:\n",
    "                pairs.append(f\"{narr_name}: Other\")\n",
    "            else:\n",
    "                for local_idx, _ in active_subnarratives:\n",
    "                    global_sub_idx = self.narrative_to_sub_map[narr_idx][local_idx]\n",
    "                    sub_name = self.subnarrative_classes[global_sub_idx]\n",
    "                    pair = f\"{narr_name}: {sub_name}\"\n",
    "                    if pair not in seen_pairs:\n",
    "                        pairs.append(pair)\n",
    "                        seen_pairs.add(pair)\n",
    "            narratives.append(narr_name)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'narratives': narratives,\n",
    "            'pairs': pairs\n",
    "        }\n",
    "\n",
    "    def _compute_metrics_coarse_fine(self, predictions, dataset):\n",
    "        \"\"\"\n",
    "        Evaluates the problem predictions with the gold.\n",
    "        Mimics the challenge evaluation function.\n",
    "        \"\"\"\n",
    "        gold_coarse_all = []\n",
    "        gold_fine_all = []\n",
    "        pred_coarse_all = []\n",
    "        pred_fine_all = []\n",
    "\n",
    "        for pred, (_, row) in zip(predictions, dataset.iterrows()):\n",
    "            gold_coarse = row['narratives']\n",
    "            gold_subnarratives = row['subnarratives']\n",
    "            \n",
    "            pred_coarse = pred['narratives']\n",
    "            pred_fine = []\n",
    "            for p in pred['pairs']:\n",
    "                if p == \"Other\":\n",
    "                    pred_fine.append(\"Other\")\n",
    "                else:\n",
    "                    pred_fine.append(p)\n",
    "\n",
    "            gold_fine = []\n",
    "            for gold_nar, gold_sub in zip(gold_coarse, gold_subnarratives):\n",
    "                if gold_nar == \"Other\":\n",
    "                    gold_fine.append(\"Other\")\n",
    "                else:\n",
    "                    gold_fine.append(f\"{gold_nar}: {gold_sub}\")\n",
    "            \n",
    "            gold_coarse_all.append(gold_coarse)\n",
    "            gold_fine_all.append(gold_fine)\n",
    "            pred_coarse_all.append(pred_coarse)\n",
    "            pred_fine_all.append(pred_fine)\n",
    "\n",
    "        f1_coarse_mean, coarse_std = self._evaluate_multi_label(gold_coarse_all, pred_coarse_all, self.classes_coarse)\n",
    "        f1_fine_mean, fine_std = self._evaluate_multi_label(gold_fine_all, pred_fine_all, self.classes_fine)\n",
    "        \n",
    "        gold_coarse_flat = []\n",
    "        pred_coarse_flat = []\n",
    "        for g_labels, p_labels in zip(gold_coarse_all, pred_coarse_all):\n",
    "            g_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    g_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            p_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    p_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            gold_coarse_flat.append(g_onehot)\n",
    "            pred_coarse_flat.append(p_onehot)\n",
    "        gold_coarse_flat = np.array(gold_coarse_flat)\n",
    "        pred_coarse_flat = np.array(pred_coarse_flat)\n",
    "        report_coarse = metrics.classification_report(\n",
    "            gold_coarse_flat, pred_coarse_flat, target_names=self.classes_coarse, zero_division=0\n",
    "        )\n",
    "        \n",
    "        gold_fine_flat = []\n",
    "        pred_fine_flat = []\n",
    "        for g_labels, p_labels in zip(gold_fine_all, pred_fine_all):\n",
    "            g_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    g_onehot[self.classes_fine.index(lab)] = 1\n",
    "            p_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    p_onehot[self.classes_fine.index(lab)] = 1\n",
    "            gold_fine_flat.append(g_onehot)\n",
    "            pred_fine_flat.append(p_onehot)\n",
    "        gold_fine_flat = np.array(gold_fine_flat)\n",
    "        pred_fine_flat = np.array(pred_fine_flat)\n",
    "\n",
    "        \n",
    "        return f1_coarse_mean, coarse_std, f1_fine_mean, fine_std\n",
    "\n",
    "    def _evaluate_multi_label(self, gold, predicted, class_list):\n",
    "        \"\"\"\n",
    "        Evaluates the predicted, with the gold and returns the mean and std f1 scores.\n",
    "        Mimics the challenge evaluation function.\n",
    "        \"\"\"\n",
    "        f1_scores = []\n",
    "        for g_labels, p_labels in zip(gold, predicted):\n",
    "            g_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in class_list:\n",
    "                    g_onehot[class_list.index(lab)] = 1\n",
    "                    \n",
    "            p_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in class_list:\n",
    "                    p_onehot[class_list.index(lab)] = 1\n",
    "\n",
    "            f1_doc = metrics.f1_score(g_onehot, p_onehot, zero_division=0)\n",
    "            f1_scores.append(f1_doc)\n",
    "        \n",
    "        return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "    def _save_predictions(self, best_results, filepath):\n",
    "        predictions = best_results['predictions']\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                line = (f\"{pred['article_id']}\\t\"\n",
    "                        f\"{';'.join(pred['narratives'])}\\t\"\n",
    "                        f\"{';'.join(pred['pairs'])}\\n\")\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17623237-9ec8-41b0-9244-a46a8b9ed930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e052178-2256-4347-bc64-3ea1b3e5db97",
   "metadata": {},
   "source": [
    "We also define the same loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54e384f0-c67c-4f6b-9748-5761c280537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=0.3, sub_weight=0.3):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        \n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "        \n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            \n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_loss += sub_loss_func(sub_probs, y_sub_tensor)\n",
    "\n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.mean(\n",
    "                # Penalize high probs of sub, based on first level narr predictinos\n",
    "                torch.abs(sub_probs * (1 - narr_pred)) + \n",
    "                # If a narrative is true, then the subnarrative predictions should match their actual true values.\n",
    "                narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            )\n",
    "            condition_loss += condition_term\n",
    "            \n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        \n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + \\\n",
    "                    self.sub_weight * sub_loss + \\\n",
    "                    self.condition_weight * condition_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4301557f-da13-4af7-9212-c50e2d6bb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    train_embeddings,\n",
    "    y_train_nar,\n",
    "    y_train_sub_heads,\n",
    "    val_embeddings,\n",
    "    y_val_nar,\n",
    "    y_val_sub_heads,\n",
    "    patience=10,\n",
    "    num_epochs=100,\n",
    "    scheduler=None,\n",
    "    min_delta=0.001,\n",
    "    show_progress=True\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "        train_loss = loss_fn(train_narr_probs, train_sub_probs_dict, y_train_nar, y_train_sub_heads)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            val_loss = loss_fn(val_narr_probs, val_sub_probs_dict, y_val_nar, y_val_sub_heads)\n",
    "            \n",
    "        if show_progress:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                  f\"Training Loss: {train_loss.item():.4f}, \"\n",
    "                  f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if val_loss.item() < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not significantly improve for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec801bb2-e5d7-4554-b40a-62b2dfa0e608",
   "metadata": {},
   "source": [
    "We first create a splitter, shuffle data and iterate each fold.\n",
    "- Each fold gives us a train and val idx in order for us to access the train and val in that fold.\n",
    "- We load our model, optimizer and class weights and then we train like usual.\n",
    "- After that, we validate our model on the validation embeddings for that fold.\n",
    "- Lastly, we save those models, their configs, and thresholds for later use during inference.\n",
    "- We finally averages the performance across folds so that we can get an estimate of how the model does on different splits of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85606afd-7e17-477c-8ba8-f2b76f5276b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "class CrossValEnsembleTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class,\n",
    "        embeddings=embeddings,\n",
    "        dataset=dataset,\n",
    "        y_nar=y_nar,\n",
    "        y_sub_heads=y_sub_heads,\n",
    "        input_size=input_size,\n",
    "        hidden_size=1024,\n",
    "        dropout_rate=0.4,\n",
    "        target_dataset=dataset_val_target,\n",
    "        target_embed=val_embeddings_target_tensor,\n",
    "        lr=0.001,\n",
    "        n_splits=5,\n",
    "        patience=10,\n",
    "        num_epochs=100,\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.embeddings = embeddings\n",
    "        self.dataset = dataset\n",
    "        self.y_nar = y_nar\n",
    "        self.y_sub_heads = y_sub_heads\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lr = lr\n",
    "        self.n_splits = n_splits\n",
    "        self.patience = patience\n",
    "        self.num_epochs = num_epochs\n",
    "        self.target_dataset = target_dataset\n",
    "        self.target_embed=target_embed\n",
    "        \n",
    "        self.fold_models = []\n",
    "        self.fold_results = []\n",
    "\n",
    "    def fit(self, sub_weight=0.3, condition_weight=0.3):\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.embeddings.cpu(), self.dataset['language'])):\n",
    "            print(f\"\\nTraining Fold {fold + 1}/{self.n_splits}\")\n",
    "            \n",
    "            train_embeddings = self.embeddings[train_idx].to(device)\n",
    "            val_embeddings_fold = self.embeddings[val_idx].to(device)\n",
    "            \n",
    "            train_nar = self.y_nar[train_idx].to(device)\n",
    "            val_nar = self.y_nar[val_idx].to(device)\n",
    "            \n",
    "            train_sub = self.y_sub_heads[train_idx]\n",
    "            val_sub = self.y_sub_heads[val_idx]\n",
    "            \n",
    "            model = self.model_class(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                dropout_rate=self.dropout_rate\n",
    "            ).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "\n",
    "            class_weights_nar_fold = compute_class_weights(train_nar)\n",
    "            narrative_criterion_fold = WeightedBCELoss(class_weights_nar_fold).to(device)\n",
    "            \n",
    "            sub_criterion_dict_fold = {}\n",
    "            for narr_idx_str in model.subnarrative_heads.keys():\n",
    "                narr_idx = int(narr_idx_str)\n",
    "                y_sub_fold = torch.tensor([row[narr_idx] for row in train_sub]).to(device)\n",
    "                class_weights_sub = compute_class_weights(y_sub_fold)\n",
    "                sub_criterion_dict_fold[narr_idx_str] = WeightedBCELoss(class_weights_sub).to(device)\n",
    "\n",
    "            loss_fn = MultiHeadLoss(\n",
    "                narrative_criterion=narrative_criterion_fold,\n",
    "                sub_criterion_dict=sub_criterion_dict_fold,\n",
    "                sub_weight=sub_weight,\n",
    "                condition_weight=condition_weight\n",
    "            )\n",
    "            \n",
    "            trained_model = train_with_multihead(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                train_embeddings=train_embeddings,\n",
    "                y_train_nar=train_nar,\n",
    "                y_train_sub_heads=train_sub,\n",
    "                val_embeddings=val_embeddings_fold,\n",
    "                y_val_nar=val_nar,\n",
    "                y_val_sub_heads=val_sub,\n",
    "                patience=self.patience,\n",
    "                num_epochs=self.num_epochs,\n",
    "                show_progress=False\n",
    "            )\n",
    "            \n",
    "            evaluator = MultiHeadEvaluator()\n",
    "            fold_dataset = self.dataset.iloc[val_idx].reset_index(drop=True)\n",
    "            metrics = evaluator.evaluate(\n",
    "                trained_model, \n",
    "                val_embeddings_fold,\n",
    "                dataset=fold_dataset\n",
    "            )\n",
    "            \n",
    "            self.fold_models.append(trained_model.state_dict())\n",
    "            self.fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'metrics': metrics,\n",
    "                'val_indices': val_idx\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nFold {fold + 1} Results:\")\n",
    "            print(f\"Coarse-F1: {metrics['best_coarse_f1']:.3f}\")\n",
    "            print(f\"Fine-F1: {metrics['best_fine_f1']:.3f}\")\n",
    "\n",
    "        self._display_cv_performance()\n",
    "        return self.fold_models, self.fold_results\n",
    "\n",
    "\n",
    "    def _display_cv_performance(self):\n",
    "        print(\"\\nCross-Validation Performance:\")\n",
    "        avg_coarse_f1 = np.mean([r['metrics']['best_coarse_f1'] for r in self.fold_results])\n",
    "        std_coarse_f1 = np.std([r['metrics']['best_coarse_f1'] for r in self.fold_results])\n",
    "        avg_fine_f1 = np.mean([r['metrics']['best_fine_f1'] for r in self.fold_results])\n",
    "        std_fine_f1 = np.std([r['metrics']['best_fine_f1'] for r in self.fold_results])\n",
    "        \n",
    "        print(f\"Average Coarse-F1: {avg_coarse_f1:.3f} ± {std_coarse_f1:.3f}\")\n",
    "        print(f\"Average Fine-F1:   {avg_fine_f1:.3f} ± {std_fine_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef28f2d0-f7e5-437b-af6e-1e36aeaac5fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.576\n",
      "F1 st. dev. coarse: 0.381\n",
      "Fine-F1: 0.416\n",
      "F1 st. dev. fine: 0.352\n",
      "\n",
      "Fold 1 Results:\n",
      "Coarse-F1: 0.576\n",
      "Fine-F1: 0.416\n",
      "\n",
      "Training Fold 2/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.597\n",
      "F1 st. dev. coarse: 0.383\n",
      "Fine-F1: 0.415\n",
      "F1 st. dev. fine: 0.346\n",
      "\n",
      "Fold 2 Results:\n",
      "Coarse-F1: 0.597\n",
      "Fine-F1: 0.415\n",
      "\n",
      "Training Fold 3/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.55\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.608\n",
      "F1 st. dev. coarse: 0.372\n",
      "Fine-F1: 0.454\n",
      "F1 st. dev. fine: 0.366\n",
      "\n",
      "Fold 3 Results:\n",
      "Coarse-F1: 0.608\n",
      "Fine-F1: 0.454\n",
      "\n",
      "Training Fold 4/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.55\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.585\n",
      "F1 st. dev. coarse: 0.375\n",
      "Fine-F1: 0.418\n",
      "F1 st. dev. fine: 0.358\n",
      "\n",
      "Fold 4 Results:\n",
      "Coarse-F1: 0.585\n",
      "Fine-F1: 0.418\n",
      "\n",
      "Training Fold 5/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.606\n",
      "F1 st. dev. coarse: 0.378\n",
      "Fine-F1: 0.434\n",
      "F1 st. dev. fine: 0.356\n",
      "\n",
      "Fold 5 Results:\n",
      "Coarse-F1: 0.606\n",
      "Fine-F1: 0.434\n",
      "\n",
      "Cross-Validation Performance:\n",
      "Average Coarse-F1: 0.595 ± 0.012\n",
      "Average Fine-F1:   0.427 ± 0.015\n"
     ]
    }
   ],
   "source": [
    "simple_trainer = CrossValEnsembleTrainer(\n",
    "    model_class=MultiTaskClassifierMultiHead,\n",
    "    hidden_size=1024,\n",
    "    dropout_rate=0.4,\n",
    "    lr=0.001,\n",
    "    n_splits=5,\n",
    "    patience=10,\n",
    "    num_epochs=100,\n",
    ")\n",
    "ensemble_simple_models, simple_fold_results = simple_trainer.fit(sub_weight=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb96d90-d933-4d71-9f81-794ebd5f9ce6",
   "metadata": {},
   "source": [
    "We can also see how our model did in a separated validation dataset that has articles targeted to the desired language we want to validate for. For that we need to create a wrapper with a forward method to calculate the predictions from embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f41f560-fe32-4af1-8579-6a35ad66773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVEnsembleWrapper:\n",
    "    def __init__(self, fold_models, model_class, input_size, hidden_size, dropout_rate):\n",
    "        self.fold_models = fold_models\n",
    "        self.model_class = model_class\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self, embeddings):\n",
    "        models = []\n",
    "        for state_dict in self.fold_models:\n",
    "            model = self.model_class(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                dropout_rate=self.dropout_rate\n",
    "            ).to(device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "\n",
    "        all_narr_probs = []\n",
    "        all_sub_probs_dicts = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                narr_probs, sub_probs_dict = model(embeddings)\n",
    "                all_narr_probs.append(narr_probs)\n",
    "                all_sub_probs_dicts.append(sub_probs_dict)\n",
    "\n",
    "        avg_narr_probs = torch.mean(torch.stack(all_narr_probs), dim=0)\n",
    "\n",
    "        avg_sub_probs_dict = {}\n",
    "        for key in all_sub_probs_dicts[0].keys():\n",
    "            sub_probs_stack = torch.stack([d[key] for d in all_sub_probs_dicts])\n",
    "            avg_sub_probs_dict[key] = torch.mean(sub_probs_stack, dim=0)\n",
    "\n",
    "        return avg_narr_probs, avg_sub_probs_dict\n",
    "\n",
    "    def eval(self):\n",
    "        return self\n",
    "\n",
    "    def to(self, device):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74522d8c-4030-4bf9-8c83-7fab176e7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ensemble_simple = CVEnsembleWrapper(\n",
    "    fold_models=ensemble_simple_models,\n",
    "    model_class=MultiTaskClassifierMultiHead,\n",
    "    input_size=input_size,\n",
    "    hidden_size=1024,\n",
    "    dropout_rate=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38372510-5025-46e3-a575-ee1447af6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultiHeadEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f2e23-aea3-4b16-8529-e062ea1431db",
   "metadata": {},
   "source": [
    "We also create a evaluator wrapper for the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e9e2167-606d-476e-a2f6-efcdc8528060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(\n",
    "    base_evaluator,\n",
    "    ensemble_model,\n",
    "    embeddings,\n",
    "    dataset,\n",
    "    save=False,\n",
    "):\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "    \n",
    "    results = base_evaluator.evaluate(\n",
    "        model=ensemble_model,\n",
    "        embeddings=embeddings,\n",
    "        dataset=dataset,\n",
    "        save=save\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "282ca5e8-5dfd-4184-98bd-ee4d334350f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.466\n",
      "F1 st. dev. coarse: 0.399\n",
      "Fine-F1: 0.335\n",
      "F1 st. dev. fine: 0.356\n"
     ]
    }
   ],
   "source": [
    "evaluator = MultiHeadEvaluator(device=device)\n",
    "results = evaluate_ensemble(\n",
    "    base_evaluator=evaluator,\n",
    "    ensemble_model=cv_ensemble_simple,\n",
    "    embeddings=val_embeddings_target,\n",
    "    dataset=dataset_val_target.reset_index(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d752254e-edf2-41b5-9e72-b8837e844a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassifierMultiHeadConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout']\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2 + 1, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            conditioned_input = torch.cat((shared_out, narr_probs[:, int(narr_idx)].unsqueeze(1)), dim=1)\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ebf3f9d-de55-4332-a9d2-33816fe2d276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.581\n",
      "F1 st. dev. coarse: 0.380\n",
      "Fine-F1: 0.405\n",
      "F1 st. dev. fine: 0.347\n",
      "\n",
      "Fold 1 Results:\n",
      "Coarse-F1: 0.581\n",
      "Fine-F1: 0.405\n",
      "\n",
      "Training Fold 2/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.600\n",
      "F1 st. dev. coarse: 0.371\n",
      "Fine-F1: 0.401\n",
      "F1 st. dev. fine: 0.344\n",
      "\n",
      "Fold 2 Results:\n",
      "Coarse-F1: 0.600\n",
      "Fine-F1: 0.401\n",
      "\n",
      "Training Fold 3/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.608\n",
      "F1 st. dev. coarse: 0.375\n",
      "Fine-F1: 0.446\n",
      "F1 st. dev. fine: 0.354\n",
      "\n",
      "Fold 3 Results:\n",
      "Coarse-F1: 0.608\n",
      "Fine-F1: 0.446\n",
      "\n",
      "Training Fold 4/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.55\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.581\n",
      "F1 st. dev. coarse: 0.375\n",
      "Fine-F1: 0.411\n",
      "F1 st. dev. fine: 0.351\n",
      "\n",
      "Fold 4 Results:\n",
      "Coarse-F1: 0.581\n",
      "Fine-F1: 0.411\n",
      "\n",
      "Training Fold 5/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.607\n",
      "F1 st. dev. coarse: 0.379\n",
      "Fine-F1: 0.431\n",
      "F1 st. dev. fine: 0.361\n",
      "\n",
      "Fold 5 Results:\n",
      "Coarse-F1: 0.607\n",
      "Fine-F1: 0.431\n",
      "\n",
      "Cross-Validation Performance:\n",
      "Average Coarse-F1: 0.596 ± 0.012\n",
      "Average Fine-F1:   0.419 ± 0.017\n"
     ]
    }
   ],
   "source": [
    "trainer_concat = CrossValEnsembleTrainer(\n",
    "    model_class=MultiTaskClassifierMultiHeadConcat,\n",
    "    hidden_size=2048,\n",
    "    dropout_rate=0.4,\n",
    "    lr=0.001,\n",
    "    n_splits=5,\n",
    "    patience=10,\n",
    "    num_epochs=100,\n",
    ")\n",
    "ensemble_models_concat, fold_results_concat = trainer_concat.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2dffd26-6c18-4d9e-a223-f51b9eb77d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ensemble_concat = CVEnsembleWrapper(\n",
    "    fold_models=ensemble_models_concat,\n",
    "    model_class=MultiTaskClassifierMultiHeadConcat,\n",
    "    input_size=input_size,\n",
    "    hidden_size=2048,\n",
    "    dropout_rate=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cc014ff-285e-4b17-947c-e24a2b4ed156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.478\n",
      "F1 st. dev. coarse: 0.388\n",
      "Fine-F1: 0.338\n",
      "F1 st. dev. fine: 0.336\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_ensemble(\n",
    "    base_evaluator=evaluator,\n",
    "    ensemble_model=cv_ensemble_concat,\n",
    "    embeddings=val_embeddings_target,\n",
    "    dataset=dataset_val_target.reset_index(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
