{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc359a13-e78a-40a6-8481-56d671a82533",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267d496-fc3d-49e0-8082-5b005af2c867",
   "metadata": {},
   "source": [
    "## Ensemble Model Using Cross-Validation\n",
    "\n",
    "As of now, we’ve been training on a combined dataset of all languages to handle limited data. Our final submission will focus on a single test set, but we want to leverage as much training data as possible.\n",
    "\n",
    "One practical approach is to perform n-fold cross-validation on our combined set, which creates n different models—each learning slightly different patterns. \n",
    "* Then, at prediction time, we can load each fold model and average their outputs.\n",
    "  By combining multiple models it is said that, we smooth out any biases or quirks of individual folds and often get more robust predictions.\n",
    "* This ensemble method will help us get the most out of our data while still producing a single set of final predictions for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a67f7-6996-4d2f-9009-7979fb276831",
   "metadata": {},
   "source": [
    "We start by again loading our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0798ffdf-4d77-4061-bda1-9628da7ee37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"../../\"\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d279dc8-b4c5-4e6d-acd3-dc5f2006cf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6765b0-232d-4f50-bbf4-adf35b52dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76727fac-2b2a-4908-82f5-a1209c1bb3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ce6eef-209f-433f-b993-bbe9ecac98d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URW: Discrediting Ukraine': ['Rewriting Ukraine’s history',\n",
       "  'Ukraine is associated with nazism',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Discrediting Ukrainian nation and society',\n",
       "  'Discrediting Ukrainian military',\n",
       "  'Ukraine is a puppet of the West',\n",
       "  'Ukraine is a hub for criminal activities',\n",
       "  'Other',\n",
       "  'Discrediting Ukrainian government and officials and policies'],\n",
       " 'URW: Discrediting the West, Diplomacy': ['The EU is divided',\n",
       "  'West is tired of Ukraine',\n",
       "  'The West is weak',\n",
       "  'The West does not care about Ukraine, only about its interests',\n",
       "  'Other',\n",
       "  'The West is overreacting',\n",
       "  'Diplomacy does/will not work'],\n",
       " 'URW: Praise of Russia': ['Praise of Russian military might',\n",
       "  'Russian invasion has strong national support',\n",
       "  'Russia has international support from a number of countries and people',\n",
       "  'Praise of Russian President Vladimir Putin',\n",
       "  'Other',\n",
       "  'Russia is a guarantor of peace and prosperity'],\n",
       " 'URW: Russia is the Victim': ['Russia actions in Ukraine are only self-defence',\n",
       "  'The West is russophobic',\n",
       "  'UA is anti-RU extremists',\n",
       "  'Other'],\n",
       " 'URW: Distrust towards Media': ['Ukrainian media cannot be trusted',\n",
       "  'Other',\n",
       "  'Western media is an instrument of propaganda'],\n",
       " 'URW: Amplifying war-related fears': ['By continuing the war we risk WWIII',\n",
       "  'There is a real possibility that nuclear weapons will be employed',\n",
       "  'NATO should/will directly intervene',\n",
       "  'Other',\n",
       "  'Russia will also attack other countries'],\n",
       " 'URW: Blaming the war on others rather than the invader': ['Other',\n",
       "  'Ukraine is the aggressor',\n",
       "  'The West are the aggressors'],\n",
       " 'URW: Overpraising the West': ['The West has the strongest international support',\n",
       "  'The West belongs in the right side of history',\n",
       "  'Other',\n",
       "  'NATO will destroy Russia'],\n",
       " 'URW: Speculating war outcomes': ['Russian army will lose all the occupied territories',\n",
       "  'Russian army is collapsing',\n",
       "  'Other',\n",
       "  'Ukrainian army is collapsing'],\n",
       " 'URW: Hidden plots by secret schemes of powerful groups': ['Other'],\n",
       " 'Other': ['Other'],\n",
       " 'URW: Negative Consequences for the West': ['Sanctions imposed by Western countries will backfire',\n",
       "  'The conflict will increase the Ukrainian refugee flows to Europe',\n",
       "  'Other'],\n",
       " 'CC: Amplifying Climate Fears': ['Earth will be uninhabitable soon',\n",
       "  'Doomsday scenarios for humans',\n",
       "  'Whatever we do it is already too late',\n",
       "  'Other',\n",
       "  'Amplifying existing fears of global warming'],\n",
       " 'CC: Criticism of institutions and authorities': ['Criticism of national governments',\n",
       "  'Criticism of international entities',\n",
       "  'Criticism of the EU',\n",
       "  'Other',\n",
       "  'Criticism of political organizations and figures'],\n",
       " 'CC: Criticism of climate movement': ['Climate movement is corrupt',\n",
       "  'Climate movement is alarmist',\n",
       "  'Other',\n",
       "  'Ad hominem attacks on key activists'],\n",
       " 'CC: Downplaying climate change': ['Human activities do not impact climate change',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Temperature increase does not have significant impact',\n",
       "  'Ice is not melting',\n",
       "  'Climate cycles are natural',\n",
       "  'CO2 concentrations are too small to have an impact',\n",
       "  'Humans and nature will adapt to the changes',\n",
       "  'Other',\n",
       "  'Sea levels are not rising'],\n",
       " 'CC: Criticism of climate policies': ['Climate policies have negative impact on the economy',\n",
       "  'Climate policies are only for profit',\n",
       "  'Other',\n",
       "  'Climate policies are ineffective'],\n",
       " 'CC: Questioning the measurements and science': ['Scientific community is unreliable',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Methodologies/metrics used are unreliable/faulty',\n",
       "  'Data shows no temperature increase',\n",
       "  'Other'],\n",
       " 'CC: Hidden plots by secret schemes of powerful groups': ['Climate agenda has hidden motives',\n",
       "  'Blaming global elites',\n",
       "  'Other'],\n",
       " 'CC: Climate change is beneficial': ['Temperature increase is beneficial',\n",
       "  'CO2 is beneficial',\n",
       "  'Other'],\n",
       " 'CC: Controversy about green technologies': ['Renewable energy is costly',\n",
       "  'Renewable energy is unreliable',\n",
       "  'Renewable energy is dangerous',\n",
       "  'Other'],\n",
       " 'CC: Green policies are geopolitical instruments': ['Green activities are a form of neo-colonialism',\n",
       "  'Climate-related international relations are abusive/exploitative',\n",
       "  'Other']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfa15c7-687f-4871-8a3d-b872cd632124",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77339ce-78a0-4344-92a0-8f36c514314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_train_stella.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "train_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ecc917-221b-416d-add9-38357edb6443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6c8f89-b82a-42d6-bf57-1af05734720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aee4215-a420-4261-a249-e2127f3ea099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc5b75f-faa3-44e1-afe9-7e4a54cc221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_dev_stella.npy')\n",
    "\n",
    "val_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4e333-9238-43e0-89a9-1397c55ac21a",
   "metadata": {},
   "source": [
    "We combine both train and val datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f4854f3-9ba0-426a-bee7-9bca8b73a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combined = pd.concat([dataset_train, dataset_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "777afd4c-b9ca-44a7-a84a-2c315e746ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f1fb0-7013-4184-a4a2-6ee21b9bc57c",
   "metadata": {},
   "source": [
    "We also do the same for the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be890e76-ea7d-4ad4-af58-16be91b08e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_combined = torch.cat([train_embeddings_tensor, val_embeddings_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3571206-3413-4868-b07a-6bdca8cc003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_shuffling(data, embeddings, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    shuffled_indices = np.arange(len(data))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "    embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    return data, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70196352-5aad-4d1a-b317-e2918032fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "991d6642-28c8-4cee-86ea-7b1e55386f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, embeddings = custom_shuffling(dataset_combined, embeddings_combined, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cbf2e-1533-4709-af94-bf46b4872901",
   "metadata": {},
   "source": [
    "### 1.2 Remapping our subnarrative indices\n",
    "\n",
    "We know that our articls have many narratives, and each one maps to several subnarratives, creating a hierarchy.  \n",
    "The problem is, our `subnarratives_encoded` currently looks like a flat list of zeros:\n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
    "```\n",
    "\n",
    "But we need it to reflect the hierarchy properly:\n",
    "\n",
    "So, we break it down into a list of lists—each inner list represents the true labels for a specific hierarchy:\n",
    "\n",
    "```\n",
    "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0] , [0, 0, 0, ...\n",
    " ^ hierarchy 0       ^ hierarchy 1          ^ hierarchy 2 ...\n",
    "```\n",
    "\n",
    "This will help us significantly later when we need to know for a specific article, the true subnarrative labels for a specific hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e95752a6-0be9-4f14-b11d-ff2fd08bab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40f7eaef-40aa-4237-a3a2-c72dede4d6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{13: [39, 66, 50, 22, 21, 65, 64, 33, 20], 14: [53, 71, 60, 56, 33, 58, 19], 19: [35, 46, 41, 34, 33, 42], 20: [40, 59, 63, 33], 15: [69, 33, 72], 11: [3, 62, 31, 33, 43], 12: [33, 67, 54], 18: [57, 55, 33, 32], 21: [45, 44, 33, 68], 16: [33], 10: [33], 17: [47, 61, 33], 0: [24, 23, 73, 33, 1], 5: [15, 14, 17, 33, 16], 3: [9, 8, 33, 0], 6: [27, 70, 51, 29, 7, 4, 28, 33, 49], 4: [12, 11, 33, 10], 9: [48, 26, 30, 18, 33], 8: [6, 2, 33], 1: [52, 5, 33], 2: [36, 38, 37, 33], 7: [25, 13, 33]}\n"
     ]
    }
   ],
   "source": [
    "narrative_to_sub_map = {}\n",
    "narrative_classes = list(mlb_narratives.classes_)\n",
    "subnarrative_classes = list(mlb_subnarratives.classes_)\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    narrative_idx = narrative_classes.index(narrative)\n",
    "    subnarrative_indices = [subnarrative_classes.index(sub) for sub in subnarratives]\n",
    "    narrative_to_sub_map[narrative_idx] = subnarrative_indices\n",
    "\n",
    "print(narrative_to_sub_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9775adf-1e84-4af9-8bbd-4d511d5f7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_new_column_name = \"narrative_hierarchy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "135f6d1a-3c3f-4f1f-8a8e-9ee18728ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_subnarratives(row, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row and encodes the current subnarrative list to the associated hierarchy based on the narr-subnar map\"\"\"\n",
    "    for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "        sub_labels = [row['subnarratives_encoded'][sub_idx] for sub_idx in sub_indices]\n",
    "        col_name = f\"{hierarchy_new_column_name}_{narr_idx}\"\n",
    "        row[col_name] = sub_labels\n",
    "    return row\n",
    "\n",
    "dataset_cpy = dataset.apply(remap_subnarratives, axis=1, args=(narrative_to_sub_map,)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b747cbf-314a-4ba5-84a6-d9e282b72760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of narrative_hierarchy_13:\n",
      "0    [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "1    [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "2    [0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "3    [0, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "4    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_13, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_14:\n",
      "0    [0, 0, 0, 0, 1, 0, 0]\n",
      "1    [0, 0, 0, 0, 1, 0, 0]\n",
      "2    [0, 0, 0, 0, 1, 0, 1]\n",
      "3    [0, 0, 0, 0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_14, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_19:\n",
      "0    [0, 0, 0, 0, 1, 0]\n",
      "1    [0, 0, 0, 0, 1, 0]\n",
      "2    [0, 0, 0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0, 0, 1]\n",
      "4    [0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_19, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_20:\n",
      "0    [0, 0, 0, 1]\n",
      "1    [0, 0, 0, 1]\n",
      "2    [0, 0, 0, 1]\n",
      "3    [1, 0, 0, 0]\n",
      "4    [0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_20, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_15:\n",
      "0    [0, 1, 0]\n",
      "1    [0, 1, 0]\n",
      "2    [0, 1, 0]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 0]\n",
      "Name: narrative_hierarchy_15, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_11:\n",
      "0    [0, 0, 0, 1, 0]\n",
      "1    [0, 0, 0, 1, 0]\n",
      "2    [0, 0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0, 1]\n",
      "4    [0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_11, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_12:\n",
      "0    [1, 0, 0]\n",
      "1    [1, 0, 0]\n",
      "2    [1, 0, 0]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 0]\n",
      "Name: narrative_hierarchy_12, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_18:\n",
      "0    [0, 0, 1, 0]\n",
      "1    [0, 0, 1, 0]\n",
      "2    [0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_18, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_21:\n",
      "0    [0, 0, 1, 0]\n",
      "1    [0, 0, 1, 0]\n",
      "2    [0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_21, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_16:\n",
      "0    [1]\n",
      "1    [1]\n",
      "2    [1]\n",
      "3    [0]\n",
      "4    [0]\n",
      "Name: narrative_hierarchy_16, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_10:\n",
      "0    [1]\n",
      "1    [1]\n",
      "2    [1]\n",
      "3    [0]\n",
      "4    [0]\n",
      "Name: narrative_hierarchy_10, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_17:\n",
      "0    [0, 0, 1]\n",
      "1    [0, 0, 1]\n",
      "2    [0, 0, 1]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 0]\n",
      "Name: narrative_hierarchy_17, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_0:\n",
      "0    [0, 0, 0, 1, 0]\n",
      "1    [0, 0, 0, 1, 1]\n",
      "2    [0, 0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_0, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_5:\n",
      "0    [0, 0, 0, 1, 0]\n",
      "1    [0, 0, 0, 1, 0]\n",
      "2    [0, 0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0, 1]\n",
      "Name: narrative_hierarchy_5, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_3:\n",
      "0    [0, 0, 1, 0]\n",
      "1    [0, 0, 1, 0]\n",
      "2    [0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_3, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_6:\n",
      "0    [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "1    [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "2    [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_6, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_4:\n",
      "0    [0, 0, 1, 0]\n",
      "1    [0, 0, 1, 0]\n",
      "2    [0, 0, 1, 0]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_4, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_9:\n",
      "0    [0, 0, 0, 0, 1]\n",
      "1    [0, 0, 0, 0, 1]\n",
      "2    [0, 0, 0, 0, 1]\n",
      "3    [0, 0, 0, 0, 0]\n",
      "4    [0, 0, 0, 0, 0]\n",
      "Name: narrative_hierarchy_9, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_8:\n",
      "0    [0, 0, 1]\n",
      "1    [0, 0, 1]\n",
      "2    [0, 0, 1]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 0]\n",
      "Name: narrative_hierarchy_8, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_1:\n",
      "0    [0, 0, 1]\n",
      "1    [0, 0, 1]\n",
      "2    [0, 0, 1]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 0]\n",
      "Name: narrative_hierarchy_1, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_2:\n",
      "0    [0, 0, 0, 1]\n",
      "1    [0, 0, 0, 1]\n",
      "2    [0, 0, 0, 1]\n",
      "3    [0, 0, 0, 0]\n",
      "4    [1, 0, 0, 0]\n",
      "Name: narrative_hierarchy_2, dtype: object\n",
      "\n",
      "\n",
      "Sample of narrative_hierarchy_7:\n",
      "0    [0, 0, 1]\n",
      "1    [0, 0, 1]\n",
      "2    [0, 0, 1]\n",
      "3    [0, 0, 0]\n",
      "4    [0, 0, 0]\n",
      "Name: narrative_hierarchy_7, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    dataset_hierarchy_col_name = f\"{hierarchy_new_column_name}_{narr_idx}\"\n",
    "    res = dataset_cpy[dataset_hierarchy_col_name]\n",
    "    print(f\"Sample of {dataset_hierarchy_col_name}:\")\n",
    "    print(res.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "819cbe29-f36f-4565-8a71-5dde06e1c6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order = sorted(narrative_to_sub_map.keys())\n",
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8a9ef8-abfc-4fb3-bf9e-16417ec73a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subnarratives(row, narrative_order, narrative_to_sub_map):\n",
    "    \"\"\"Takes in a row, and aggregates all hierarchy columns to 1 list.\n",
    "    The encoded list will be a list of lists, starting from the first hierarchy\"\"\"\n",
    "    aggregated = []\n",
    "    for narr_idx in narrative_order:\n",
    "        column_name = f\"narrative_hierarchy_{narr_idx}\"\n",
    "        sub_labels = row[column_name]\n",
    "        aggregated.append(sub_labels)\n",
    "    return aggregated\n",
    "\n",
    "dataset['aggregated_subnarratives'] = dataset_cpy.apply(\n",
    "    aggregate_subnarratives,\n",
    "    axis=1,\n",
    "    args=(narrative_order, narrative_to_sub_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cdf2c20-f46e-4007-98ea-1ea2602888ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HI</td>\n",
       "      <td>HI_376.txt</td>\n",
       "      <td>&lt;PARA&gt;भारत आ रहे रूसी राष्ट्रपति व्लादिमीर पुत...</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>[Other]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_140.txt</td>\n",
       "      <td>&lt;PARA&gt;mundo tem recorde de dia mais quente pel...</td>\n",
       "      <td>[CC: Amplifying Climate Fears, CC: Amplifying ...</td>\n",
       "      <td>[Amplifying existing fears of global warming, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 1], [0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_17.txt</td>\n",
       "      <td>&lt;PARA&gt;mundo passou do pós-guerra para o pré-gu...</td>\n",
       "      <td>[URW: Amplifying war-related fears, URW: Discr...</td>\n",
       "      <td>[Other, Diplomacy does/will not work, Ukraine ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT</td>\n",
       "      <td>PT_35.txt</td>\n",
       "      <td>&lt;PARA&gt;canadá pretende transferir mísseis aéreo...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Discrediting Ukrainian military, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN</td>\n",
       "      <td>EN_CC_200071.txt</td>\n",
       "      <td>&lt;PARA&gt;Biden’s green policies are making housin...</td>\n",
       "      <td>[CC: Criticism of institutions and authorities...</td>\n",
       "      <td>[Criticism of political organizations and figu...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language        article_id  \\\n",
       "0       HI        HI_376.txt   \n",
       "1       PT        PT_140.txt   \n",
       "2       PT         PT_17.txt   \n",
       "3       PT         PT_35.txt   \n",
       "4       EN  EN_CC_200071.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>भारत आ रहे रूसी राष्ट्रपति व्लादिमीर पुत...   \n",
       "1  <PARA>mundo tem recorde de dia mais quente pel...   \n",
       "2  <PARA>mundo passou do pós-guerra para o pré-gu...   \n",
       "3  <PARA>canadá pretende transferir mísseis aéreo...   \n",
       "4  <PARA>Biden’s green policies are making housin...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0                                            [Other]   \n",
       "1  [CC: Amplifying Climate Fears, CC: Amplifying ...   \n",
       "2  [URW: Amplifying war-related fears, URW: Discr...   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4  [CC: Criticism of institutions and authorities...   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0                                            [Other]   \n",
       "1  [Amplifying existing fears of global warming, ...   \n",
       "2  [Other, Diplomacy does/will not work, Ukraine ...   \n",
       "3  [Discrediting Ukrainian military, Discrediting...   \n",
       "4  [Criticism of political organizations and figu...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "4  [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 0, 1, 0], [0, 0, 1], [0, 0, 0, 1], [0,...  \n",
       "1  [[0, 0, 0, 1, 1], [0, 0, 1], [0, 0, 0, 1], [0,...  \n",
       "2  [[0, 0, 0, 1, 0], [0, 0, 1], [0, 0, 0, 1], [0,...  \n",
       "3  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [1, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63265bcf-6a01-4dbc-98d4-3ad555b4a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub_heads = dataset['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e40ce54-7b7f-438c-b69d-ad340c0e44f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "input_size = embeddings.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ded6266-8004-45bf-8110-102378630944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=1024,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db5cc04c-8223-4956-a196-be29352c87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 1024,\n",
    "    'dropout': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca024dba-3271-42d3-8699-4ab35a3f2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nar = dataset['narratives_encoded'].tolist()\n",
    "\n",
    "y_sub_nar = dataset['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f40bfd3f-4350-47e4-b327-8eab082eb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nar = torch.tensor(y_nar, dtype=torch.float32)\n",
    "y_sub_nar = torch.tensor(y_sub_nar, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82182150-3702-4d68-a319-d9be3e8359e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_classes = sorted(narrative_to_subnarratives.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4284e389-b29a-4aad-a339-5ac2b263d83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears',\n",
       " 'CC: Climate change is beneficial',\n",
       " 'CC: Controversy about green technologies',\n",
       " 'CC: Criticism of climate movement',\n",
       " 'CC: Criticism of climate policies']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "630377d4-c001-41aa-bed0-9352cb18dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89df4af6-492e-4446-8a4c-5884e0ce7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_label_set = set()\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    if narrative == \"Other\":\n",
    "        fine_label_set.add(\"Other\")\n",
    "    else:\n",
    "        for sub in subnarratives:\n",
    "            if sub == \"Other\":\n",
    "                fine_label_set.add(f\"{narrative}: Other\")\n",
    "            else:\n",
    "                fine_label_set.add(f\"{narrative}: {sub}\")\n",
    "\n",
    "fine_classes = sorted(fine_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7eeb49a-a41e-450f-96f4-ace246e788d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears: Amplifying existing fears of global warming',\n",
       " 'CC: Amplifying Climate Fears: Doomsday scenarios for humans',\n",
       " 'CC: Amplifying Climate Fears: Earth will be uninhabitable soon',\n",
       " 'CC: Amplifying Climate Fears: Other',\n",
       " 'CC: Amplifying Climate Fears: Whatever we do it is already too late']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67076b-b2ab-4711-899c-81db6b57fa78",
   "metadata": {},
   "source": [
    "We define the same evaluator we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1e6d91c-39d0-406c-8aad-19ad26ab709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "class MultiHeadEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes_coarse=coarse_classes,\n",
    "        classes_fine=fine_classes,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu',\n",
    "        output_dir='../../../submissions',\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.narrative_classes = list(narrative_classes)\n",
    "        self.subnarrative_classes = list(subnarrative_classes)\n",
    "        \n",
    "        self.classes_coarse = classes_coarse\n",
    "        self.classes_fine = classes_fine\n",
    "\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings,\n",
    "        dataset,\n",
    "        thresholds=None,\n",
    "        save=False,\n",
    "        std_weight=0.6,\n",
    "        lower_thres=0.1,\n",
    "        upper_thres=0.55\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(lower_thres, upper_thres, 0.05)    \n",
    "        embeddings = embeddings.to(self.device)\n",
    "    \n",
    "        best_results = {\n",
    "            'best_coarse_f1': -1,\n",
    "            'best_coarse_std': float('inf'),\n",
    "            'best_fine_f1': -1,\n",
    "            'best_fine_std': float('inf'),\n",
    "            'narr_threshold': 0,\n",
    "            'sub_threshold': 0,\n",
    "            'predictions': None,\n",
    "            'best_combined_score': -float('inf'),\n",
    "            'coarse_classification_report': None,\n",
    "            'fine_precision': None,\n",
    "            'fine_recall': None,\n",
    "            'samples_f1_fine': None,\n",
    "        }\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "    \n",
    "        for narr_threshold in thresholds:\n",
    "            for sub_threshold in thresholds:\n",
    "                predictions = []\n",
    "                for sample_idx, row in dataset.iterrows():\n",
    "                    pred = self._make_prediction(\n",
    "                        row['article_id'],\n",
    "                        sample_idx,\n",
    "                        narr_probs,\n",
    "                        sub_probs_dict,\n",
    "                        narr_threshold,\n",
    "                        sub_threshold\n",
    "                    )\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, samples_f1_fine = self._compute_metrics_coarse_fine(predictions, dataset)\n",
    "                \n",
    "                combined_score = f1_fine_mean - (std_weight * coarse_std)\n",
    "                \n",
    "                if combined_score > best_results['best_combined_score']:\n",
    "                    best_results.update({\n",
    "                        'best_coarse_f1': f1_coarse_mean,\n",
    "                        'best_coarse_std': coarse_std,\n",
    "                        'best_fine_f1': f1_fine_mean,\n",
    "                        'best_fine_std': fine_std,\n",
    "                        'narr_threshold': narr_threshold,\n",
    "                        'sub_threshold': sub_threshold,\n",
    "                        'predictions': predictions,\n",
    "                        'best_combined_score': combined_score,\n",
    "                        'coarse_classification_report': report_coarse,\n",
    "                        'fine_precision': precision_fine,\n",
    "                        'fine_recall': recall_fine,\n",
    "                        'samples_f1_fine': samples_f1_fine,\n",
    "                    })\n",
    "    \n",
    "        print(\"\\nBest thresholds found:\")\n",
    "        print(f\"Narrative threshold: {best_results['narr_threshold']:.2f}\")\n",
    "        print(f\"Subnarrative threshold: {best_results['sub_threshold']:.2f}\")\n",
    "        print('\\nCompetition Values')\n",
    "        print(f\"Coarse-F1: {best_results['best_coarse_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. coarse: {best_results['best_coarse_std']:.3f}\")\n",
    "        print(f\"Fine-F1: {best_results['best_fine_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. fine: {best_results['best_fine_std']:.3f}\")\n",
    "\n",
    "        if save:\n",
    "            self._save_predictions(best_results, os.path.join(self.output_dir, 'submission.txt'))\n",
    "        \n",
    "        return best_results\n",
    "\n",
    "    def _make_prediction(self, article_id, sample_idx, narr_probs, sub_probs_dict, narr_threshold, sub_threshold):\n",
    "        other_idx = self.narrative_classes.index(\"Other\")\n",
    "        active_narratives = [\n",
    "            (n_idx, prob)\n",
    "            for n_idx, prob in enumerate(narr_probs[sample_idx])\n",
    "            if n_idx != other_idx and prob >= narr_threshold\n",
    "        ]\n",
    "        \n",
    "        if not active_narratives:\n",
    "            return {\n",
    "                'article_id': article_id,\n",
    "                'narratives': [\"Other\"],\n",
    "                'pairs': [\"Other\"]\n",
    "            }\n",
    "        \n",
    "        narratives = []\n",
    "        pairs = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        active_narratives.sort(key=lambda x: x[1], reverse=True)\n",
    "        for narr_idx, _ in active_narratives:\n",
    "            narr_name = self.narrative_classes[narr_idx]\n",
    "            \n",
    "            sub_probs = sub_probs_dict[str(narr_idx)][sample_idx]\n",
    "            active_subnarratives = [\n",
    "                (local_idx, s_prob)\n",
    "                for local_idx, s_prob in enumerate(sub_probs)\n",
    "                if s_prob >= sub_threshold\n",
    "            ]\n",
    "            active_subnarratives.sort(key=lambda x: x[1], reverse=True)\n",
    "            if not active_subnarratives:\n",
    "                pairs.append(f\"{narr_name}: Other\")\n",
    "            else:\n",
    "                for local_idx, _ in active_subnarratives:\n",
    "                    global_sub_idx = self.narrative_to_sub_map[narr_idx][local_idx]\n",
    "                    sub_name = self.subnarrative_classes[global_sub_idx]\n",
    "                    pair = f\"{narr_name}: {sub_name}\"\n",
    "                    if pair not in seen_pairs:\n",
    "                        pairs.append(pair)\n",
    "                        seen_pairs.add(pair)\n",
    "            narratives.append(narr_name)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'narratives': narratives,\n",
    "            'pairs': pairs\n",
    "        }\n",
    "\n",
    "    def _compute_metrics_coarse_fine(self, predictions, dataset):\n",
    "        gold_coarse_all = []\n",
    "        gold_fine_all = []\n",
    "        pred_coarse_all = []\n",
    "        pred_fine_all = []\n",
    "\n",
    "        for pred, (_, row) in zip(predictions, dataset.iterrows()):\n",
    "            gold_coarse = row['narratives']\n",
    "            gold_subnarratives = row['subnarratives']\n",
    "            \n",
    "            pred_coarse = pred['narratives']\n",
    "            pred_fine = []\n",
    "            for p in pred['pairs']:\n",
    "                if p == \"Other\":\n",
    "                    pred_fine.append(\"Other\")\n",
    "                else:\n",
    "                    pred_fine.append(p)\n",
    "\n",
    "            gold_fine = []\n",
    "            for gold_nar, gold_sub in zip(gold_coarse, gold_subnarratives):\n",
    "                if gold_nar == \"Other\":\n",
    "                    gold_fine.append(\"Other\")\n",
    "                else:\n",
    "                    gold_fine.append(f\"{gold_nar}: {gold_sub}\")\n",
    "            \n",
    "            gold_coarse_all.append(gold_coarse)\n",
    "            gold_fine_all.append(gold_fine)\n",
    "            pred_coarse_all.append(pred_coarse)\n",
    "            pred_fine_all.append(pred_fine)\n",
    "\n",
    "        f1_coarse_mean, coarse_std = self._evaluate_multi_label(gold_coarse_all, pred_coarse_all, self.classes_coarse)\n",
    "        f1_fine_mean, fine_std = self._evaluate_multi_label(gold_fine_all, pred_fine_all, self.classes_fine)\n",
    "        \n",
    "        gold_coarse_flat = []\n",
    "        pred_coarse_flat = []\n",
    "        for g_labels, p_labels in zip(gold_coarse_all, pred_coarse_all):\n",
    "            g_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    g_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            p_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    p_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            gold_coarse_flat.append(g_onehot)\n",
    "            pred_coarse_flat.append(p_onehot)\n",
    "        gold_coarse_flat = np.array(gold_coarse_flat)\n",
    "        pred_coarse_flat = np.array(pred_coarse_flat)\n",
    "        report_coarse = metrics.classification_report(\n",
    "            gold_coarse_flat, pred_coarse_flat, target_names=self.classes_coarse, zero_division=0\n",
    "        )\n",
    "        \n",
    "        gold_fine_flat = []\n",
    "        pred_fine_flat = []\n",
    "        for g_labels, p_labels in zip(gold_fine_all, pred_fine_all):\n",
    "            g_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    g_onehot[self.classes_fine.index(lab)] = 1\n",
    "            p_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    p_onehot[self.classes_fine.index(lab)] = 1\n",
    "            gold_fine_flat.append(g_onehot)\n",
    "            pred_fine_flat.append(p_onehot)\n",
    "        gold_fine_flat = np.array(gold_fine_flat)\n",
    "        pred_fine_flat = np.array(pred_fine_flat)\n",
    "        \n",
    "        precision_fine = metrics.precision_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        recall_fine = metrics.recall_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        samples_f1_fine = metrics.f1_score(\n",
    "            gold_fine_flat, \n",
    "            pred_fine_flat, \n",
    "            average='samples',\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        return f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, samples_f1_fine\n",
    "\n",
    "    def _evaluate_multi_label(self, gold, predicted, class_list):\n",
    "        f1_scores = []\n",
    "        for g_labels, p_labels in zip(gold, predicted):\n",
    "            g_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in class_list:\n",
    "                    g_onehot[class_list.index(lab)] = 1\n",
    "                    \n",
    "            p_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in class_list:\n",
    "                    p_onehot[class_list.index(lab)] = 1\n",
    "\n",
    "            f1_doc = metrics.f1_score(g_onehot, p_onehot, zero_division=0)\n",
    "            f1_scores.append(f1_doc)\n",
    "        \n",
    "        return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "    def _save_predictions(self, best_results, filepath):\n",
    "        predictions = best_results['predictions']\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                line = (f\"{pred['article_id']}\\t\"\n",
    "                        f\"{';'.join(pred['narratives'])}\\t\"\n",
    "                        f\"{';'.join(pred['pairs'])}\\n\")\n",
    "                f.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17623237-9ec8-41b0-9244-a46a8b9ed930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e052178-2256-4347-bc64-3ea1b3e5db97",
   "metadata": {},
   "source": [
    "We also define the same loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54e384f0-c67c-4f6b-9748-5761c280537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=0.8, sub_weight=0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        \n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "        \n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            \n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_loss += sub_loss_func(sub_probs, y_sub_tensor)\n",
    "\n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.mean(\n",
    "                # Penalize high probs of sub, based on first level narr predictinos\n",
    "                torch.abs(sub_probs * (1 - narr_pred)) + \n",
    "                # If a narrative is true, then the subnarrative predictions should match their actual true values.\n",
    "                narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            )\n",
    "            condition_loss += condition_term\n",
    "            \n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        \n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + \\\n",
    "                    self.sub_weight * sub_loss + \\\n",
    "                    self.condition_weight * condition_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4301557f-da13-4af7-9212-c50e2d6bb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    train_embeddings,\n",
    "    y_train_nar,\n",
    "    y_train_sub_heads,\n",
    "    val_embeddings,\n",
    "    y_val_nar,\n",
    "    y_val_sub_heads,\n",
    "    patience=5,\n",
    "    num_epochs=100,\n",
    "    scheduler=None,\n",
    "    min_delta=0.001,\n",
    "    show_progress=True\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "        train_loss = loss_fn(train_narr_probs, train_sub_probs_dict, y_train_nar, y_train_sub_heads)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            val_loss = loss_fn(val_narr_probs, val_sub_probs_dict, y_val_nar, y_val_sub_heads)\n",
    "            \n",
    "        if show_progress:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                  f\"Training Loss: {train_loss.item():.4f}, \"\n",
    "                  f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if val_loss.item() < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not significantly improve for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec801bb2-e5d7-4554-b40a-62b2dfa0e608",
   "metadata": {},
   "source": [
    "We first create a splitter, shuffle data and iterate each fold.\n",
    "- Each fold gives us a train and val idx in order for us to access the train and val in that fold.\n",
    "- We load our model, optimizer and class weights and then we train like usual.\n",
    "- After that, we validate our model on the validation embeddings for that fold.\n",
    "- Lastly, we save those models, their configs, and thresholds for later use during inference.\n",
    "- We finally averages the performance across folds so that we can get an estimate of how the model does on different splits of our complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85606afd-7e17-477c-8ba8-f2b76f5276b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "class CrossValEnsembleTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class,\n",
    "        embeddings=embeddings,\n",
    "        dataset=dataset,\n",
    "        y_nar=y_nar,\n",
    "        y_sub_heads=y_sub_heads,\n",
    "        input_size=input_size,\n",
    "        hidden_size=1024,\n",
    "        dropout_rate=0.4,\n",
    "        lr=0.001,\n",
    "        n_splits=5,\n",
    "        patience=5,\n",
    "        num_epochs=100,\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.embeddings = embeddings\n",
    "        self.dataset = dataset\n",
    "        self.y_nar = y_nar\n",
    "        self.y_sub_heads = y_sub_heads\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lr = lr\n",
    "        self.n_splits = n_splits\n",
    "        self.patience = patience\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.fold_models = []\n",
    "        self.fold_results = []\n",
    "\n",
    "    def fit(self):\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.embeddings)):\n",
    "            print(f\"\\nTraining Fold {fold + 1}/{self.n_splits}\")\n",
    "            \n",
    "            # Split data based on indices\n",
    "            train_embeddings = self.embeddings[train_idx]\n",
    "            val_embeddings_fold = self.embeddings[val_idx]\n",
    "            \n",
    "            train_nar = self.y_nar[train_idx]\n",
    "            val_nar = self.y_nar[val_idx]\n",
    "            \n",
    "            train_sub = self.y_sub_heads[train_idx]\n",
    "            val_sub = self.y_sub_heads[val_idx]\n",
    "            \n",
    "            model = self.model_class(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                dropout_rate=self.dropout_rate\n",
    "            )\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "\n",
    "            # Compute class weights for narratives\n",
    "            class_weights_nar_fold = compute_class_weights(val_nar)\n",
    "            narrative_criterion_fold = WeightedBCELoss(class_weights_nar_fold)\n",
    "            \n",
    "            # Compute subnarrative class weights\n",
    "            sub_criterion_dict_fold = {}\n",
    "            for narr_idx_str in model.subnarrative_heads.keys():\n",
    "                narr_idx = int(narr_idx_str)\n",
    "                y_sub_fold = torch.tensor([row[narr_idx] for row in val_sub])\n",
    "                class_weights_sub = compute_class_weights(y_sub_fold)\n",
    "                sub_criterion_dict_fold[narr_idx_str] = WeightedBCELoss(class_weights_sub)\n",
    "\n",
    "            loss_fn = MultiHeadLoss(\n",
    "                narrative_criterion=narrative_criterion_fold,\n",
    "                sub_criterion_dict=sub_criterion_dict_fold\n",
    "            )\n",
    "            \n",
    "            trained_model = train_with_multihead(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                train_embeddings=train_embeddings,\n",
    "                y_train_nar=train_nar,\n",
    "                y_train_sub_heads=train_sub,\n",
    "                val_embeddings=val_embeddings_fold,\n",
    "                y_val_nar=val_nar,\n",
    "                y_val_sub_heads=val_sub,\n",
    "                patience=self.patience,\n",
    "                num_epochs=self.num_epochs,\n",
    "                show_progress=False\n",
    "            )\n",
    "            \n",
    "            evaluator = MultiHeadEvaluator()\n",
    "            fold_dataset = self.dataset.iloc[val_idx].reset_index(drop=True)\n",
    "            metrics = evaluator.evaluate(\n",
    "                trained_model, \n",
    "                val_embeddings_fold,\n",
    "                dataset=fold_dataset\n",
    "            )\n",
    "            \n",
    "            self.fold_models.append(trained_model.state_dict())\n",
    "            self.fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'metrics': metrics,\n",
    "                'val_indices': val_idx\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nFold {fold + 1} Results:\")\n",
    "            print(f\"Coarse-F1: {metrics['best_coarse_f1']:.3f}\")\n",
    "            print(f\"Fine-F1: {metrics['best_fine_f1']:.3f}\")\n",
    "            \n",
    "        self._display_cv_performance()\n",
    "        return self.fold_models, self.fold_results\n",
    "\n",
    "\n",
    "    def _display_cv_performance(self):\n",
    "        print(\"\\nCross-Validation Performance:\")\n",
    "        avg_coarse_f1 = np.mean([r['metrics']['best_coarse_f1'] for r in self.fold_results])\n",
    "        std_coarse_f1 = np.std([r['metrics']['best_coarse_f1'] for r in self.fold_results])\n",
    "        avg_fine_f1 = np.mean([r['metrics']['best_fine_f1'] for r in self.fold_results])\n",
    "        std_fine_f1 = np.std([r['metrics']['best_fine_f1'] for r in self.fold_results])\n",
    "        \n",
    "        print(f\"Average Coarse-F1: {avg_coarse_f1:.3f} ± {std_coarse_f1:.3f}\")\n",
    "        print(f\"Average Fine-F1:   {avg_fine_f1:.3f} ± {std_fine_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef28f2d0-f7e5-437b-af6e-1e36aeaac5fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.587\n",
      "F1 st. dev. coarse: 0.373\n",
      "Fine-F1: 0.394\n",
      "F1 st. dev. fine: 0.338\n",
      "\n",
      "Fold 1 Results:\n",
      "Coarse-F1: 0.587\n",
      "Fine-F1: 0.394\n",
      "\n",
      "Training Fold 2/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.45\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.572\n",
      "F1 st. dev. coarse: 0.346\n",
      "Fine-F1: 0.399\n",
      "F1 st. dev. fine: 0.310\n",
      "\n",
      "Fold 2 Results:\n",
      "Coarse-F1: 0.572\n",
      "Fine-F1: 0.399\n",
      "\n",
      "Training Fold 3/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.581\n",
      "F1 st. dev. coarse: 0.365\n",
      "Fine-F1: 0.407\n",
      "F1 st. dev. fine: 0.339\n",
      "\n",
      "Fold 3 Results:\n",
      "Coarse-F1: 0.581\n",
      "Fine-F1: 0.407\n",
      "\n",
      "Training Fold 4/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.580\n",
      "F1 st. dev. coarse: 0.361\n",
      "Fine-F1: 0.404\n",
      "F1 st. dev. fine: 0.336\n",
      "\n",
      "Fold 4 Results:\n",
      "Coarse-F1: 0.580\n",
      "Fine-F1: 0.404\n",
      "\n",
      "Training Fold 5/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.591\n",
      "F1 st. dev. coarse: 0.357\n",
      "Fine-F1: 0.402\n",
      "F1 st. dev. fine: 0.330\n",
      "\n",
      "Fold 5 Results:\n",
      "Coarse-F1: 0.591\n",
      "Fine-F1: 0.402\n",
      "\n",
      "Cross-Validation Performance:\n",
      "Average Coarse-F1: 0.582 ± 0.007\n",
      "Average Fine-F1:   0.401 ± 0.005\n"
     ]
    }
   ],
   "source": [
    "simple_trainer = CrossValEnsembleTrainer(\n",
    "    model_class=MultiTaskClassifierMultiHead,\n",
    "    hidden_size=1024,\n",
    "    dropout_rate=0.4,\n",
    "    lr=0.001,\n",
    "    n_splits=5,\n",
    "    patience=5,\n",
    "    num_epochs=100,\n",
    ")\n",
    "ensemble_models, fold_results = simple_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d752254e-edf2-41b5-9e72-b8837e844a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassifierMultiHeadConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout']\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2 + 1, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            conditioned_input = torch.cat((shared_out, narr_probs[:, int(narr_idx)].unsqueeze(1)), dim=1)\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ebf3f9d-de55-4332-a9d2-33816fe2d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.570\n",
      "F1 st. dev. coarse: 0.360\n",
      "Fine-F1: 0.409\n",
      "F1 st. dev. fine: 0.337\n",
      "\n",
      "Fold 1 Results:\n",
      "Coarse-F1: 0.570\n",
      "Fine-F1: 0.409\n",
      "\n",
      "Training Fold 2/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.576\n",
      "F1 st. dev. coarse: 0.364\n",
      "Fine-F1: 0.404\n",
      "F1 st. dev. fine: 0.328\n",
      "\n",
      "Fold 2 Results:\n",
      "Coarse-F1: 0.576\n",
      "Fine-F1: 0.404\n",
      "\n",
      "Training Fold 3/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.585\n",
      "F1 st. dev. coarse: 0.368\n",
      "Fine-F1: 0.412\n",
      "F1 st. dev. fine: 0.345\n",
      "\n",
      "Fold 3 Results:\n",
      "Coarse-F1: 0.585\n",
      "Fine-F1: 0.412\n",
      "\n",
      "Training Fold 4/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.578\n",
      "F1 st. dev. coarse: 0.362\n",
      "Fine-F1: 0.403\n",
      "F1 st. dev. fine: 0.327\n",
      "\n",
      "Fold 4 Results:\n",
      "Coarse-F1: 0.578\n",
      "Fine-F1: 0.403\n",
      "\n",
      "Training Fold 5/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.585\n",
      "F1 st. dev. coarse: 0.358\n",
      "Fine-F1: 0.389\n",
      "F1 st. dev. fine: 0.314\n",
      "\n",
      "Fold 5 Results:\n",
      "Coarse-F1: 0.585\n",
      "Fine-F1: 0.389\n",
      "\n",
      "Cross-Validation Performance:\n",
      "Average Coarse-F1: 0.579 ± 0.006\n",
      "Average Fine-F1:   0.403 ± 0.008\n"
     ]
    }
   ],
   "source": [
    "trainer_concat = CrossValEnsembleTrainer(\n",
    "    model_class=MultiTaskClassifierMultiHeadConcat,\n",
    "    hidden_size=2048,\n",
    "    dropout_rate=0.4,\n",
    "    lr=0.001,\n",
    "    n_splits=5,\n",
    "    patience=5,\n",
    "    num_epochs=100,\n",
    ")\n",
    "ensemble_models_concat, fold_results_concat = trainer_concat.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac128f2d-29bd-4727-b985-3a4f37e23078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskClassifierMultiHeadMult(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout'],\n",
    "        bias=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            narr_pred = narr_probs[:, int(narr_idx)].unsqueeze(1)\n",
    "\n",
    "            conditioned_input = shared_out * (narr_pred + self.bias)\n",
    "\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "311131c1-116d-4f77-b87c-32e198d0a368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.35\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.536\n",
      "F1 st. dev. coarse: 0.327\n",
      "Fine-F1: 0.341\n",
      "F1 st. dev. fine: 0.300\n",
      "\n",
      "Fold 1 Results:\n",
      "Coarse-F1: 0.536\n",
      "Fine-F1: 0.341\n",
      "\n",
      "Training Fold 2/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.565\n",
      "F1 st. dev. coarse: 0.338\n",
      "Fine-F1: 0.363\n",
      "F1 st. dev. fine: 0.311\n",
      "\n",
      "Fold 2 Results:\n",
      "Coarse-F1: 0.565\n",
      "Fine-F1: 0.363\n",
      "\n",
      "Training Fold 3/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.35\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.493\n",
      "F1 st. dev. coarse: 0.333\n",
      "Fine-F1: 0.327\n",
      "F1 st. dev. fine: 0.291\n",
      "\n",
      "Fold 3 Results:\n",
      "Coarse-F1: 0.493\n",
      "Fine-F1: 0.327\n",
      "\n",
      "Training Fold 4/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.30\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.519\n",
      "F1 st. dev. coarse: 0.323\n",
      "Fine-F1: 0.340\n",
      "F1 st. dev. fine: 0.276\n",
      "\n",
      "Fold 4 Results:\n",
      "Coarse-F1: 0.519\n",
      "Fine-F1: 0.340\n",
      "\n",
      "Training Fold 5/5\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.40\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.555\n",
      "F1 st. dev. coarse: 0.337\n",
      "Fine-F1: 0.375\n",
      "F1 st. dev. fine: 0.321\n",
      "\n",
      "Fold 5 Results:\n",
      "Coarse-F1: 0.555\n",
      "Fine-F1: 0.375\n",
      "\n",
      "Cross-Validation Performance:\n",
      "Average Coarse-F1: 0.534 ± 0.026\n",
      "Average Fine-F1:   0.349 ± 0.017\n"
     ]
    }
   ],
   "source": [
    "trainer_mult = CrossValEnsembleTrainer(\n",
    "    model_class=MultiTaskClassifierMultiHeadMult,\n",
    "    hidden_size=2048,\n",
    "    dropout_rate=0.4,\n",
    "    lr=0.001,\n",
    "    n_splits=5,\n",
    "    patience=5,\n",
    "    num_epochs=100,\n",
    ")\n",
    "ensemble_models_mult, fold_results_mult = trainer_mult.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
