{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d99e5d6-7e11-492f-a64d-b37cf0c45205",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c39088-d95e-477c-89d0-1fdeb22ee015",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3326e463-8379-49cd-b744-64ac197b172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "if random_state:\n",
    "    print('[WARNING] Setting random state')\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state) \n",
    "    random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786310c-1d16-4caa-aa50-25de4385bf9c",
   "metadata": {},
   "source": [
    "## Continual Learning\n",
    "\n",
    "As of current, we were using all multilingual training data (Russian, Bulgarian, Portuguese, Hindi, and English) at once, mixing it together during training, and then evaluating specifically on English validation data. However, since our final evaluation is language-target based we can leverage a sequential training of langauges (Russian -> Bulgarian -> Portuguese -> Hindi -> English) in order to aim for better results.\n",
    "\n",
    "This is also similar to how we as humans might learn languages. We start with one then move to another one while maintaining knowledge of the past ones.\n",
    "\n",
    "This way might help our learning on identifying different useful patterns per language that could later help a specific language classification. For example:\n",
    "* Russian articles might help learn certain propaganda patterns.\n",
    "* Bulgarian articles might contribute different narrative structures.\n",
    "* Each language adds its own unique perspective to the model's understanding, the model get's this knoweledge sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59caa7a8-d3fd-49e6-9577-ab31af120073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"../../\"\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3471b8e-6a33-4304-842b-0f33d486a2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "1  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd314e8-e067-4f8e-8e9a-55c7bd31ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16de16f1-f5a2-4d0b-9163-a0309370c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives_map.pkl'), 'rb') as f:\n",
    "    narrative_to_sub_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a297e8c-ed56-4a0d-a4b7-357b1a874a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'coarse_classes.pkl'), 'rb') as f:\n",
    "    coarse_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'fine_classes.pkl'), 'rb') as f:\n",
    "    fine_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_order.pkl'), 'rb') as f:\n",
    "    narrative_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41a87fa-30be-4c8f-9361-bbf4051b5a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6c8d29-b38d-4cbf-b786-6ceb77727fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22dda616-22e7-4809-9745-68dea421e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_train_stella.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "train_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e19b5a1-1aa9-4fa5-8122-052f158aae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c624ed2-a874-4168-843a-2aa49d87858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65251811-2cf0-4875-9293-c6746eefe0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6760b44-5c00-444d-945a-17deb00626d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1014.txt</td>\n",
       "      <td>&lt;PARA&gt;алаудинов: российские силы растянули и р...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Praise of Russian military might]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1174.txt</td>\n",
       "      <td>&lt;PARA&gt;других сценариев нет. никаких переговоро...</td>\n",
       "      <td>[URW: Speculating war outcomes, URW: Discredit...</td>\n",
       "      <td>[Ukrainian army is collapsing, Discrediting Uk...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1166.txt</td>\n",
       "      <td>&lt;PARA&gt;попытка запада изолировать путина провал...</td>\n",
       "      <td>[URW: Praise of Russia, URW: Distrust towards ...</td>\n",
       "      <td>[Praise of Russian President Vladimir Putin, W...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1170.txt</td>\n",
       "      <td>&lt;PARA&gt;часть территории украины войдет в состав...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Speculating w...</td>\n",
       "      <td>[Discrediting Ukrainian government and officia...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1004.txt</td>\n",
       "      <td>&lt;PARA&gt;зеленскому не очень понравилась идея о в...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Discrediting Ukrainian government and officia...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1014.txt   \n",
       "1       RU  RU-URW-1174.txt   \n",
       "2       RU  RU-URW-1166.txt   \n",
       "3       RU  RU-URW-1170.txt   \n",
       "4       RU  RU-URW-1004.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>алаудинов: российские силы растянули и р...   \n",
       "1  <PARA>других сценариев нет. никаких переговоро...   \n",
       "2  <PARA>попытка запада изолировать путина провал...   \n",
       "3  <PARA>часть территории украины войдет в состав...   \n",
       "4  <PARA>зеленскому не очень понравилась идея о в...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0                            [URW: Praise of Russia]   \n",
       "1  [URW: Speculating war outcomes, URW: Discredit...   \n",
       "2  [URW: Praise of Russia, URW: Distrust towards ...   \n",
       "3  [URW: Discrediting Ukraine, URW: Speculating w...   \n",
       "4  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0                 [Praise of Russian military might]   \n",
       "1  [Ukrainian army is collapsing, Discrediting Uk...   \n",
       "2  [Praise of Russian President Vladimir Putin, W...   \n",
       "3  [Discrediting Ukrainian government and officia...   \n",
       "4  [Discrediting Ukrainian government and officia...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "1  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a538dc67-c1e6-4e44-8d0e-18dec04c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_dev_stella.npy')\n",
    "\n",
    "val_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f850e1-f926-4ab6-9f7a-0f67b9389e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_and_embeddings(dataset, embeddings, condition_fn):\n",
    "    filtered_indices = dataset.index[dataset.apply(condition_fn, axis=1)].tolist()\n",
    "    \n",
    "    filtered_dataset = dataset.loc[filtered_indices]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "\n",
    "    return filtered_dataset, filtered_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034f315c-406e-4a6b-a7bc-2aa75f5c3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_en, val_embeddings_en = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == \"EN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc926b5-3f23-43df-93c5-89520c9e7aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5033bbd-0be9-4e77-a6ec-b197dee8ce06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e9db19b-9cad-4baf-9112-7a48d6b38783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1898d2-d812-40cd-b591-684f068e1044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd2c347-8260-418c-b970-299f3a2c8b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...\n",
       "1       [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...\n",
       "2       [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "3       [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "4       [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "                              ...                        \n",
       "1776    [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "1777    [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...\n",
       "1778    [[0, 0, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0,...\n",
       "1779    [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "1780    [[0, 0, 1, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "Name: aggregated_subnarratives, Length: 1781, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['aggregated_subnarratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cb9c1c0-510c-45a6-b7b9-1fd356cef6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prefer_cpu=True\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() and not prefer_cpu\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47887e9a-e6db-4ce6-9de4-41980f14d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04b31463-d7d2-462f-98b7-45019fbdb3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RU', 'PT', 'BG', 'HI', 'EN'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73e14dc2-947d-4eed-822e-e121b5f488f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe4df1aa-83e0-497e-86ff-f2879b508a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60fbb460-43de-460f-a379-5533ba50ac4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RU', 'PT', 'BG', 'HI', 'EN'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aaee05b-af6b-4dc2-9b3b-634659bc7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 2048,\n",
    "    'dropout': 0.4,\n",
    "    'patience': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3a89ba3-d486-42c6-99fb-0dc380c0b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=1024,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.4,\n",
    "        model_name=\"MultiTaskClassifierMultiHead\" \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "        \n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "            \n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92db08b7-dd96-4eaa-902b-1fa9e3085acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassifierMultiHeadConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout'],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size        \n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            # Here each head expects an additional 1-dimension input (the narrative probability for that head)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2 + 1, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            # Add a new dimension: get the probability for the narrative corresponding to narr_idx\n",
    "            # Then concatenate it with shared layer's output.\n",
    "            conditioned_input = torch.cat((shared_out, narr_probs[:, int(narr_idx)].unsqueeze(1)), dim=1)\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bc39239-d7a5-418c-bb41-6314338f27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02f836fd-f58a-4e2f-b73c-f1cc1a3a29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32).to(device)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "138b7fdc-b2f9-4b66-bee9-2f072a23cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb6865a1-4cd3-4a00-939f-7c0785b9f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97c597ac-d39f-4e74-9a1f-9b3652797d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head = MultiTaskClassifierMultiHead(\n",
    "    input_size=input_size,\n",
    "    hidden_size=network_params['hidden_size'],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02aee58e-ac1c-4b67-87fa-20ffca4bd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e03513c5-f138-419a-886e-0c5e7b49feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_params = {\n",
    "    'sub_weight': 0.3,\n",
    "    'condition_weight': 0.3,\n",
    "    'target_weight': 2.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "131b5778-0dd0-43d1-a10d-c1b8b9523ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=loss_params['condition_weight'],\n",
    "                 sub_weight=loss_params['sub_weight'],\n",
    "                 target_weight=loss_params['target_weight'],\n",
    "                 is_target=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        self.target_weight = target_weight\n",
    "        self.is_target = is_target\n",
    "        \n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "        \n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            \n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_loss += sub_loss_func(sub_probs, y_sub_tensor)\n",
    "            \n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.mean(\n",
    "                torch.abs(sub_probs * (1 - narr_pred)) + \n",
    "                narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            )\n",
    "            condition_loss += condition_term\n",
    "            \n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        \n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + \\\n",
    "                    self.sub_weight * sub_loss + \\\n",
    "                    self.condition_weight * condition_loss\n",
    "        \n",
    "        if self.is_target:\n",
    "            total_loss *= self.target_weight\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e1f6e1-f393-4f0e-8657-fe0e73f3e483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears',\n",
       " 'CC: Climate change is beneficial',\n",
       " 'CC: Controversy about green technologies',\n",
       " 'CC: Criticism of climate movement',\n",
       " 'CC: Criticism of climate policies',\n",
       " 'CC: Criticism of institutions and authorities',\n",
       " 'CC: Downplaying climate change',\n",
       " 'CC: Green policies are geopolitical instruments',\n",
       " 'CC: Hidden plots by secret schemes of powerful groups',\n",
       " 'CC: Questioning the measurements and science',\n",
       " 'Other',\n",
       " 'URW: Amplifying war-related fears',\n",
       " 'URW: Blaming the war on others rather than the invader',\n",
       " 'URW: Discrediting Ukraine',\n",
       " 'URW: Discrediting the West, Diplomacy',\n",
       " 'URW: Distrust towards Media',\n",
       " 'URW: Hidden plots by secret schemes of powerful groups',\n",
       " 'URW: Negative Consequences for the West',\n",
       " 'URW: Overpraising the West',\n",
       " 'URW: Praise of Russia',\n",
       " 'URW: Russia is the Victim',\n",
       " 'URW: Speculating war outcomes']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4890d1d-9e36-48b9-a078-b229462ce0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears: Amplifying existing fears of global warming',\n",
       " 'CC: Amplifying Climate Fears: Doomsday scenarios for humans',\n",
       " 'CC: Amplifying Climate Fears: Earth will be uninhabitable soon',\n",
       " 'CC: Amplifying Climate Fears: Other',\n",
       " 'CC: Amplifying Climate Fears: Whatever we do it is already too late',\n",
       " 'CC: Climate change is beneficial: CO2 is beneficial',\n",
       " 'CC: Climate change is beneficial: Other',\n",
       " 'CC: Climate change is beneficial: Temperature increase is beneficial',\n",
       " 'CC: Controversy about green technologies: Other',\n",
       " 'CC: Controversy about green technologies: Renewable energy is costly',\n",
       " 'CC: Controversy about green technologies: Renewable energy is dangerous',\n",
       " 'CC: Controversy about green technologies: Renewable energy is unreliable',\n",
       " 'CC: Criticism of climate movement: Ad hominem attacks on key activists',\n",
       " 'CC: Criticism of climate movement: Climate movement is alarmist',\n",
       " 'CC: Criticism of climate movement: Climate movement is corrupt']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b8a9780-236e-4b63-a7e2-2a0042425736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50c4dba6-4054-4318-b1d1-32ef72770183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class MultiHeadEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes_coarse=coarse_classes,\n",
    "        classes_fine=fine_classes,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu',\n",
    "        output_dir='../../../submissions',\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.narrative_classes = list(narrative_classes)\n",
    "        self.subnarrative_classes = list(subnarrative_classes)\n",
    "        \n",
    "        self.classes_coarse = classes_coarse\n",
    "        self.classes_fine = classes_fine\n",
    "\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings,\n",
    "        dataset,\n",
    "        thresholds=None,\n",
    "        save=False,\n",
    "        std_weight=0.6,\n",
    "        lower_thres=0.1,\n",
    "        upper_thres=0.6,\n",
    "        show_results=True\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(lower_thres, upper_thres, 0.05)\n",
    "        \n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        embeddings = embeddings.to(self.device)\n",
    "    \n",
    "        best_results = {\n",
    "            'best_coarse_f1': -1,\n",
    "            'best_coarse_std': float('inf'),\n",
    "            'best_fine_f1': -1,\n",
    "            'best_fine_std': float('inf'),\n",
    "            'narr_threshold': 0,\n",
    "            'sub_threshold': 0,\n",
    "            'predictions': None,\n",
    "            'best_combined_score': -float('inf'),\n",
    "            'coarse_classification_report': None,\n",
    "            'fine_precision': None,\n",
    "            'fine_recall': None,\n",
    "            'samples_f1_fine': None,\n",
    "        }\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "    \n",
    "        for narr_threshold in thresholds:\n",
    "            for sub_threshold in thresholds:\n",
    "                predictions = []\n",
    "                try:\n",
    "                    for sample_idx, row in dataset.iterrows():\n",
    "                        pred = self._make_prediction(\n",
    "                            row['article_id'],\n",
    "                            sample_idx,\n",
    "                            narr_probs,\n",
    "                            sub_probs_dict,\n",
    "                            narr_threshold,\n",
    "                            sub_threshold\n",
    "                        )\n",
    "                        predictions.append(pred)\n",
    "                    \n",
    "                    metrics_result = self._compute_metrics_coarse_fine(predictions, dataset)\n",
    "                    f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, \\\n",
    "                    samples_f1_fine = metrics_result\n",
    "                    \n",
    "                    combined_score = f1_fine_mean - (std_weight * coarse_std)\n",
    "                    \n",
    "                    if combined_score > best_results['best_combined_score']:\n",
    "                        best_results.update({\n",
    "                            'best_coarse_f1': f1_coarse_mean,\n",
    "                            'best_coarse_std': coarse_std,\n",
    "                            'best_fine_f1': f1_fine_mean,\n",
    "                            'best_fine_std': fine_std,\n",
    "                            'narr_threshold': narr_threshold,\n",
    "                            'sub_threshold': sub_threshold,\n",
    "                            'predictions': predictions,\n",
    "                            'best_combined_score': combined_score,\n",
    "                            'coarse_classification_report': report_coarse,\n",
    "                            'fine_precision': precision_fine,\n",
    "                            'fine_recall': recall_fine,\n",
    "                            'samples_f1_fine': samples_f1_fine,\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during evaluation with thresholds {narr_threshold:.2f}, {sub_threshold:.2f}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        if show_results:\n",
    "            print(\"\\nBest thresholds found:\")\n",
    "            print(f\"Narrative threshold: {best_results['narr_threshold']:.2f}\")\n",
    "            print(f\"Subnarrative threshold: {best_results['sub_threshold']:.2f}\")\n",
    "            print('\\nCompetition Values')\n",
    "            print(f\"Coarse-F1: {best_results['best_coarse_f1']:.3f}\")\n",
    "            print(f\"F1 st. dev. coarse: {best_results['best_coarse_std']:.3f}\")\n",
    "            print(f\"Fine-F1: {best_results['best_fine_f1']:.3f}\")\n",
    "            print(f\"F1 st. dev. fine: {best_results['best_fine_std']:.3f}\")\n",
    "            print(\"\\nFine Metrics:\")\n",
    "            print(\"Precision: {:.3f}\".format(best_results['fine_precision']))\n",
    "            print(\"Recall: {:.3f}\".format(best_results['fine_recall']))\n",
    "            print(\"F1 Samples: {:.3f}\".format(best_results['samples_f1_fine']))\n",
    "\n",
    "        if save:\n",
    "            self._save_predictions(best_results, os.path.join(self.output_dir, 'submission.txt'))\n",
    "        \n",
    "        return best_results\n",
    "\n",
    "    def _make_prediction(self, article_id, sample_idx, narr_probs, sub_probs_dict, narr_threshold, sub_threshold):       \n",
    "        other_idx = self.narrative_classes.index(\"Other\")  \n",
    "        active_narratives = [\n",
    "            (n_idx, prob)\n",
    "            for n_idx, prob in enumerate(narr_probs[sample_idx])\n",
    "            if n_idx != other_idx and prob >= narr_threshold\n",
    "        ]\n",
    "\n",
    "        if not active_narratives:\n",
    "            return {\n",
    "                'article_id': article_id,\n",
    "                'narratives': [\"Other\"],\n",
    "                'pairs': [\"Other\"]\n",
    "            }\n",
    "        \n",
    "        narratives = []\n",
    "        pairs = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        active_narratives.sort(key=lambda x: x[1], reverse=True)\n",
    "        for narr_idx, _ in active_narratives:\n",
    "            narr_name = self.narrative_classes[narr_idx]\n",
    "                \n",
    "            sub_probs = sub_probs_dict[str(narr_idx)][sample_idx]\n",
    "            active_subnarratives = [\n",
    "                (local_idx, s_prob)\n",
    "                for local_idx, s_prob in enumerate(sub_probs)\n",
    "                if s_prob >= sub_threshold\n",
    "            ]\n",
    "            \n",
    "            active_subnarratives.sort(key=lambda x: x[1], reverse=True)\n",
    "            if not active_subnarratives:\n",
    "                pairs.append(f\"{narr_name}: Other\")\n",
    "            else:\n",
    "                for local_idx, _ in active_subnarratives:   \n",
    "                    global_sub_idx = self.narrative_to_sub_map[narr_idx][local_idx]\n",
    "                    sub_name = self.subnarrative_classes[global_sub_idx]\n",
    "                    pair = f\"{narr_name}: {sub_name}\"\n",
    "                    if pair not in seen_pairs:\n",
    "                        pairs.append(pair)\n",
    "                        seen_pairs.add(pair)\n",
    "            narratives.append(narr_name)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'narratives': narratives,\n",
    "            'pairs': pairs\n",
    "        }\n",
    "\n",
    "    def _compute_metrics_coarse_fine(self, predictions, dataset):\n",
    "        gold_coarse_all = []\n",
    "        gold_fine_all = []\n",
    "        pred_coarse_all = []\n",
    "        pred_fine_all = []\n",
    "\n",
    "        for pred, (_, row) in zip(predictions, dataset.iterrows()):\n",
    "            gold_coarse = row['narratives']\n",
    "            gold_subnarratives = row['subnarratives']\n",
    "            \n",
    "            pred_coarse = pred['narratives']\n",
    "            pred_fine = []\n",
    "            for p in pred['pairs']:\n",
    "                if p == \"Other\":\n",
    "                    pred_fine.append(\"Other\")\n",
    "                else:\n",
    "                    pred_fine.append(p)\n",
    "\n",
    "            gold_fine = []\n",
    "            for gold_nar, gold_sub in zip(gold_coarse, gold_subnarratives):\n",
    "                if gold_nar == \"Other\":\n",
    "                    gold_fine.append(\"Other\")\n",
    "                else:\n",
    "                    gold_fine.append(f\"{gold_nar}: {gold_sub}\")\n",
    "            \n",
    "            gold_coarse_all.append(gold_coarse)\n",
    "            gold_fine_all.append(gold_fine)\n",
    "            pred_coarse_all.append(pred_coarse)\n",
    "            pred_fine_all.append(pred_fine)\n",
    "\n",
    "        f1_coarse_mean, coarse_std = self._evaluate_multi_label(gold_coarse_all, pred_coarse_all, self.classes_coarse)\n",
    "        f1_fine_mean, fine_std = self._evaluate_multi_label(gold_fine_all, pred_fine_all, self.classes_fine)\n",
    "        \n",
    "        gold_coarse_flat = []\n",
    "        pred_coarse_flat = []\n",
    "        for g_labels, p_labels in zip(gold_coarse_all, pred_coarse_all):\n",
    "            g_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            p_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            \n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    g_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    p_onehot[self.classes_coarse.index(lab)] = 1\n",
    "                    \n",
    "            gold_coarse_flat.append(g_onehot)\n",
    "            pred_coarse_flat.append(p_onehot)\n",
    "            \n",
    "        gold_coarse_flat = np.array(gold_coarse_flat)\n",
    "        pred_coarse_flat = np.array(pred_coarse_flat)\n",
    "        \n",
    "        report_coarse = metrics.classification_report(\n",
    "                gold_coarse_flat, pred_coarse_flat, \n",
    "                target_names=self.classes_coarse, \n",
    "                zero_division=0\n",
    "        )\n",
    "        \n",
    "        gold_fine_flat = []\n",
    "        pred_fine_flat = []\n",
    "        for g_labels, p_labels in zip(gold_fine_all, pred_fine_all):\n",
    "            g_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            p_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            \n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    g_onehot[self.classes_fine.index(lab)] = 1\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    p_onehot[self.classes_fine.index(lab)] = 1\n",
    "                    \n",
    "            gold_fine_flat.append(g_onehot)\n",
    "            pred_fine_flat.append(p_onehot)\n",
    "            \n",
    "        gold_fine_flat = np.array(gold_fine_flat)\n",
    "        pred_fine_flat = np.array(pred_fine_flat)\n",
    "        \n",
    "        precision_fine = metrics.precision_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        recall_fine = metrics.recall_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        samples_f1_fine = metrics.f1_score(gold_fine_flat, pred_fine_flat, average='samples', zero_division=0)\n",
    "        \n",
    "        return f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, samples_f1_fine\n",
    "\n",
    "    def _evaluate_multi_label(self, gold, predicted, class_list):\n",
    "        f1_scores = []\n",
    "        for g_labels, p_labels in zip(gold, predicted):\n",
    "            g_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            p_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            \n",
    "            for lab in g_labels:\n",
    "                if lab in class_list:\n",
    "                    g_onehot[class_list.index(lab)] = 1\n",
    "            for lab in p_labels:\n",
    "                if lab in class_list:\n",
    "                    p_onehot[class_list.index(lab)] = 1\n",
    "                    \n",
    "            f1_doc = metrics.f1_score(g_onehot, p_onehot, zero_division=0)\n",
    "            f1_scores.append(f1_doc)\n",
    "            \n",
    "        return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "    def _save_predictions(self, best_results, filepath):\n",
    "        predictions = best_results['predictions']\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                line = (f\"{pred['article_id']}\\t\"\n",
    "                       f\"{';'.join(pred['narratives'])}\\t\"\n",
    "                       f\"{';'.join(pred['pairs'])}\\n\")\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71e91a-9e25-492a-8828-bc6a249310d7",
   "metadata": {},
   "source": [
    "As we train on multiple languages in sequence, the last language (our target) needs some kind of special care. \n",
    "Our goal is to make sure the model performs best on our target language, without losing what it learned from previous languages. When we reach the target language, we make two key changes:\n",
    "- We increase the patience parameter to train the model more carefully on the target language.\n",
    "- We also lower the learning rate, because the model has already learned some patterns from other languages, we want smaller, more precise updates for the target language.\n",
    "- We monitor target language validation throughout all sequential training phases, allowing each language to train until no further improvements are seen on the target language metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cbdc283-3c7c-46c5-bb41-cb607b4837b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinualLearningModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_params,\n",
    "        dataset_val,\n",
    "        val_embeddings,\n",
    "        model_class=MultiTaskClassifierMultiHeadConcat,\n",
    "        dataset_train=dataset_train,\n",
    "        train_embeddings=train_embeddings,\n",
    "        language_order=['RU', 'BG', 'HI', 'PT', 'EN'],\n",
    "        learning_rate=0.001,\n",
    "        target=\"EN\",\n",
    "        device=device,\n",
    "        show_progress=True\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.dataset_train = dataset_train\n",
    "        self.train_embeddings = train_embeddings\n",
    "        self.dataset_val = dataset_val\n",
    "        self.val_embeddings = val_embeddings\n",
    "        self.language_order = language_order\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.target = target\n",
    "        self.y_val_nar = self.dataset_val['narratives_encoded'].tolist()\n",
    "        self.y_val_sub_heads = self.dataset_val['aggregated_subnarratives'].tolist()\n",
    "        self.show_progress = show_progress\n",
    "        \n",
    "\n",
    "    def _prepare_language_data(self, language, shuffle=False):\n",
    "        language_mask = self.dataset_train[\"language\"] == language\n",
    "        train_data = self.dataset_train[language_mask].copy()\n",
    "        train_emb = self.train_embeddings[language_mask]\n",
    "        \n",
    "        if shuffle:\n",
    "            indices = torch.randperm(len(train_data))\n",
    "            train_data = train_data.iloc[indices].reset_index(drop=True)\n",
    "            train_emb = train_emb[indices]\n",
    "        \n",
    "        y_train_nar = torch.tensor(train_data['narratives_encoded'].tolist(), dtype=torch.float32).to(self.device)\n",
    "        y_train_sub_heads = train_data['aggregated_subnarratives'].tolist()\n",
    "        train_emb = torch.tensor(train_emb, dtype=torch.float32).to(self.device)\n",
    "        return train_data, train_emb, y_train_nar, y_train_sub_heads\n",
    "\n",
    "    def _setup_loss_function(self, y_train_nar, y_train_sub_heads, language):\n",
    "        class_weights_nar = compute_class_weights(y_train_nar)\n",
    "        narrative_criterion = WeightedBCELoss(class_weights_nar)\n",
    "        \n",
    "        sub_criterion_dict = {}\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            local_weights = compute_class_weights(torch.tensor([h[narr_idx] for h in y_train_sub_heads]))\n",
    "            sub_criterion = WeightedBCELoss(local_weights)\n",
    "            sub_criterion_dict[str(narr_idx)] = sub_criterion\n",
    "            \n",
    "        if (language==self.target):\n",
    "            print('Focusing on', self.target)\n",
    "            \n",
    "        return MultiHeadLoss(narrative_criterion, sub_criterion_dict, is_target=(language == self.target))\n",
    "\n",
    "    def train(self, epochs_per_language=100, patience=10, shuffle=False):\n",
    "        self.model = self.model_class(**self.model_params).to(self.device)\n",
    "\n",
    "        for lang_idx, language in enumerate(self.language_order):\n",
    "            print(f\"\\nTraining on {language} data...\")\n",
    "            \n",
    "            if language == self.target:\n",
    "                patience = patience * 2\n",
    "                learning_rate = self.learning_rate * 0.2\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, \n",
    "                    mode='min', \n",
    "                    factor=0.5,\n",
    "                    patience=8,\n",
    "                    min_lr=2e-5,\n",
    "                    threshold=1e-4\n",
    "                )\n",
    "            else:\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=5\n",
    "                )\n",
    "            \n",
    "            train_data, train_emb, y_train_nar, y_train_sub_heads = self._prepare_language_data(language, shuffle=shuffle)\n",
    "            loss_fn = self._setup_loss_function(y_train_nar, y_train_sub_heads, language)\n",
    "            val_emb_tensor = torch.tensor(self.val_embeddings, dtype=torch.float32).to(self.device)\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            best_model_state = None\n",
    "\n",
    "            for epoch in range(epochs_per_language):\n",
    "                self.model.train()\n",
    "                train_narr_probs, train_sub_probs_dict = self.model(train_emb)\n",
    "                train_loss = loss_fn(\n",
    "                    train_narr_probs,\n",
    "                    train_sub_probs_dict,\n",
    "                    y_train_nar,\n",
    "                    y_train_sub_heads\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                                    \n",
    "                optimizer.step()\n",
    "\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_narr_probs, val_sub_probs_dict = self.model(val_emb_tensor)\n",
    "                    val_loss = loss_fn(\n",
    "                        val_narr_probs,\n",
    "                        val_sub_probs_dict,\n",
    "                        torch.tensor(self.y_val_nar, dtype=torch.float32).to(self.device),\n",
    "                        self.y_val_sub_heads\n",
    "                    )\n",
    "                if self.show_progress:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs_per_language}, \"\n",
    "                          f\"Train Loss: {train_loss.item():.4f}, \"\n",
    "                          f\"Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(val_loss)\n",
    "                    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "                    if self.show_progress: print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    if self.show_progress: print(f\"Early stopping triggered for {language}\")\n",
    "                    break\n",
    "\n",
    "            if best_model_state:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate_final(self, save_predictions=True):\n",
    "        evaluator = MultiHeadEvaluator(device=self.device)\n",
    "        val_emb_tensor = torch.tensor(self.val_embeddings, dtype=torch.float32).to(self.device)\n",
    "        res = evaluator.evaluate(\n",
    "            self.model,\n",
    "            val_emb_tensor,\n",
    "            self.dataset_val,\n",
    "            save=save_predictions\n",
    "        )\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e582f89-6a54-42f9-b63e-218de500cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_order=['RU', 'BG', 'PT', 'HI', 'EN']\n",
    "target=language_order[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64f57653-fd1a-4cae-9576-35f1ff023fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size': train_embeddings.shape[1],\n",
    "    'hidden_size': 2048,\n",
    "    'dropout_rate': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe44ba3b-307f-4b94-ab05-de4566f1f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_cl_model = ContinualLearningModel(\n",
    "    model_params=model_params,\n",
    "    language_order=language_order,\n",
    "    dataset_val=dataset_val_en,\n",
    "    val_embeddings=val_embeddings_en,\n",
    "    target=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39f528c1-e47f-4e41-bf74-46e049801217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on RU data...\n",
      "Epoch 1/100, Train Loss: 0.7238, Val Loss: 0.8750\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.4172, Val Loss: 0.8721\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.2860, Val Loss: 0.8734\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.2264, Val Loss: 0.8789\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.1901, Val Loss: 0.8866\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.1656, Val Loss: 0.8948\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.1474, Val Loss: 0.9040\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.1319, Val Loss: 0.9139\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.1213, Val Loss: 0.9200\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.1148, Val Loss: 0.9265\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.1075, Val Loss: 0.9337\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.1009, Val Loss: 0.9418\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.0963, Val Loss: 0.9509\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.0918, Val Loss: 0.9618\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.0863, Val Loss: 0.9720\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.0829, Val Loss: 0.9837\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.0812, Val Loss: 0.9969\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for RU\n",
      "\n",
      "Training on BG data...\n",
      "Epoch 1/100, Train Loss: 3.1385, Val Loss: 2.6831\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 1.5751, Val Loss: 2.4723\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.8312, Val Loss: 2.3914\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5643, Val Loss: 2.3690\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4737, Val Loss: 2.3749\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4413, Val Loss: 2.3947\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4185, Val Loss: 2.4217\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.4062, Val Loss: 2.4531\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.3914, Val Loss: 2.4891\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.3692, Val Loss: 2.5319\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.3508, Val Loss: 2.5265\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.3402, Val Loss: 2.5268\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.3264, Val Loss: 2.5395\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.3144, Val Loss: 2.5633\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 15/100, Train Loss: 0.3015, Val Loss: 2.6014\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 16/100, Train Loss: 0.2843, Val Loss: 2.6604\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.2763, Val Loss: 2.6906\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 18/100, Train Loss: 0.2712, Val Loss: 2.7340\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 19/100, Train Loss: 0.2666, Val Loss: 2.7902\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for BG\n",
      "\n",
      "Training on PT data...\n",
      "Epoch 1/100, Train Loss: 0.9242, Val Loss: 1.4846\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.5627, Val Loss: 1.5577\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.4783, Val Loss: 1.6883\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.4300, Val Loss: 1.8369\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.3901, Val Loss: 1.9781\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.3609, Val Loss: 2.1406\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3313, Val Loss: 2.3509\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.2979, Val Loss: 2.3306\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.2879, Val Loss: 2.3139\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.2754, Val Loss: 2.2980\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.2620, Val Loss: 2.2978\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.2555, Val Loss: 2.3040\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.2459, Val Loss: 2.3183\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 14/100, Train Loss: 0.2376, Val Loss: 2.2546\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.2339, Val Loss: 2.2091\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.2297, Val Loss: 2.1818\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for PT\n",
      "\n",
      "Training on HI data...\n",
      "Epoch 1/100, Train Loss: 1.7243, Val Loss: 7.2944\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.9065, Val Loss: 10.0356\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.6642, Val Loss: 12.8812\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5597, Val Loss: 15.3698\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4831, Val Loss: 17.2557\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4297, Val Loss: 18.6075\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3870, Val Loss: 19.7727\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.3586, Val Loss: 18.4229\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.3436, Val Loss: 17.5443\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.3274, Val Loss: 17.2115\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.3090, Val Loss: 17.3624\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.2957, Val Loss: 17.8939\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.2846, Val Loss: 18.7837\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 14/100, Train Loss: 0.2752, Val Loss: 19.2551\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.2701, Val Loss: 19.9729\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.2628, Val Loss: 20.9206\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for HI\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Epoch 1/100, Train Loss: 2.3924, Val Loss: 4.7782\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 2/100, Train Loss: 2.1451, Val Loss: 4.0621\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 3/100, Train Loss: 1.9150, Val Loss: 3.5513\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 4/100, Train Loss: 1.7467, Val Loss: 3.1968\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 5/100, Train Loss: 1.6188, Val Loss: 2.9617\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 6/100, Train Loss: 1.5019, Val Loss: 2.8107\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 7/100, Train Loss: 1.4155, Val Loss: 2.7187\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 8/100, Train Loss: 1.3273, Val Loss: 2.6648\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 9/100, Train Loss: 1.2230, Val Loss: 2.6296\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 10/100, Train Loss: 1.1573, Val Loss: 2.6078\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 11/100, Train Loss: 1.1032, Val Loss: 2.5970\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 12/100, Train Loss: 1.0697, Val Loss: 2.5892\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 13/100, Train Loss: 1.0229, Val Loss: 2.5825\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 14/100, Train Loss: 0.9817, Val Loss: 2.5737\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 15/100, Train Loss: 0.9412, Val Loss: 2.5619\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 16/100, Train Loss: 0.9096, Val Loss: 2.5500\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 17/100, Train Loss: 0.8901, Val Loss: 2.5355\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 18/100, Train Loss: 0.8514, Val Loss: 2.5228\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 19/100, Train Loss: 0.8379, Val Loss: 2.5114\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 20/100, Train Loss: 0.7990, Val Loss: 2.5045\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 21/100, Train Loss: 0.7840, Val Loss: 2.5030\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 22/100, Train Loss: 0.7645, Val Loss: 2.5018\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 23/100, Train Loss: 0.7387, Val Loss: 2.5026\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 24/100, Train Loss: 0.7239, Val Loss: 2.4997\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 25/100, Train Loss: 0.7155, Val Loss: 2.4956\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 26/100, Train Loss: 0.6942, Val Loss: 2.4923\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 27/100, Train Loss: 0.6816, Val Loss: 2.4818\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 28/100, Train Loss: 0.6631, Val Loss: 2.4717\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 29/100, Train Loss: 0.6509, Val Loss: 2.4551\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 30/100, Train Loss: 0.6383, Val Loss: 2.4362\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 31/100, Train Loss: 0.6293, Val Loss: 2.4146\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 32/100, Train Loss: 0.6172, Val Loss: 2.3936\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 33/100, Train Loss: 0.6039, Val Loss: 2.3671\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 34/100, Train Loss: 0.5949, Val Loss: 2.3335\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 35/100, Train Loss: 0.5925, Val Loss: 2.3025\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 36/100, Train Loss: 0.5745, Val Loss: 2.2716\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 37/100, Train Loss: 0.5682, Val Loss: 2.2436\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 38/100, Train Loss: 0.5626, Val Loss: 2.2130\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 39/100, Train Loss: 0.5504, Val Loss: 2.1838\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 40/100, Train Loss: 0.5456, Val Loss: 2.1571\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 41/100, Train Loss: 0.5364, Val Loss: 2.1288\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 42/100, Train Loss: 0.5295, Val Loss: 2.1038\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 43/100, Train Loss: 0.5185, Val Loss: 2.0832\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 44/100, Train Loss: 0.5147, Val Loss: 2.0645\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 45/100, Train Loss: 0.5073, Val Loss: 2.0489\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 46/100, Train Loss: 0.5019, Val Loss: 2.0361\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 47/100, Train Loss: 0.4946, Val Loss: 2.0230\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 48/100, Train Loss: 0.4849, Val Loss: 2.0114\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 49/100, Train Loss: 0.4828, Val Loss: 2.0018\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 50/100, Train Loss: 0.4748, Val Loss: 1.9945\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 51/100, Train Loss: 0.4674, Val Loss: 1.9866\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 52/100, Train Loss: 0.4640, Val Loss: 1.9785\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 53/100, Train Loss: 0.4545, Val Loss: 1.9746\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 54/100, Train Loss: 0.4497, Val Loss: 1.9722\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 55/100, Train Loss: 0.4422, Val Loss: 1.9742\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 56/100, Train Loss: 0.4385, Val Loss: 1.9822\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 57/100, Train Loss: 0.4316, Val Loss: 1.9924\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 58/100, Train Loss: 0.4293, Val Loss: 2.0032\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 59/100, Train Loss: 0.4220, Val Loss: 2.0136\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 60/100, Train Loss: 0.4189, Val Loss: 2.0212\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 61/100, Train Loss: 0.4140, Val Loss: 2.0310\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 62/100, Train Loss: 0.4062, Val Loss: 2.0432\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 63/100, Train Loss: 0.4012, Val Loss: 2.0562\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 64/100, Train Loss: 0.3967, Val Loss: 2.0558\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 65/100, Train Loss: 0.3971, Val Loss: 2.0548\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 66/100, Train Loss: 0.3923, Val Loss: 2.0545\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 67/100, Train Loss: 0.3915, Val Loss: 2.0564\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 68/100, Train Loss: 0.3932, Val Loss: 2.0596\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 69/100, Train Loss: 0.3857, Val Loss: 2.0635\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 70/100, Train Loss: 0.3835, Val Loss: 2.0690\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 71/100, Train Loss: 0.3841, Val Loss: 2.0777\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 72/100, Train Loss: 0.3809, Val Loss: 2.0892\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 73/100, Train Loss: 0.3801, Val Loss: 2.0930\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 74/100, Train Loss: 0.3765, Val Loss: 2.0971\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 75/100, Train Loss: 0.3764, Val Loss: 2.1024\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 76/100, Train Loss: 0.3743, Val Loss: 2.1083\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 77/100, Train Loss: 0.3739, Val Loss: 2.1146\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 78/100, Train Loss: 0.3728, Val Loss: 2.1207\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 79/100, Train Loss: 0.3733, Val Loss: 2.1272\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 80/100, Train Loss: 0.3708, Val Loss: 2.1336\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 81/100, Train Loss: 0.3693, Val Loss: 2.1408\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 82/100, Train Loss: 0.3663, Val Loss: 2.1438\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 83/100, Train Loss: 0.3698, Val Loss: 2.1462\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 84/100, Train Loss: 0.3687, Val Loss: 2.1479\n",
      "Current Learning Rate: 0.000025\n",
      "Early stopping triggered for EN\n"
     ]
    }
   ],
   "source": [
    "en_model_right = en_cl_model.train(epochs_per_language=100, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6834c3ab-39f1-4105-9d5d-9d5d9e6f23f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.30\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.538\n",
      "F1 st. dev. coarse: 0.346\n",
      "Fine-F1: 0.374\n",
      "F1 st. dev. fine: 0.343\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.135\n",
      "Recall: 0.292\n",
      "F1 Samples: 0.374\n"
     ]
    }
   ],
   "source": [
    "results = en_cl_model.evaluate_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14841703-2a96-44ef-87bd-bbd619e62be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_thres = results['narr_threshold']\n",
    "sub_thres = results['sub_threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da931e-ca57-4ed9-bb02-3f3f91b4695d",
   "metadata": {},
   "source": [
    "If we change the order of the languages being trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4409aa38-c6f3-4088-83a2-e17ac59b9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_order=['RU', 'HI', 'PT', 'BG', 'EN']\n",
    "target=language_order[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74ab6b2a-021c-4cc2-91ba-9829c29bbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_model_demo = ContinualLearningModel(\n",
    "    model_params=model_params,\n",
    "    language_order=language_order,\n",
    "    dataset_val=dataset_val_en,\n",
    "    val_embeddings=val_embeddings_en,\n",
    "    target=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "614777df-64e0-408c-ba20-f2da6c6dd8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on RU data...\n",
      "Epoch 1/100, Train Loss: 0.7062, Val Loss: 0.8722\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.4080, Val Loss: 0.8698\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.2828, Val Loss: 0.8713\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.2222, Val Loss: 0.8766\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.1884, Val Loss: 0.8852\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.1639, Val Loss: 0.8959\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.1470, Val Loss: 0.9079\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.1316, Val Loss: 0.9203\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.1183, Val Loss: 0.9272\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.1121, Val Loss: 0.9341\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.1062, Val Loss: 0.9412\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.1003, Val Loss: 0.9492\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.0963, Val Loss: 0.9583\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.0905, Val Loss: 0.9690\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.0859, Val Loss: 0.9793\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.0834, Val Loss: 0.9908\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.0810, Val Loss: 1.0036\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for RU\n",
      "\n",
      "Training on HI data...\n",
      "Epoch 1/100, Train Loss: 3.8849, Val Loss: 6.3826\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 2.4344, Val Loss: 6.0644\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 1.2803, Val Loss: 5.7732\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.7194, Val Loss: 5.5856\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5177, Val Loss: 5.4809\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4403, Val Loss: 5.4231\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4146, Val Loss: 5.4132\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.4016, Val Loss: 5.4441\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.3952, Val Loss: 5.5057\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.3752, Val Loss: 5.6357\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.3491, Val Loss: 5.8546\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.3309, Val Loss: 6.1874\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.3071, Val Loss: 6.6645\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.2864, Val Loss: 7.0823\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 15/100, Train Loss: 0.2783, Val Loss: 7.6534\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 16/100, Train Loss: 0.2706, Val Loss: 8.3643\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 17/100, Train Loss: 0.2624, Val Loss: 9.2306\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 18/100, Train Loss: 0.2548, Val Loss: 10.2536\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 19/100, Train Loss: 0.2496, Val Loss: 11.4272\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 20/100, Train Loss: 0.2458, Val Loss: 12.5131\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 21/100, Train Loss: 0.2436, Val Loss: 13.7022\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 22/100, Train Loss: 0.2407, Val Loss: 14.9907\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for HI\n",
      "\n",
      "Training on PT data...\n",
      "Epoch 1/100, Train Loss: 1.2867, Val Loss: 3.2743\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.6975, Val Loss: 3.0167\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.5481, Val Loss: 3.1028\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.4827, Val Loss: 3.3414\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4309, Val Loss: 3.6676\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.3970, Val Loss: 4.0078\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3698, Val Loss: 4.3373\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.3446, Val Loss: 4.6591\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.3244, Val Loss: 4.4751\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.3122, Val Loss: 4.2864\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.2992, Val Loss: 4.0876\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.2877, Val Loss: 3.8921\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.2797, Val Loss: 3.7121\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.2721, Val Loss: 3.5479\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.2660, Val Loss: 3.2934\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.2613, Val Loss: 3.0765\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.2573, Val Loss: 2.8933\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 18/100, Train Loss: 0.2519, Val Loss: 2.7390\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 19/100, Train Loss: 0.2479, Val Loss: 2.6115\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 20/100, Train Loss: 0.2431, Val Loss: 2.5122\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 21/100, Train Loss: 0.2417, Val Loss: 2.4364\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 22/100, Train Loss: 0.2368, Val Loss: 2.3812\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 23/100, Train Loss: 0.2328, Val Loss: 2.3461\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 24/100, Train Loss: 0.2291, Val Loss: 2.3255\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 25/100, Train Loss: 0.2268, Val Loss: 2.3169\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 26/100, Train Loss: 0.2238, Val Loss: 2.3203\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 27/100, Train Loss: 0.2182, Val Loss: 2.3339\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 28/100, Train Loss: 0.2154, Val Loss: 2.3542\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 29/100, Train Loss: 0.2130, Val Loss: 2.3819\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 30/100, Train Loss: 0.2094, Val Loss: 2.4173\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 31/100, Train Loss: 0.2062, Val Loss: 2.4573\n",
      "Current Learning Rate: 0.000125\n",
      "Epoch 32/100, Train Loss: 0.2032, Val Loss: 2.4662\n",
      "Current Learning Rate: 0.000125\n",
      "Epoch 33/100, Train Loss: 0.2025, Val Loss: 2.4813\n",
      "Current Learning Rate: 0.000125\n",
      "Epoch 34/100, Train Loss: 0.1999, Val Loss: 2.5020\n",
      "Current Learning Rate: 0.000125\n",
      "Epoch 35/100, Train Loss: 0.1984, Val Loss: 2.5261\n",
      "Current Learning Rate: 0.000125\n",
      "Epoch 36/100, Train Loss: 0.1980, Val Loss: 2.5548\n",
      "Current Learning Rate: 0.000125\n",
      "Epoch 37/100, Train Loss: 0.1965, Val Loss: 2.5864\n",
      "Current Learning Rate: 0.000063\n",
      "Epoch 38/100, Train Loss: 0.1944, Val Loss: 2.5971\n",
      "Current Learning Rate: 0.000063\n",
      "Epoch 39/100, Train Loss: 0.1940, Val Loss: 2.6105\n",
      "Current Learning Rate: 0.000063\n",
      "Epoch 40/100, Train Loss: 0.1924, Val Loss: 2.6253\n",
      "Current Learning Rate: 0.000063\n",
      "Early stopping triggered for PT\n",
      "\n",
      "Training on BG data...\n",
      "Epoch 1/100, Train Loss: 1.3636, Val Loss: 5.5477\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.8406, Val Loss: 8.1822\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.6728, Val Loss: 10.9736\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.6239, Val Loss: 13.2939\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5704, Val Loss: 14.9359\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.5198, Val Loss: 16.0289\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4884, Val Loss: 16.6201\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.4670, Val Loss: 15.0887\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.4500, Val Loss: 13.5368\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.4387, Val Loss: 11.9945\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.4197, Val Loss: 10.5704\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.4070, Val Loss: 9.3582\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.3898, Val Loss: 8.3669\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 14/100, Train Loss: 0.3818, Val Loss: 7.5037\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.3726, Val Loss: 6.8690\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.3662, Val Loss: 6.4060\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for BG\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Epoch 1/100, Train Loss: 2.3401, Val Loss: 3.8681\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 2/100, Train Loss: 2.1281, Val Loss: 3.1105\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 3/100, Train Loss: 1.9828, Val Loss: 2.6782\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 4/100, Train Loss: 1.8200, Val Loss: 2.4447\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 5/100, Train Loss: 1.7166, Val Loss: 2.3259\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 6/100, Train Loss: 1.6307, Val Loss: 2.2748\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 7/100, Train Loss: 1.5520, Val Loss: 2.2657\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 8/100, Train Loss: 1.4651, Val Loss: 2.2760\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 9/100, Train Loss: 1.4017, Val Loss: 2.3006\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 10/100, Train Loss: 1.3442, Val Loss: 2.3342\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 11/100, Train Loss: 1.2837, Val Loss: 2.3719\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 12/100, Train Loss: 1.2252, Val Loss: 2.4105\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 13/100, Train Loss: 1.1685, Val Loss: 2.4524\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 14/100, Train Loss: 1.1208, Val Loss: 2.4944\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 15/100, Train Loss: 1.0655, Val Loss: 2.5360\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 16/100, Train Loss: 1.0371, Val Loss: 2.5761\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 17/100, Train Loss: 1.0071, Val Loss: 2.5200\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 18/100, Train Loss: 0.9792, Val Loss: 2.4703\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 19/100, Train Loss: 0.9750, Val Loss: 2.4276\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 20/100, Train Loss: 0.9638, Val Loss: 2.3894\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 21/100, Train Loss: 0.9444, Val Loss: 2.3560\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 22/100, Train Loss: 0.9327, Val Loss: 2.3281\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 23/100, Train Loss: 0.9098, Val Loss: 2.3049\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 24/100, Train Loss: 0.9014, Val Loss: 2.2858\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 25/100, Train Loss: 0.8768, Val Loss: 2.2703\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 26/100, Train Loss: 0.8686, Val Loss: 2.2317\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 27/100, Train Loss: 0.8692, Val Loss: 2.2003\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 28/100, Train Loss: 0.8529, Val Loss: 2.1746\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 29/100, Train Loss: 0.8442, Val Loss: 2.1534\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 30/100, Train Loss: 0.8417, Val Loss: 2.1373\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 31/100, Train Loss: 0.8362, Val Loss: 2.1247\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 32/100, Train Loss: 0.8257, Val Loss: 2.1150\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 33/100, Train Loss: 0.8181, Val Loss: 2.1085\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 34/100, Train Loss: 0.8097, Val Loss: 2.1045\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 35/100, Train Loss: 0.8041, Val Loss: 2.1022\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 36/100, Train Loss: 0.7992, Val Loss: 2.1008\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 37/100, Train Loss: 0.7981, Val Loss: 2.1005\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 38/100, Train Loss: 0.7959, Val Loss: 2.1018\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 39/100, Train Loss: 0.7832, Val Loss: 2.1038\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 40/100, Train Loss: 0.7739, Val Loss: 2.1071\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 41/100, Train Loss: 0.7680, Val Loss: 2.1111\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 42/100, Train Loss: 0.7666, Val Loss: 2.1154\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 43/100, Train Loss: 0.7557, Val Loss: 2.1202\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 44/100, Train Loss: 0.7551, Val Loss: 2.1249\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 45/100, Train Loss: 0.7515, Val Loss: 2.1295\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 46/100, Train Loss: 0.7386, Val Loss: 2.1348\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 47/100, Train Loss: 0.7475, Val Loss: 2.1279\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 48/100, Train Loss: 0.7252, Val Loss: 2.1219\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 49/100, Train Loss: 0.7325, Val Loss: 2.1171\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 50/100, Train Loss: 0.7299, Val Loss: 2.1137\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 51/100, Train Loss: 0.7281, Val Loss: 2.1109\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 52/100, Train Loss: 0.7221, Val Loss: 2.1090\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 53/100, Train Loss: 0.7214, Val Loss: 2.1076\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 54/100, Train Loss: 0.7197, Val Loss: 2.1068\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 55/100, Train Loss: 0.7221, Val Loss: 2.1061\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 56/100, Train Loss: 0.7106, Val Loss: 2.1034\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 57/100, Train Loss: 0.7077, Val Loss: 2.1010\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 58/100, Train Loss: 0.7209, Val Loss: 2.0992\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 59/100, Train Loss: 0.7033, Val Loss: 2.0975\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 60/100, Train Loss: 0.7046, Val Loss: 2.0959\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 61/100, Train Loss: 0.7067, Val Loss: 2.0945\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 62/100, Train Loss: 0.7027, Val Loss: 2.0931\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 63/100, Train Loss: 0.7037, Val Loss: 2.0919\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 64/100, Train Loss: 0.6995, Val Loss: 2.0908\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 65/100, Train Loss: 0.6921, Val Loss: 2.0897\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 66/100, Train Loss: 0.6921, Val Loss: 2.0891\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 67/100, Train Loss: 0.6992, Val Loss: 2.0890\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 68/100, Train Loss: 0.6889, Val Loss: 2.0890\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 69/100, Train Loss: 0.6898, Val Loss: 2.0893\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 70/100, Train Loss: 0.6924, Val Loss: 2.0895\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 71/100, Train Loss: 0.6875, Val Loss: 2.0899\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 72/100, Train Loss: 0.6875, Val Loss: 2.0906\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 73/100, Train Loss: 0.6827, Val Loss: 2.0915\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 74/100, Train Loss: 0.6880, Val Loss: 2.0924\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 75/100, Train Loss: 0.6793, Val Loss: 2.0932\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 76/100, Train Loss: 0.6838, Val Loss: 2.0946\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 77/100, Train Loss: 0.6756, Val Loss: 2.0960\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 78/100, Train Loss: 0.6758, Val Loss: 2.0972\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 79/100, Train Loss: 0.6767, Val Loss: 2.0983\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 80/100, Train Loss: 0.6773, Val Loss: 2.0996\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 81/100, Train Loss: 0.6744, Val Loss: 2.1011\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 82/100, Train Loss: 0.6782, Val Loss: 2.1022\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 83/100, Train Loss: 0.6677, Val Loss: 2.1033\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 84/100, Train Loss: 0.6661, Val Loss: 2.1049\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 85/100, Train Loss: 0.6647, Val Loss: 2.1065\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 86/100, Train Loss: 0.6631, Val Loss: 2.1079\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 87/100, Train Loss: 0.6719, Val Loss: 2.1095\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 88/100, Train Loss: 0.6621, Val Loss: 2.1110\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 89/100, Train Loss: 0.6580, Val Loss: 2.1125\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 90/100, Train Loss: 0.6574, Val Loss: 2.1140\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 91/100, Train Loss: 0.6597, Val Loss: 2.1153\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 92/100, Train Loss: 0.6564, Val Loss: 2.1170\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 93/100, Train Loss: 0.6566, Val Loss: 2.1186\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 94/100, Train Loss: 0.6558, Val Loss: 2.1202\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 95/100, Train Loss: 0.6536, Val Loss: 2.1218\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 96/100, Train Loss: 0.6497, Val Loss: 2.1233\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 97/100, Train Loss: 0.6490, Val Loss: 2.1248\n",
      "Current Learning Rate: 0.000020\n",
      "Early stopping triggered for EN\n"
     ]
    }
   ],
   "source": [
    "model_demo = cl_model_demo.train(epochs_per_language=100, patience=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277fc9b-c9fc-469a-a8df-291b10cf6d39",
   "metadata": {},
   "source": [
    "The results are poor compared to the first language order.\n",
    "\n",
    "That could be because, the jump from Russian directly to Hindi could be too drastic as these languages have very different structures, and we might try to get very good at it without having the right \"prerequisites\".\n",
    "\n",
    "- In the first instance, however, we have a somewhat more smooth transition, Russian and Bulgarian are both Slavic languages, then the model gets to know different patterns from Portuguese and maybe harder ones from Hindi before moving on to the final target language.\n",
    "\n",
    "However, other factors such as Russian and Bulgarian having certain patterns that overly help on being learned at early stages could actually help on classifying English articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f8be43a-35f1-4018-b765-61c256e69ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.55\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.453\n",
      "F1 st. dev. coarse: 0.348\n",
      "Fine-F1: 0.283\n",
      "F1 st. dev. fine: 0.300\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.110\n",
      "Recall: 0.277\n",
      "F1 Samples: 0.283\n"
     ]
    }
   ],
   "source": [
    "_ = cl_model_demo.evaluate_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0313946-2baa-43ae-89c6-8d390b30971f",
   "metadata": {},
   "source": [
    "While we are at it, notice that we cannot rely on a single language order per target language, as the validation set might have characteristics that work particularly well with that order or the validation set could favor some narratives/subnarratives, and the distribution for the test might (and will) be different.\n",
    "\n",
    "We create an ensemble of models trained with different language orders:\n",
    "* Each model follows some principles, like keeping linguistically similar languages close together.\n",
    "* Models are trained with different but linguistically sensible variations of the order.\n",
    "* The final prediction combines all models outputs, weighted by their validation performance (since clearly, some orders appear to do better than others).\n",
    "\n",
    "This is also helpful since we get a view with continual learning of different languages that have specific narrative/subnarratives that are language-based and thus not in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aeca9789-fef6-4354-8794-944d2310fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinualLearningEnsemble:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_params,\n",
    "        dataset_val,\n",
    "        val_embeddings,\n",
    "        language_orders,\n",
    "        model_class=MultiTaskClassifierMultiHead,\n",
    "        dataset_train=dataset_train,\n",
    "        train_embeddings=train_embeddings,\n",
    "        learning_rate=0.001,\n",
    "        device=device\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.dataset_train = dataset_train\n",
    "        self.train_embeddings = train_embeddings\n",
    "        self.dataset_val = dataset_val\n",
    "        self.val_embeddings = val_embeddings\n",
    "        self.language_orders = language_orders\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.models = []  \n",
    "        \n",
    "    def train(self):\n",
    "        for order in self.language_orders:\n",
    "            print(f\"\\nTraining model with order: {order}\")\n",
    "            cur_target = order[-1]\n",
    "            \n",
    "            model = ContinualLearningModel(\n",
    "                model_params=self.model_params,\n",
    "                dataset_val=self.dataset_val,\n",
    "                val_embeddings=self.val_embeddings,\n",
    "                model_class=self.model_class,\n",
    "                dataset_train=self.dataset_train,\n",
    "                train_embeddings=self.train_embeddings,\n",
    "                language_order=order,\n",
    "                learning_rate=self.learning_rate,\n",
    "                target=cur_target,\n",
    "                device=self.device,\n",
    "                show_progress=False\n",
    "            )\n",
    "            \n",
    "            trained_model = model.train()\n",
    "            evaluator = MultiHeadEvaluator(device=self.device)\n",
    "            val_emb_tensor = torch.tensor(self.val_embeddings, dtype=torch.float32).to(self.device)\n",
    "            results = evaluator.evaluate(trained_model, val_emb_tensor, self.dataset_val, show_results=False)\n",
    "            \n",
    "            self.models.append((trained_model, results['best_fine_f1']))\n",
    "            \n",
    "            print(f\"Model with order {order} achieved Fine-F1: {results['best_fine_f1']:.3f}\")\n",
    "            \n",
    "        self.models.sort(key=lambda x: x[1], reverse=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, embeddings):\n",
    "        all_narr_probs = []\n",
    "        all_sub_probs_dicts = []\n",
    "        scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model, score in self.models:\n",
    "                narr_probs, sub_probs_dict = model(embeddings)\n",
    "                all_narr_probs.append(narr_probs)\n",
    "                all_sub_probs_dicts.append(sub_probs_dict)\n",
    "                scores.append(score)\n",
    "        \n",
    "        weights = torch.tensor(scores)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        weighted_narr_probs = torch.zeros_like(all_narr_probs[0])\n",
    "        for w, probs in zip(weights, all_narr_probs):\n",
    "            weighted_narr_probs += w * probs\n",
    "        \n",
    "        weighted_sub_probs_dict = {}\n",
    "        for key in all_sub_probs_dicts[0].keys():\n",
    "            sub_probs_stack = torch.stack([d[key] for d in all_sub_probs_dicts])\n",
    "            weighted_sub_probs = torch.zeros_like(sub_probs_stack[0])\n",
    "            for w, probs in zip(weights, sub_probs_stack):\n",
    "                weighted_sub_probs += w * probs\n",
    "            weighted_sub_probs_dict[key] = weighted_sub_probs\n",
    "        \n",
    "        return weighted_narr_probs, weighted_sub_probs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a052c3-dd3e-40fd-9eb5-a0550a7d5a62",
   "metadata": {},
   "source": [
    "We select the first order is the order that achieved the best validation performance, while the remaining orders are variations of the first one that also keep certain closely related languages together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48de93ef-ea97-4ef4-87a7-13aaab546753",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_orders = [\n",
    "    ['RU', 'BG', 'PT', 'HI', 'EN'],  # Best performing\n",
    "    ['RU', 'BG', 'HI', 'PT', 'EN'],  # Second best\n",
    "    ['BG', 'RU', 'PT', 'HI', 'EN'],  # Starting with slavic\n",
    "    ['HI', 'PT', 'RU', 'BG', 'EN'],  # Variant\n",
    "    ['PT', 'HI', 'RU', 'BG', 'EN'],  # Variant\n",
    "]\n",
    "\n",
    "en_cl_model_ensemble = ContinualLearningEnsemble(\n",
    "    model_params=model_params,\n",
    "    language_orders=language_orders,\n",
    "    dataset_val=dataset_val_en,\n",
    "    val_embeddings=val_embeddings_en,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91f132-a975-4a6b-91d9-f17554b0c770",
   "metadata": {},
   "source": [
    "In the best performing orders for english, both Russian and Bulgarian come together before they reach English.\n",
    "\n",
    "```\n",
    "['RU', 'BG', 'PT', 'HI', 'EN']: 0.382\n",
    "['RU', 'BG', 'HI', 'PT', 'EN']: 0.356\n",
    "```\n",
    "\n",
    "Simply swapping the starting Slavic language shows impact:\n",
    "```\n",
    "['BG', 'RU', 'PT', 'HI', 'EN']: 0.314\n",
    "```\n",
    "\n",
    "This could mean that the starting language in a continual learning can shape how the model build it's representantions. For isntance Russian might have certain features that make it a a strong foundational model.\n",
    "\n",
    "But, that might not mean that lingustuic patterns surely helped the model, other factors can also influence the models transfer knowledge.\n",
    "\n",
    "Since RU and BG have proven to help benefit English in some way, learning them later seems to reduce the impact of those shared patterns on the final model. That could be because learning too less-related languages with not much significant patterns for English, whatever patterns that may be, can lead the model's params to a different direction making it hard to then learn from RU/BG.\n",
    "\n",
    "```\n",
    "['HI', 'PT', 'RU', 'BG', 'EN']: 0.302\n",
    "['PT', 'HI', 'RU', 'BG', 'EN']: 0.300\n",
    "```\n",
    "\n",
    "And this make sense, because by the time we have reached RU and BG, the model has already adapted to two less-related languages, so integrating RU and BG, doesn't integrate as strongly as it would if they were learned as foundational step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c811e5c-8f60-4483-bac2-91f08a4ed06c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with order: ['RU', 'BG', 'PT', 'HI', 'EN']\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Model with order ['RU', 'BG', 'PT', 'HI', 'EN'] achieved Fine-F1: 0.327\n",
      "\n",
      "Training model with order: ['RU', 'BG', 'HI', 'PT', 'EN']\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Model with order ['RU', 'BG', 'HI', 'PT', 'EN'] achieved Fine-F1: 0.355\n",
      "\n",
      "Training model with order: ['BG', 'RU', 'PT', 'HI', 'EN']\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Model with order ['BG', 'RU', 'PT', 'HI', 'EN'] achieved Fine-F1: 0.341\n",
      "\n",
      "Training model with order: ['HI', 'PT', 'RU', 'BG', 'EN']\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Model with order ['HI', 'PT', 'RU', 'BG', 'EN'] achieved Fine-F1: 0.279\n",
      "\n",
      "Training model with order: ['PT', 'HI', 'RU', 'BG', 'EN']\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "Focusing on EN\n",
      "Model with order ['PT', 'HI', 'RU', 'BG', 'EN'] achieved Fine-F1: 0.323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ContinualLearningEnsemble at 0x17ed20ad0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_cl_model_ensemble.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3382752e-7aff-4b56-8a06-4facd47f8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultiHeadEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "085324b7-9663-42a9-9f3b-3c32ca4ec170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(\n",
    "    ensemble_model,\n",
    "    embeddings,\n",
    "    dataset,\n",
    "    base_evaluator=evaluator,\n",
    "    save=False,\n",
    "):\n",
    "    def model_wrapper(x):\n",
    "        return ensemble_model.predict(x)\n",
    "    \n",
    "    emb_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)    \n",
    "    results = base_evaluator.evaluate(\n",
    "        model=model_wrapper,\n",
    "        embeddings=emb_tensor,\n",
    "        dataset=dataset,\n",
    "        save=save,\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "557162db-d1b6-408a-86b0-98a0a27e4fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.510\n",
      "F1 st. dev. coarse: 0.351\n",
      "Fine-F1: 0.348\n",
      "F1 st. dev. fine: 0.341\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.127\n",
      "Recall: 0.296\n",
      "F1 Samples: 0.348\n"
     ]
    }
   ],
   "source": [
    "results_ensemble_en = evaluate_ensemble(\n",
    "    ensemble_model=en_cl_model_ensemble,\n",
    "    embeddings=val_embeddings_en,\n",
    "    dataset=dataset_val_en,\n",
    "    save=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fbb1f-a303-452a-b25b-09b39fbf9ecc",
   "metadata": {},
   "source": [
    "## Training a Portuguese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0d4340b-0074-4410-af5b-0142349236d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_pt, val_embeddings_pt = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == \"PT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcef0ce3-8294-4d66-b211-0a05b42634b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a3c3981-c069-4f9e-abbb-96e84ce9a5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 1024)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2a7244e-70e9-4025-ad0b-1b019ebd0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_order=['RU', 'BG', 'EN', 'HI', 'PT']\n",
    "target=language_order[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "042775fd-dc9a-4434-8254-547a2a98c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_cl_model = ContinualLearningModel(\n",
    "    model_params=model_params,\n",
    "    language_order=language_order,\n",
    "    dataset_val=dataset_val_pt,\n",
    "    val_embeddings=val_embeddings_pt,\n",
    "    target=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21cef41d-7dfd-420a-99f4-92e6451be54a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on RU data...\n",
      "Epoch 1/100, Train Loss: 0.7187, Val Loss: 0.6356\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.4220, Val Loss: 0.6161\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.2928, Val Loss: 0.5990\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.2280, Val Loss: 0.5853\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.1943, Val Loss: 0.5747\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.1714, Val Loss: 0.5662\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.1519, Val Loss: 0.5578\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.1357, Val Loss: 0.5498\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.1241, Val Loss: 0.5418\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.1133, Val Loss: 0.5339\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.1021, Val Loss: 0.5270\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.0928, Val Loss: 0.5212\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.0847, Val Loss: 0.5170\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.0766, Val Loss: 0.5138\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Train Loss: 0.0720, Val Loss: 0.5103\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Train Loss: 0.0661, Val Loss: 0.5073\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Train Loss: 0.0625, Val Loss: 0.5050\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Train Loss: 0.0578, Val Loss: 0.5035\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Train Loss: 0.0538, Val Loss: 0.5041\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Train Loss: 0.0511, Val Loss: 0.5074\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Train Loss: 0.0485, Val Loss: 0.5130\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Train Loss: 0.0458, Val Loss: 0.5220\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Train Loss: 0.0443, Val Loss: 0.5328\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Train Loss: 0.0424, Val Loss: 0.5455\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 25/100, Train Loss: 0.0413, Val Loss: 0.5556\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 26/100, Train Loss: 0.0409, Val Loss: 0.5665\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 27/100, Train Loss: 0.0394, Val Loss: 0.5784\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 28/100, Train Loss: 0.0391, Val Loss: 0.5911\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for RU\n",
      "\n",
      "Training on BG data...\n",
      "Epoch 1/100, Train Loss: 4.2003, Val Loss: 0.9797\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 2.4442, Val Loss: 0.8518\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 1.3358, Val Loss: 0.7786\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.8582, Val Loss: 0.7365\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.6582, Val Loss: 0.7150\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.5596, Val Loss: 0.7063\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.5122, Val Loss: 0.7044\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.4863, Val Loss: 0.7002\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.4731, Val Loss: 0.6876\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.4475, Val Loss: 0.6658\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.4150, Val Loss: 0.6373\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.3917, Val Loss: 0.6074\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.3726, Val Loss: 0.5810\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.3561, Val Loss: 0.5615\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Train Loss: 0.3364, Val Loss: 0.5498\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Train Loss: 0.3184, Val Loss: 0.5439\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Train Loss: 0.2993, Val Loss: 0.5395\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Train Loss: 0.2851, Val Loss: 0.5358\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Train Loss: 0.2728, Val Loss: 0.5333\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Train Loss: 0.2554, Val Loss: 0.5329\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Train Loss: 0.2453, Val Loss: 0.5352\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Train Loss: 0.2379, Val Loss: 0.5425\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Train Loss: 0.2306, Val Loss: 0.5520\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Train Loss: 0.2210, Val Loss: 0.5644\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Train Loss: 0.2129, Val Loss: 0.5778\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Train Loss: 0.2040, Val Loss: 0.5915\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 27/100, Train Loss: 0.1999, Val Loss: 0.5972\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 28/100, Train Loss: 0.1936, Val Loss: 0.6023\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 29/100, Train Loss: 0.1928, Val Loss: 0.6076\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 30/100, Train Loss: 0.1886, Val Loss: 0.6138\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for BG\n",
      "\n",
      "Training on EN data...\n",
      "Epoch 1/100, Train Loss: 1.5060, Val Loss: 0.7255\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.9904, Val Loss: 0.9835\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.7849, Val Loss: 1.2547\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.6444, Val Loss: 1.5157\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5410, Val Loss: 1.7838\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4838, Val Loss: 2.0571\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4492, Val Loss: 2.3578\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.4307, Val Loss: 2.3731\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.4203, Val Loss: 2.3493\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.4010, Val Loss: 2.3122\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.3844, Val Loss: 2.2700\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for EN\n",
      "\n",
      "Training on HI data...\n",
      "Epoch 1/100, Train Loss: 1.3572, Val Loss: 1.7628\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.7413, Val Loss: 1.4936\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.6083, Val Loss: 1.2799\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5188, Val Loss: 1.0537\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4714, Val Loss: 0.8807\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4186, Val Loss: 0.7887\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3892, Val Loss: 0.7645\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.3627, Val Loss: 0.7657\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.3394, Val Loss: 0.7728\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.3190, Val Loss: 0.8077\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.3056, Val Loss: 0.8343\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.2936, Val Loss: 0.8953\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.2826, Val Loss: 0.9749\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.2711, Val Loss: 1.0313\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 15/100, Train Loss: 0.2653, Val Loss: 1.0961\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 16/100, Train Loss: 0.2583, Val Loss: 1.1824\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 17/100, Train Loss: 0.2512, Val Loss: 1.2807\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for HI\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Epoch 1/100, Train Loss: 1.6034, Val Loss: 0.9316\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 2/100, Train Loss: 1.4348, Val Loss: 0.8779\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 3/100, Train Loss: 1.3043, Val Loss: 0.8442\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 4/100, Train Loss: 1.1945, Val Loss: 0.8251\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 5/100, Train Loss: 1.1366, Val Loss: 0.8155\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 6/100, Train Loss: 1.0623, Val Loss: 0.8108\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 7/100, Train Loss: 1.0362, Val Loss: 0.8081\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 8/100, Train Loss: 0.9910, Val Loss: 0.8043\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 9/100, Train Loss: 0.9598, Val Loss: 0.7975\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 10/100, Train Loss: 0.9309, Val Loss: 0.7887\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 11/100, Train Loss: 0.9157, Val Loss: 0.7780\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 12/100, Train Loss: 0.8818, Val Loss: 0.7660\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 13/100, Train Loss: 0.8564, Val Loss: 0.7535\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 14/100, Train Loss: 0.8445, Val Loss: 0.7410\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 15/100, Train Loss: 0.8222, Val Loss: 0.7293\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 16/100, Train Loss: 0.8032, Val Loss: 0.7191\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 17/100, Train Loss: 0.7866, Val Loss: 0.7113\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 18/100, Train Loss: 0.7719, Val Loss: 0.7055\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 19/100, Train Loss: 0.7591, Val Loss: 0.7027\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 20/100, Train Loss: 0.7393, Val Loss: 0.7016\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 21/100, Train Loss: 0.7289, Val Loss: 0.7023\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 22/100, Train Loss: 0.7131, Val Loss: 0.7043\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 23/100, Train Loss: 0.7008, Val Loss: 0.7079\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 24/100, Train Loss: 0.6892, Val Loss: 0.7129\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 25/100, Train Loss: 0.6763, Val Loss: 0.7191\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 26/100, Train Loss: 0.6685, Val Loss: 0.7269\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 27/100, Train Loss: 0.6587, Val Loss: 0.7351\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 28/100, Train Loss: 0.6476, Val Loss: 0.7436\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 29/100, Train Loss: 0.6268, Val Loss: 0.7517\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 30/100, Train Loss: 0.6193, Val Loss: 0.7422\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 31/100, Train Loss: 0.6163, Val Loss: 0.7337\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 32/100, Train Loss: 0.6089, Val Loss: 0.7264\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 33/100, Train Loss: 0.6098, Val Loss: 0.7202\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 34/100, Train Loss: 0.6025, Val Loss: 0.7147\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 35/100, Train Loss: 0.6006, Val Loss: 0.7100\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 36/100, Train Loss: 0.5967, Val Loss: 0.7059\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 37/100, Train Loss: 0.5860, Val Loss: 0.7021\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 38/100, Train Loss: 0.5769, Val Loss: 0.6984\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 39/100, Train Loss: 0.5800, Val Loss: 0.6947\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 40/100, Train Loss: 0.5724, Val Loss: 0.6912\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 41/100, Train Loss: 0.5664, Val Loss: 0.6875\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 42/100, Train Loss: 0.5693, Val Loss: 0.6840\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 43/100, Train Loss: 0.5603, Val Loss: 0.6801\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 44/100, Train Loss: 0.5559, Val Loss: 0.6766\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 45/100, Train Loss: 0.5512, Val Loss: 0.6729\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 46/100, Train Loss: 0.5488, Val Loss: 0.6692\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 47/100, Train Loss: 0.5440, Val Loss: 0.6657\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 48/100, Train Loss: 0.5357, Val Loss: 0.6624\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 49/100, Train Loss: 0.5355, Val Loss: 0.6593\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 50/100, Train Loss: 0.5283, Val Loss: 0.6566\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 51/100, Train Loss: 0.5284, Val Loss: 0.6540\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 52/100, Train Loss: 0.5244, Val Loss: 0.6514\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 53/100, Train Loss: 0.5165, Val Loss: 0.6493\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 54/100, Train Loss: 0.5127, Val Loss: 0.6477\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 55/100, Train Loss: 0.5102, Val Loss: 0.6466\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 56/100, Train Loss: 0.5078, Val Loss: 0.6461\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 57/100, Train Loss: 0.4988, Val Loss: 0.6458\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 58/100, Train Loss: 0.4913, Val Loss: 0.6459\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 59/100, Train Loss: 0.4912, Val Loss: 0.6464\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 60/100, Train Loss: 0.4911, Val Loss: 0.6476\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 61/100, Train Loss: 0.4837, Val Loss: 0.6492\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 62/100, Train Loss: 0.4782, Val Loss: 0.6511\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 63/100, Train Loss: 0.4757, Val Loss: 0.6532\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 64/100, Train Loss: 0.4735, Val Loss: 0.6552\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 65/100, Train Loss: 0.4675, Val Loss: 0.6570\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 66/100, Train Loss: 0.4589, Val Loss: 0.6590\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 67/100, Train Loss: 0.4571, Val Loss: 0.6602\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 68/100, Train Loss: 0.4563, Val Loss: 0.6615\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 69/100, Train Loss: 0.4512, Val Loss: 0.6630\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 70/100, Train Loss: 0.4471, Val Loss: 0.6644\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 71/100, Train Loss: 0.4468, Val Loss: 0.6659\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 72/100, Train Loss: 0.4449, Val Loss: 0.6675\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 73/100, Train Loss: 0.4462, Val Loss: 0.6691\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 74/100, Train Loss: 0.4423, Val Loss: 0.6707\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 75/100, Train Loss: 0.4388, Val Loss: 0.6724\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 76/100, Train Loss: 0.4360, Val Loss: 0.6733\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 77/100, Train Loss: 0.4334, Val Loss: 0.6743\n",
      "Current Learning Rate: 0.000025\n",
      "Early stopping triggered for PT\n"
     ]
    }
   ],
   "source": [
    "pt_model = pt_cl_model.train(epochs_per_language=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "55f358df-38a1-4dba-9e5c-df5519213506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.10\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.587\n",
      "F1 st. dev. coarse: 0.198\n",
      "Fine-F1: 0.408\n",
      "F1 st. dev. fine: 0.187\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.090\n",
      "Recall: 0.188\n",
      "F1 Samples: 0.408\n"
     ]
    }
   ],
   "source": [
    "_ = pt_cl_model.evaluate_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d411b3-ed40-4ac8-88e1-b1af38ac6bc5",
   "metadata": {},
   "source": [
    "We can also try shuffling the articles per language we are processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dc3318a-864d-4541-9c6b-c61201136196",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_cl_model_ord = ContinualLearningModel(\n",
    "    model_params=model_params,\n",
    "    language_order=language_order,\n",
    "    dataset_val=dataset_val_pt,\n",
    "    val_embeddings=val_embeddings_pt,\n",
    "    target=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b41257c7-6656-4011-9a3f-05f687df953f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on RU data...\n",
      "Epoch 1/100, Train Loss: 0.7147, Val Loss: 0.6324\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.4101, Val Loss: 0.6136\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.2833, Val Loss: 0.5962\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.2226, Val Loss: 0.5814\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.1880, Val Loss: 0.5701\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.1683, Val Loss: 0.5613\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.1486, Val Loss: 0.5538\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.1338, Val Loss: 0.5463\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.1207, Val Loss: 0.5387\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.1091, Val Loss: 0.5312\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.0996, Val Loss: 0.5243\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.0897, Val Loss: 0.5186\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.0832, Val Loss: 0.5142\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.0749, Val Loss: 0.5108\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Train Loss: 0.0694, Val Loss: 0.5084\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Train Loss: 0.0647, Val Loss: 0.5058\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Train Loss: 0.0597, Val Loss: 0.5041\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Train Loss: 0.0559, Val Loss: 0.5033\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Train Loss: 0.0519, Val Loss: 0.5043\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Train Loss: 0.0489, Val Loss: 0.5066\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Train Loss: 0.0466, Val Loss: 0.5102\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Train Loss: 0.0446, Val Loss: 0.5163\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Train Loss: 0.0426, Val Loss: 0.5250\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Train Loss: 0.0414, Val Loss: 0.5361\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 25/100, Train Loss: 0.0398, Val Loss: 0.5454\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 26/100, Train Loss: 0.0391, Val Loss: 0.5559\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 27/100, Train Loss: 0.0388, Val Loss: 0.5672\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 28/100, Train Loss: 0.0381, Val Loss: 0.5798\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for RU\n",
      "\n",
      "Training on BG data...\n",
      "Epoch 1/100, Train Loss: 4.2094, Val Loss: 0.9738\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 2.4197, Val Loss: 0.8417\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 1.3647, Val Loss: 0.7712\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.8757, Val Loss: 0.7314\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.6611, Val Loss: 0.7115\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.5671, Val Loss: 0.7039\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.5211, Val Loss: 0.7009\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.4964, Val Loss: 0.6946\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.4773, Val Loss: 0.6814\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.4521, Val Loss: 0.6581\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.4233, Val Loss: 0.6285\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.4005, Val Loss: 0.5965\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.3819, Val Loss: 0.5683\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.3588, Val Loss: 0.5486\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Train Loss: 0.3396, Val Loss: 0.5362\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Train Loss: 0.3201, Val Loss: 0.5278\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Train Loss: 0.3051, Val Loss: 0.5226\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Train Loss: 0.2883, Val Loss: 0.5191\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Train Loss: 0.2746, Val Loss: 0.5163\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Train Loss: 0.2632, Val Loss: 0.5150\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Train Loss: 0.2507, Val Loss: 0.5180\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Train Loss: 0.2423, Val Loss: 0.5253\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Train Loss: 0.2319, Val Loss: 0.5359\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Train Loss: 0.2215, Val Loss: 0.5479\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Train Loss: 0.2150, Val Loss: 0.5571\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Train Loss: 0.2077, Val Loss: 0.5644\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 27/100, Train Loss: 0.1979, Val Loss: 0.5656\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 28/100, Train Loss: 0.1982, Val Loss: 0.5658\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 29/100, Train Loss: 0.1929, Val Loss: 0.5649\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 30/100, Train Loss: 0.1902, Val Loss: 0.5642\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for BG\n",
      "\n",
      "Training on EN data...\n",
      "Epoch 1/100, Train Loss: 1.5092, Val Loss: 0.6933\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 1.0081, Val Loss: 0.9606\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.7942, Val Loss: 1.2504\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.6604, Val Loss: 1.5289\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5610, Val Loss: 1.7976\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4832, Val Loss: 2.0901\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4492, Val Loss: 2.4378\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.4277, Val Loss: 2.4568\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.4102, Val Loss: 2.4371\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.3924, Val Loss: 2.3881\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.3749, Val Loss: 2.3252\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for EN\n",
      "\n",
      "Training on HI data...\n",
      "Epoch 1/100, Train Loss: 1.2197, Val Loss: 1.6652\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.7501, Val Loss: 1.4119\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.5856, Val Loss: 1.1949\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5222, Val Loss: 1.0113\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4627, Val Loss: 0.8709\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4210, Val Loss: 0.7811\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3908, Val Loss: 0.7461\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.3611, Val Loss: 0.7283\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.3341, Val Loss: 0.7317\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.3199, Val Loss: 0.7551\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.3045, Val Loss: 0.8162\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.2934, Val Loss: 0.9140\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.2882, Val Loss: 0.9622\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.2722, Val Loss: 1.0363\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 15/100, Train Loss: 0.2636, Val Loss: 1.0723\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 16/100, Train Loss: 0.2592, Val Loss: 1.1217\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 17/100, Train Loss: 0.2529, Val Loss: 1.1847\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 18/100, Train Loss: 0.2495, Val Loss: 1.2568\n",
      "Current Learning Rate: 0.000500\n",
      "Early stopping triggered for HI\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Epoch 1/100, Train Loss: 1.6945, Val Loss: 0.9213\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 2/100, Train Loss: 1.5084, Val Loss: 0.8685\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 3/100, Train Loss: 1.3578, Val Loss: 0.8352\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 4/100, Train Loss: 1.2495, Val Loss: 0.8163\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 5/100, Train Loss: 1.1690, Val Loss: 0.8069\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 6/100, Train Loss: 1.1000, Val Loss: 0.8036\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 7/100, Train Loss: 1.0538, Val Loss: 0.8025\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 8/100, Train Loss: 1.0110, Val Loss: 0.8017\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 9/100, Train Loss: 0.9818, Val Loss: 0.7995\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 10/100, Train Loss: 0.9427, Val Loss: 0.7954\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 11/100, Train Loss: 0.9271, Val Loss: 0.7887\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 12/100, Train Loss: 0.8937, Val Loss: 0.7806\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 13/100, Train Loss: 0.8810, Val Loss: 0.7712\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 14/100, Train Loss: 0.8581, Val Loss: 0.7617\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 15/100, Train Loss: 0.8459, Val Loss: 0.7526\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 16/100, Train Loss: 0.8191, Val Loss: 0.7440\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 17/100, Train Loss: 0.8017, Val Loss: 0.7366\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 18/100, Train Loss: 0.7868, Val Loss: 0.7307\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 19/100, Train Loss: 0.7741, Val Loss: 0.7259\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 20/100, Train Loss: 0.7560, Val Loss: 0.7219\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 21/100, Train Loss: 0.7487, Val Loss: 0.7193\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 22/100, Train Loss: 0.7289, Val Loss: 0.7170\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 23/100, Train Loss: 0.7181, Val Loss: 0.7161\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 24/100, Train Loss: 0.7048, Val Loss: 0.7158\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 25/100, Train Loss: 0.6927, Val Loss: 0.7163\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 26/100, Train Loss: 0.6795, Val Loss: 0.7189\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 27/100, Train Loss: 0.6679, Val Loss: 0.7233\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 28/100, Train Loss: 0.6610, Val Loss: 0.7289\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 29/100, Train Loss: 0.6416, Val Loss: 0.7352\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 30/100, Train Loss: 0.6311, Val Loss: 0.7428\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 31/100, Train Loss: 0.6245, Val Loss: 0.7512\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 32/100, Train Loss: 0.6148, Val Loss: 0.7587\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 33/100, Train Loss: 0.6029, Val Loss: 0.7657\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 34/100, Train Loss: 0.5963, Val Loss: 0.7559\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 35/100, Train Loss: 0.5986, Val Loss: 0.7468\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 36/100, Train Loss: 0.5880, Val Loss: 0.7387\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 37/100, Train Loss: 0.5823, Val Loss: 0.7314\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 38/100, Train Loss: 0.5757, Val Loss: 0.7245\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 39/100, Train Loss: 0.5743, Val Loss: 0.7182\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 40/100, Train Loss: 0.5722, Val Loss: 0.7121\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 41/100, Train Loss: 0.5663, Val Loss: 0.7062\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 42/100, Train Loss: 0.5659, Val Loss: 0.7005\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 43/100, Train Loss: 0.5543, Val Loss: 0.6951\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 44/100, Train Loss: 0.5552, Val Loss: 0.6898\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 45/100, Train Loss: 0.5460, Val Loss: 0.6851\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 46/100, Train Loss: 0.5467, Val Loss: 0.6809\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 47/100, Train Loss: 0.5462, Val Loss: 0.6770\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 48/100, Train Loss: 0.5383, Val Loss: 0.6734\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 49/100, Train Loss: 0.5384, Val Loss: 0.6702\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 50/100, Train Loss: 0.5332, Val Loss: 0.6674\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 51/100, Train Loss: 0.5256, Val Loss: 0.6649\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 52/100, Train Loss: 0.5255, Val Loss: 0.6626\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 53/100, Train Loss: 0.5259, Val Loss: 0.6604\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 54/100, Train Loss: 0.5171, Val Loss: 0.6583\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 55/100, Train Loss: 0.5203, Val Loss: 0.6566\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 56/100, Train Loss: 0.5103, Val Loss: 0.6552\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 57/100, Train Loss: 0.5094, Val Loss: 0.6540\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 58/100, Train Loss: 0.5092, Val Loss: 0.6529\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 59/100, Train Loss: 0.5038, Val Loss: 0.6523\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 60/100, Train Loss: 0.5035, Val Loss: 0.6520\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 61/100, Train Loss: 0.4974, Val Loss: 0.6523\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 62/100, Train Loss: 0.4929, Val Loss: 0.6533\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 63/100, Train Loss: 0.4980, Val Loss: 0.6546\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 64/100, Train Loss: 0.4932, Val Loss: 0.6565\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 65/100, Train Loss: 0.4866, Val Loss: 0.6582\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 66/100, Train Loss: 0.4873, Val Loss: 0.6600\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 67/100, Train Loss: 0.4865, Val Loss: 0.6621\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 68/100, Train Loss: 0.4817, Val Loss: 0.6647\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 69/100, Train Loss: 0.4834, Val Loss: 0.6670\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 70/100, Train Loss: 0.4760, Val Loss: 0.6683\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 71/100, Train Loss: 0.4766, Val Loss: 0.6698\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 72/100, Train Loss: 0.4764, Val Loss: 0.6710\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 73/100, Train Loss: 0.4752, Val Loss: 0.6724\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 74/100, Train Loss: 0.4719, Val Loss: 0.6736\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 75/100, Train Loss: 0.4708, Val Loss: 0.6750\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 76/100, Train Loss: 0.4724, Val Loss: 0.6764\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 77/100, Train Loss: 0.4737, Val Loss: 0.6780\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 78/100, Train Loss: 0.4727, Val Loss: 0.6794\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 79/100, Train Loss: 0.4657, Val Loss: 0.6802\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 80/100, Train Loss: 0.4697, Val Loss: 0.6811\n",
      "Current Learning Rate: 0.000025\n",
      "Early stopping triggered for PT\n"
     ]
    }
   ],
   "source": [
    "pt_model_ord = pt_cl_model_ord.train(epochs_per_language=100, patience=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc78a7-e400-4414-a906-9c13b13a454a",
   "metadata": {},
   "source": [
    "Which is not suprising, simply shuffling the examples within each language does not fundamentally change the sequence in which the model sees languages themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db89ac05-7a3b-48e9-9d28-89d004b271f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.10\n",
      "Subnarrative threshold: 0.55\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.577\n",
      "F1 st. dev. coarse: 0.194\n",
      "Fine-F1: 0.413\n",
      "F1 st. dev. fine: 0.229\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.093\n",
      "Recall: 0.186\n",
      "F1 Samples: 0.413\n"
     ]
    }
   ],
   "source": [
    "_ = pt_cl_model_ord.evaluate_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25935a-6bdb-455e-892f-d2c089756179",
   "metadata": {},
   "source": [
    "The two highest scores come from sequences that begin with English:\n",
    "\n",
    "```\n",
    "['EN', 'BG', 'RU', 'HI', 'PT']: 0.420\n",
    "['EN', 'RU', 'BG', 'HI', 'PT']: 0.409\n",
    "```\n",
    "\n",
    "Then we follow this with slavic languages together (BG, RU), meaning that getting English in early stages can help build a strong foundation for Portuguese.\n",
    "\n",
    "When Russian and Bulgarian appear directly after English, performance tends to be better, that might mean that it's more beneficial getting English represantations established before moving to RU/BG.\n",
    "\n",
    "A suprising sequence that places Hindi first, and doesn't necessarily follow language-family closeness, still does okay. This highiligthts that it's not purely about language's that belong to the same family being close together, but just that Hindi may just helped the Portuguese language.\n",
    "*  So, other factors beyond simple language-family-similarity can also impact the model's performance from the language order, such as specific narrative and subnarratives present in each language, that is, if two langauges share ceratin kinds of narratives more than others, that can also help a lot in a continual learning setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d53dc38-81d2-4a53-8f21-c999f675dfde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with order: ['RU', 'BG', 'EN', 'HI', 'PT']\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Model with order ['RU', 'BG', 'EN', 'HI', 'PT'] achieved Fine-F1: 0.397\n",
      "\n",
      "Training model with order: ['RU', 'BG', 'HI', 'EN', 'PT']\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Model with order ['RU', 'BG', 'HI', 'EN', 'PT'] achieved Fine-F1: 0.378\n",
      "\n",
      "Training model with order: ['HI', 'BG', 'EN', 'RU', 'PT']\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Model with order ['HI', 'BG', 'EN', 'RU', 'PT'] achieved Fine-F1: 0.411\n",
      "\n",
      "Training model with order: ['EN', 'BG', 'RU', 'HI', 'PT']\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Model with order ['EN', 'BG', 'RU', 'HI', 'PT'] achieved Fine-F1: 0.400\n",
      "\n",
      "Training model with order: ['EN', 'RU', 'BG', 'HI', 'PT']\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "Focusing on PT\n",
      "Model with order ['EN', 'RU', 'BG', 'HI', 'PT'] achieved Fine-F1: 0.404\n"
     ]
    }
   ],
   "source": [
    "language_orders = [\n",
    "    ['RU', 'BG', 'EN', 'HI', 'PT'],  # Slavic first, then English for more pattern capturing\n",
    "    ['RU', 'BG', 'HI', 'EN', 'PT'],  # Slavic first, then Hindi\n",
    "    ['HI', 'BG', 'EN', 'RU', 'PT'],  # Hindi first\n",
    "    ['EN', 'BG', 'RU', 'HI', 'PT'],  # English first then Slavic\n",
    "    ['EN', 'RU', 'BG', 'HI', 'PT'],  # Variant\n",
    "]\n",
    "\n",
    "pt_cl_model_ensemble = ContinualLearningEnsemble(\n",
    "    model_params=model_params,\n",
    "    language_orders=language_orders,\n",
    "    dataset_val=dataset_val_pt,\n",
    "    val_embeddings=val_embeddings_pt,\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e63caee-ac71-421b-87cc-9c2a76d5ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.10\n",
      "Subnarrative threshold: 0.55\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.567\n",
      "F1 st. dev. coarse: 0.198\n",
      "Fine-F1: 0.407\n",
      "F1 st. dev. fine: 0.211\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.090\n",
      "Recall: 0.185\n",
      "F1 Samples: 0.407\n"
     ]
    }
   ],
   "source": [
    "results_ensemble_pt = evaluate_ensemble(\n",
    "    ensemble_model=pt_cl_model_ensemble,\n",
    "    embeddings=val_embeddings_pt,\n",
    "    dataset=dataset_val_pt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50678a6d-ac64-475f-b0b9-476a001fe6f4",
   "metadata": {},
   "source": [
    "We can follow the same approach for the other languages we have for submission.\n",
    "\n",
    "We will pick several orders, grouping related languages and select other variants based on those. Because we use an ensembler and a final voting scheme, this can ensure that we will capture both linguistic patterns if applicable, or just specific overlaps that improve performance in the continual learning setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9831b-8520-49d1-8c50-254602c0c389",
   "metadata": {},
   "source": [
    "## Training a Hindi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66f6cc5f-fa92-4eb0-beae-79108a4035fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_hi, val_embeddings_hi = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == \"HI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ac80095-fd44-4bee-90b5-5b6374d4e564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_hi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79650602-c9ea-4330-a4a2-ad266597a120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 1024)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings_hi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c882a3b7-49ab-4368-a423-1508cfb43c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with order: ['BG', 'RU', 'EN', 'PT', 'HI']\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "Focusing on HI\n",
      "Model with order ['BG', 'RU', 'EN', 'PT', 'HI'] achieved Fine-F1: 0.286\n",
      "\n",
      "Training model with order: ['RU', 'BG', 'EN', 'PT', 'HI']\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "Focusing on HI\n",
      "Model with order ['RU', 'BG', 'EN', 'PT', 'HI'] achieved Fine-F1: 0.341\n",
      "\n",
      "Training model with order: ['BG', 'RU', 'PT', 'EN', 'HI']\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on HI data...\n",
      "Focusing on HI\n",
      "Model with order ['BG', 'RU', 'PT', 'EN', 'HI'] achieved Fine-F1: 0.277\n",
      "\n",
      "Training model with order: ['PT', 'EN', 'BG', 'RU', 'HI']\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on HI data...\n",
      "Focusing on HI\n",
      "Model with order ['PT', 'EN', 'BG', 'RU', 'HI'] achieved Fine-F1: 0.344\n",
      "\n",
      "Training model with order: ['EN', 'PT', 'RU', 'BG', 'HI']\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on HI data...\n",
      "Focusing on HI\n",
      "Model with order ['EN', 'PT', 'RU', 'BG', 'HI'] achieved Fine-F1: 0.319\n"
     ]
    }
   ],
   "source": [
    "language_orders = [\n",
    "    ['BG', 'RU', 'EN', 'PT', 'HI'],  # Best order\n",
    "    ['RU', 'BG', 'EN', 'PT', 'HI'],  # Slavic languages together first\n",
    "    ['BG', 'RU', 'PT', 'EN', 'HI'],  # Variant ^\n",
    "    ['PT', 'EN', 'BG', 'RU', 'HI'],  # Slaving languages close together end\n",
    "    ['EN', 'PT', 'RU', 'BG', 'HI']   # Alternative with Western first\n",
    "]\n",
    "\n",
    "hi_cl_model_ensemble = ContinualLearningEnsemble(\n",
    "    model_params=model_params,\n",
    "    language_orders=language_orders,\n",
    "    dataset_val=dataset_val_hi,\n",
    "    val_embeddings=val_embeddings_hi,\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5ec7ab3-c776-4a55-99f7-290e7491ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.45\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.498\n",
      "F1 st. dev. coarse: 0.325\n",
      "Fine-F1: 0.327\n",
      "F1 st. dev. fine: 0.294\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.092\n",
      "Recall: 0.145\n",
      "F1 Samples: 0.327\n"
     ]
    }
   ],
   "source": [
    "results_ensemble_hi = evaluate_ensemble(\n",
    "    ensemble_model=hi_cl_model_ensemble,\n",
    "    embeddings=val_embeddings_hi,\n",
    "    dataset=dataset_val_hi,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d092cc-049e-4d57-82ac-c46520fee185",
   "metadata": {},
   "source": [
    "## Training a Bulgarian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "274d8f11-80c8-436a-9c36-9b9782d273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_bg, val_embeddings_bg = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == \"BG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5f11373-0ac7-47d0-bb7e-80a7c149f6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with order: ['HI', 'PT', 'RU', 'EN', 'BG']\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on BG data...\n",
      "Focusing on BG\n",
      "Model with order ['HI', 'PT', 'RU', 'EN', 'BG'] achieved Fine-F1: 0.360\n",
      "\n",
      "Training model with order: ['HI', 'PT', 'EN', 'RU', 'BG']\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "Focusing on BG\n",
      "Model with order ['HI', 'PT', 'EN', 'RU', 'BG'] achieved Fine-F1: 0.354\n",
      "\n",
      "Training model with order: ['PT', 'HI', 'EN', 'RU', 'BG']\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "Focusing on BG\n",
      "Model with order ['PT', 'HI', 'EN', 'RU', 'BG'] achieved Fine-F1: 0.378\n",
      "\n",
      "Training model with order: ['EN', 'PT', 'HI', 'RU', 'BG']\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "Focusing on BG\n",
      "Model with order ['EN', 'PT', 'HI', 'RU', 'BG'] achieved Fine-F1: 0.240\n",
      "\n",
      "Training model with order: ['PT', 'EN', 'HI', 'RU', 'BG']\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on RU data...\n",
      "\n",
      "Training on BG data...\n",
      "Focusing on BG\n",
      "Model with order ['PT', 'EN', 'HI', 'RU', 'BG'] achieved Fine-F1: 0.332\n"
     ]
    }
   ],
   "source": [
    "language_orders = [\n",
    "    ['HI', 'PT', 'RU', 'EN', 'BG'],  # Best order\n",
    "    ['HI', 'PT', 'EN', 'RU', 'BG'],  # RU closer to BG\n",
    "    ['PT', 'HI', 'EN', 'RU', 'BG'],  # Small Variant\n",
    "    ['EN', 'PT', 'HI', 'RU', 'BG'],  # English first\n",
    "    ['PT', 'EN', 'HI', 'RU', 'BG']   # Another variant\n",
    "]\n",
    "\n",
    "bg_cl_model_ensemble = ContinualLearningEnsemble(\n",
    "    model_params=model_params,\n",
    "    language_orders=language_orders,\n",
    "    dataset_val=dataset_val_bg,\n",
    "    val_embeddings=val_embeddings_bg,\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e50eb31a-7e87-48a1-8282-48dc140131cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.590\n",
      "F1 st. dev. coarse: 0.388\n",
      "Fine-F1: 0.407\n",
      "F1 st. dev. fine: 0.322\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.100\n",
      "Recall: 0.183\n",
      "F1 Samples: 0.407\n"
     ]
    }
   ],
   "source": [
    "results_ensemble_bg = evaluate_ensemble(\n",
    "    ensemble_model=bg_cl_model_ensemble,\n",
    "    embeddings=val_embeddings_bg,\n",
    "    dataset=dataset_val_bg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2bede-cad3-409f-b830-38f9939fa559",
   "metadata": {},
   "source": [
    "## Training a Russian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1decdc0-5ba5-4e96-adc0-8df9b5dabd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_ru, val_embeddings_ru = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == \"RU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01119e4f-71e2-4bec-80bf-c8801d88f4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with order: ['HI', 'PT', 'BG', 'EN', 'RU']\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on RU data...\n",
      "Focusing on RU\n",
      "Model with order ['HI', 'PT', 'BG', 'EN', 'RU'] achieved Fine-F1: 0.292\n",
      "\n",
      "Training model with order: ['HI', 'EN', 'BG', 'PT', 'RU']\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on RU data...\n",
      "Focusing on RU\n",
      "Model with order ['HI', 'EN', 'BG', 'PT', 'RU'] achieved Fine-F1: 0.330\n",
      "\n",
      "Training model with order: ['PT', 'HI', 'EN', 'BG', 'RU']\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "Focusing on RU\n",
      "Model with order ['PT', 'HI', 'EN', 'BG', 'RU'] achieved Fine-F1: 0.256\n",
      "\n",
      "Training model with order: ['BG', 'EN', 'PT', 'HI', 'RU']\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on RU data...\n",
      "Focusing on RU\n",
      "Model with order ['BG', 'EN', 'PT', 'HI', 'RU'] achieved Fine-F1: 0.229\n",
      "\n",
      "Training model with order: ['EN', 'PT', 'BG', 'HI', 'RU']\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on RU data...\n",
      "Focusing on RU\n",
      "Model with order ['EN', 'PT', 'BG', 'HI', 'RU'] achieved Fine-F1: 0.266\n",
      "\n",
      "Training model with order: ['PT', 'EN', 'HI', 'BG', 'RU']\n",
      "\n",
      "Training on PT data...\n",
      "\n",
      "Training on EN data...\n",
      "\n",
      "Training on HI data...\n",
      "\n",
      "Training on BG data...\n",
      "\n",
      "Training on RU data...\n",
      "Focusing on RU\n",
      "Model with order ['PT', 'EN', 'HI', 'BG', 'RU'] achieved Fine-F1: 0.291\n"
     ]
    }
   ],
   "source": [
    "language_orders = [\n",
    "    ['HI', 'PT', 'BG', 'EN', 'RU'],\n",
    "    ['HI', 'EN', 'BG', 'PT', 'RU'],\n",
    "    ['PT', 'HI', 'EN', 'BG', 'RU'],\n",
    "    ['BG', 'EN', 'PT', 'HI', 'RU'],\n",
    "    ['EN', 'PT', 'BG', 'HI', 'RU'],\n",
    "    ['PT', 'EN', 'HI', 'BG', 'RU'] \n",
    "]\n",
    "\n",
    "ru_cl_model_ensemble = ContinualLearningEnsemble(\n",
    "    model_params=model_params,\n",
    "    language_orders=language_orders,\n",
    "    dataset_val=dataset_val_ru,\n",
    "    val_embeddings=val_embeddings_ru,\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "247b9379-f740-4142-927b-5169461ffabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.30\n",
      "Subnarrative threshold: 0.50\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.481\n",
      "F1 st. dev. coarse: 0.270\n",
      "Fine-F1: 0.292\n",
      "F1 st. dev. fine: 0.222\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.078\n",
      "Recall: 0.124\n",
      "F1 Samples: 0.292\n"
     ]
    }
   ],
   "source": [
    "results_ensemble_ru = evaluate_ensemble(\n",
    "    ensemble_model=ru_cl_model_ensemble,\n",
    "    embeddings=val_embeddings_ru,\n",
    "    dataset=dataset_val_ru,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
