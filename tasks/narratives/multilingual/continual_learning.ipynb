{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d99e5d6-7e11-492f-a64d-b37cf0c45205",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c39088-d95e-477c-89d0-1fdeb22ee015",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3326e463-8379-49cd-b744-64ac197b172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "if random_state:\n",
    "    print('[WARNING] Setting random state')\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state) \n",
    "    random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786310c-1d16-4caa-aa50-25de4385bf9c",
   "metadata": {},
   "source": [
    "## Continual Learning\n",
    "\n",
    "As of current, we were using all multilingual training data (Russian, Bulgarian, Portuguese, Hindi, and English) at once, mixing it together during training, and then evaluating specifically on English validation data. However, since our final evaluation is language-target based we can leverage a sequential training of langauges (Russian -> Bulgarian -> Portuguese -> Hindi -> English) in order to aim for better results.\n",
    "\n",
    "This is also similar to how we as humans might learn languages. We start with one then move to another one while maintaining knowledge of the past ones.\n",
    "\n",
    "This way might help our learning on identifying different useful patterns per language that could later help a specific language classification. For example:\n",
    "* Russian articles might help learn certain propaganda patterns.\n",
    "* Bulgarian articles might contribute different narrative structures.\n",
    "* Each language adds its own unique perspective to the model's understanding, the model get's this knoweledge sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59caa7a8-d3fd-49e6-9577-ab31af120073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"../../\"\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3471b8e-6a33-4304-842b-0f33d486a2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...  \n",
       "1  [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd314e8-e067-4f8e-8e9a-55c7bd31ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16de16f1-f5a2-4d0b-9163-a0309370c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives_map.pkl'), 'rb') as f:\n",
    "    narrative_to_sub_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a297e8c-ed56-4a0d-a4b7-357b1a874a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'coarse_classes.pkl'), 'rb') as f:\n",
    "    coarse_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'fine_classes.pkl'), 'rb') as f:\n",
    "    fine_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_order.pkl'), 'rb') as f:\n",
    "    narrative_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41a87fa-30be-4c8f-9361-bbf4051b5a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6c8d29-b38d-4cbf-b786-6ceb77727fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22dda616-22e7-4809-9745-68dea421e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_train_stella.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "train_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e19b5a1-1aa9-4fa5-8122-052f158aae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c624ed2-a874-4168-843a-2aa49d87858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65251811-2cf0-4875-9293-c6746eefe0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6760b44-5c00-444d-945a-17deb00626d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1014.txt</td>\n",
       "      <td>&lt;PARA&gt;алаудинов: российские силы растянули и р...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Praise of Russian military might]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1174.txt</td>\n",
       "      <td>&lt;PARA&gt;других сценариев нет. никаких переговоро...</td>\n",
       "      <td>[URW: Speculating war outcomes, URW: Discredit...</td>\n",
       "      <td>[Ukrainian army is collapsing, Discrediting Uk...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1166.txt</td>\n",
       "      <td>&lt;PARA&gt;попытка запада изолировать путина провал...</td>\n",
       "      <td>[URW: Praise of Russia, URW: Distrust towards ...</td>\n",
       "      <td>[Praise of Russian President Vladimir Putin, W...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1170.txt</td>\n",
       "      <td>&lt;PARA&gt;часть территории украины войдет в состав...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Speculating w...</td>\n",
       "      <td>[Discrediting Ukrainian government and officia...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1004.txt</td>\n",
       "      <td>&lt;PARA&gt;зеленскому не очень понравилась идея о в...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Discrediting Ukrainian government and officia...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1014.txt   \n",
       "1       RU  RU-URW-1174.txt   \n",
       "2       RU  RU-URW-1166.txt   \n",
       "3       RU  RU-URW-1170.txt   \n",
       "4       RU  RU-URW-1004.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>алаудинов: российские силы растянули и р...   \n",
       "1  <PARA>других сценариев нет. никаких переговоро...   \n",
       "2  <PARA>попытка запада изолировать путина провал...   \n",
       "3  <PARA>часть территории украины войдет в состав...   \n",
       "4  <PARA>зеленскому не очень понравилась идея о в...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0                            [URW: Praise of Russia]   \n",
       "1  [URW: Speculating war outcomes, URW: Discredit...   \n",
       "2  [URW: Praise of Russia, URW: Distrust towards ...   \n",
       "3  [URW: Discrediting Ukraine, URW: Speculating w...   \n",
       "4  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0                 [Praise of Russian military might]   \n",
       "1  [Ukrainian army is collapsing, Discrediting Uk...   \n",
       "2  [Praise of Russian President Vladimir Putin, W...   \n",
       "3  [Discrediting Ukrainian government and officia...   \n",
       "4  [Discrediting Ukrainian government and officia...   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "1  [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a538dc67-c1e6-4e44-8d0e-18dec04c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_dev_stella.npy')\n",
    "\n",
    "val_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f850e1-f926-4ab6-9f7a-0f67b9389e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_and_embeddings(dataset, embeddings, condition_fn):\n",
    "    filtered_indices = dataset.index[dataset.apply(condition_fn, axis=1)].tolist()\n",
    "    \n",
    "    filtered_dataset = dataset.loc[filtered_indices]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "\n",
    "    return filtered_dataset, filtered_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034f315c-406e-4a6b-a7bc-2aa75f5c3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, val_embeddings = filter_dataset_and_embeddings(\n",
    "        dataset_val, val_embeddings, lambda row: row[\"language\"] == \"EN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc926b5-3f23-43df-93c5-89520c9e7aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5033bbd-0be9-4e77-a6ec-b197dee8ce06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e9db19b-9cad-4baf-9112-7a48d6b38783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1898d2-d812-40cd-b591-684f068e1044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd2c347-8260-418c-b970-299f3a2c8b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...\n",
       "1       [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...\n",
       "2       [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "3       [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "4       [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "                              ...                        \n",
       "1776    [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "1777    [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...\n",
       "1778    [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...\n",
       "1779    [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "1780    [[1, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...\n",
       "Name: aggregated_subnarratives, Length: 1781, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['aggregated_subnarratives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cb9c1c0-510c-45a6-b7b9-1fd356cef6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prefer_cpu=True\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() and not prefer_cpu\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47887e9a-e6db-4ce6-9de4-41980f14d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04b31463-d7d2-462f-98b7-45019fbdb3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RU', 'PT', 'BG', 'HI', 'EN'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff0fbb62-e3c6-4a03-b511-7f80b1088354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_shuffling(data, embeddings, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    shuffled_indices = np.arange(len(data))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "    embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    return data, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73e14dc2-947d-4eed-822e-e121b5f488f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe4df1aa-83e0-497e-86ff-f2879b508a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60fbb460-43de-460f-a379-5533ba50ac4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RU', 'PT', 'BG', 'HI', 'EN'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3a89ba3-d486-42c6-99fb-0dc380c0b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.input_shape = (1, input_size)\n",
    "        self.model_name = \"base_model\" \n",
    "        \n",
    "    def visualize(self, filepath=None):\n",
    "        if filepath is None:\n",
    "            filepath = f'./visualizations/{self.model_name}.onnx'\n",
    "        \n",
    "        dummy_input = torch.randn(self.input_shape)\n",
    "        torch.onnx.export(\n",
    "            self,\n",
    "            dummy_input,\n",
    "            filepath,\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['narrative_output', 'subnarrative_outputs'],\n",
    "        )\n",
    "        \n",
    "        print(f\"Model exported to {filepath}\")\n",
    "        print(\"You can visualize this using Netron: https://netron.app/\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Forward method must be implemented by subclasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5695f79-c43c-4804-b301-f4271e1aa32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskClassifierMultiHead(BaseClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=1024,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=0.4,\n",
    "        model_name=\"MultiTaskClassifierMultiHead\" \n",
    "    ):\n",
    "        super().__init__(input_size)\n",
    "        self.model_name = model_name \n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            sub_probs_dict[narr_idx] = head(shared_out)\n",
    "\n",
    "        return narr_probs, sub_probs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f8b5f9a-bfde-4a2d-8d14-28cd2e737a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 2048,\n",
    "    'dropout': 0.4,\n",
    "    'patience': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc39239-d7a5-418c-bb41-6314338f27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "y_val_nar = dataset_val['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n",
    "y_val_sub_nar = dataset_val['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02f836fd-f58a-4e2f-b73c-f1cc1a3a29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32).to(device)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32).to(device)\n",
    "\n",
    "y_val_nar = torch.tensor(y_val_nar, dtype=torch.float32).to(device)\n",
    "y_val_sub_nar = torch.tensor(y_val_sub_nar, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "138b7fdc-b2f9-4b66-bee9-2f072a23cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb6865a1-4cd3-4a00-939f-7c0785b9f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97c597ac-d39f-4e74-9a1f-9b3652797d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_head = MultiTaskClassifierMultiHead(\n",
    "    input_size=input_size,\n",
    "    hidden_size=network_params['hidden_size'],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02aee58e-ac1c-4b67-87fa-20ffca4bd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e03513c5-f138-419a-886e-0c5e7b49feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_params = {\n",
    "    'sub_weight': 0.3,\n",
    "    'condition_weight': 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "131b5778-0dd0-43d1-a10d-c1b8b9523ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=loss_params['condition_weight'],\n",
    "                 sub_weight=loss_params['sub_weight']):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        \n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads):\n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "        \n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            \n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_loss += sub_loss_func(sub_probs, y_sub_tensor)\n",
    "            \n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.mean(\n",
    "                torch.abs(sub_probs * (1 - narr_pred)) + \n",
    "                narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            )\n",
    "            condition_loss += condition_term\n",
    "            \n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        \n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + \\\n",
    "                    self.sub_weight * sub_loss + \\\n",
    "                    self.condition_weight * condition_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99e1f6e1-f393-4f0e-8657-fe0e73f3e483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears',\n",
       " 'CC: Climate change is beneficial',\n",
       " 'CC: Controversy about green technologies',\n",
       " 'CC: Criticism of climate movement',\n",
       " 'CC: Criticism of climate policies',\n",
       " 'CC: Criticism of institutions and authorities',\n",
       " 'CC: Downplaying climate change',\n",
       " 'CC: Green policies are geopolitical instruments',\n",
       " 'CC: Hidden plots by secret schemes of powerful groups',\n",
       " 'CC: Questioning the measurements and science',\n",
       " 'Other',\n",
       " 'URW: Amplifying war-related fears',\n",
       " 'URW: Blaming the war on others rather than the invader',\n",
       " 'URW: Discrediting Ukraine',\n",
       " 'URW: Discrediting the West, Diplomacy',\n",
       " 'URW: Distrust towards Media',\n",
       " 'URW: Hidden plots by secret schemes of powerful groups',\n",
       " 'URW: Negative Consequences for the West',\n",
       " 'URW: Overpraising the West',\n",
       " 'URW: Praise of Russia',\n",
       " 'URW: Russia is the Victim',\n",
       " 'URW: Speculating war outcomes']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4890d1d-9e36-48b9-a078-b229462ce0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears: Amplifying existing fears of global warming',\n",
       " 'CC: Amplifying Climate Fears: Doomsday scenarios for humans',\n",
       " 'CC: Amplifying Climate Fears: Earth will be uninhabitable soon',\n",
       " 'CC: Amplifying Climate Fears: Other',\n",
       " 'CC: Amplifying Climate Fears: Whatever we do it is already too late',\n",
       " 'CC: Climate change is beneficial: CO2 is beneficial',\n",
       " 'CC: Climate change is beneficial: Other',\n",
       " 'CC: Climate change is beneficial: Temperature increase is beneficial',\n",
       " 'CC: Controversy about green technologies: Other',\n",
       " 'CC: Controversy about green technologies: Renewable energy is costly',\n",
       " 'CC: Controversy about green technologies: Renewable energy is dangerous',\n",
       " 'CC: Controversy about green technologies: Renewable energy is unreliable',\n",
       " 'CC: Criticism of climate movement: Ad hominem attacks on key activists',\n",
       " 'CC: Criticism of climate movement: Climate movement is alarmist',\n",
       " 'CC: Criticism of climate movement: Climate movement is corrupt']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b8a9780-236e-4b63-a7e2-2a0042425736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50c4dba6-4054-4318-b1d1-32ef72770183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class MultiHeadEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes_coarse=coarse_classes,\n",
    "        classes_fine=fine_classes,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu',\n",
    "        output_dir='../../../submissions',\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.narrative_classes = list(narrative_classes)\n",
    "        self.subnarrative_classes = list(subnarrative_classes)\n",
    "        \n",
    "        self.classes_coarse = classes_coarse\n",
    "        self.classes_fine = classes_fine\n",
    "\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings,\n",
    "        dataset,\n",
    "        thresholds=None,\n",
    "        save=False,\n",
    "        std_weight=0.6,\n",
    "        lower_thres=0.1,\n",
    "        upper_thres=0.6\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(lower_thres, upper_thres, 0.05)\n",
    "        \n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        embeddings = embeddings.to(self.device)\n",
    "    \n",
    "        best_results = {\n",
    "            'best_coarse_f1': -1,\n",
    "            'best_coarse_std': float('inf'),\n",
    "            'best_fine_f1': -1,\n",
    "            'best_fine_std': float('inf'),\n",
    "            'narr_threshold': 0,\n",
    "            'sub_threshold': 0,\n",
    "            'predictions': None,\n",
    "            'best_combined_score': -float('inf'),\n",
    "            'coarse_classification_report': None,\n",
    "            'fine_precision': None,\n",
    "            'fine_recall': None,\n",
    "            'samples_f1_fine': None,\n",
    "        }\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "    \n",
    "        for narr_threshold in thresholds:\n",
    "            for sub_threshold in thresholds:\n",
    "                predictions = []\n",
    "                try:\n",
    "                    for sample_idx, row in dataset.iterrows():\n",
    "                        pred = self._make_prediction(\n",
    "                            row['article_id'],\n",
    "                            sample_idx,\n",
    "                            narr_probs,\n",
    "                            sub_probs_dict,\n",
    "                            narr_threshold,\n",
    "                            sub_threshold\n",
    "                        )\n",
    "                        predictions.append(pred)\n",
    "                    \n",
    "                    metrics_result = self._compute_metrics_coarse_fine(predictions, dataset)\n",
    "                    f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, \\\n",
    "                    samples_f1_fine = metrics_result\n",
    "                    \n",
    "                    combined_score = f1_fine_mean - (std_weight * coarse_std)\n",
    "                    \n",
    "                    if combined_score > best_results['best_combined_score']:\n",
    "                        best_results.update({\n",
    "                            'best_coarse_f1': f1_coarse_mean,\n",
    "                            'best_coarse_std': coarse_std,\n",
    "                            'best_fine_f1': f1_fine_mean,\n",
    "                            'best_fine_std': fine_std,\n",
    "                            'narr_threshold': narr_threshold,\n",
    "                            'sub_threshold': sub_threshold,\n",
    "                            'predictions': predictions,\n",
    "                            'best_combined_score': combined_score,\n",
    "                            'coarse_classification_report': report_coarse,\n",
    "                            'fine_precision': precision_fine,\n",
    "                            'fine_recall': recall_fine,\n",
    "                            'samples_f1_fine': samples_f1_fine,\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during evaluation with thresholds {narr_threshold:.2f}, {sub_threshold:.2f}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "        print(\"\\nBest thresholds found:\")\n",
    "        print(f\"Narrative threshold: {best_results['narr_threshold']:.2f}\")\n",
    "        print(f\"Subnarrative threshold: {best_results['sub_threshold']:.2f}\")\n",
    "        print('\\nCompetition Values')\n",
    "        print(f\"Coarse-F1: {best_results['best_coarse_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. coarse: {best_results['best_coarse_std']:.3f}\")\n",
    "        print(f\"Fine-F1: {best_results['best_fine_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. fine: {best_results['best_fine_std']:.3f}\")\n",
    "        print(\"\\nFine Metrics:\")\n",
    "        print(\"Precision: {:.3f}\".format(best_results['fine_precision']))\n",
    "        print(\"Recall: {:.3f}\".format(best_results['fine_recall']))\n",
    "        print(\"F1 Samples: {:.3f}\".format(best_results['samples_f1_fine']))\n",
    "\n",
    "        if save:\n",
    "            self._save_predictions(best_results, os.path.join(self.output_dir, 'submission.txt'))\n",
    "        \n",
    "        return best_results\n",
    "\n",
    "    def _make_prediction(self, article_id, sample_idx, narr_probs, sub_probs_dict, narr_threshold, sub_threshold):       \n",
    "        other_idx = self.narrative_classes.index(\"Other\")  \n",
    "        active_narratives = [\n",
    "            (n_idx, prob)\n",
    "            for n_idx, prob in enumerate(narr_probs[sample_idx])\n",
    "            if n_idx != other_idx and prob >= narr_threshold\n",
    "        ]\n",
    "\n",
    "        if not active_narratives:\n",
    "            return {\n",
    "                'article_id': article_id,\n",
    "                'narratives': [\"Other\"],\n",
    "                'pairs': [\"Other\"]\n",
    "            }\n",
    "        \n",
    "        narratives = []\n",
    "        pairs = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        active_narratives.sort(key=lambda x: x[1], reverse=True)\n",
    "        for narr_idx, _ in active_narratives:\n",
    "            narr_name = self.narrative_classes[narr_idx]\n",
    "                \n",
    "            sub_probs = sub_probs_dict[str(narr_idx)][sample_idx]\n",
    "            active_subnarratives = [\n",
    "                (local_idx, s_prob)\n",
    "                for local_idx, s_prob in enumerate(sub_probs)\n",
    "                if s_prob >= sub_threshold\n",
    "            ]\n",
    "            \n",
    "            active_subnarratives.sort(key=lambda x: x[1], reverse=True)\n",
    "            if not active_subnarratives:\n",
    "                pairs.append(f\"{narr_name}: Other\")\n",
    "            else:\n",
    "                for local_idx, _ in active_subnarratives:   \n",
    "                    global_sub_idx = self.narrative_to_sub_map[narr_idx][local_idx]\n",
    "                    sub_name = self.subnarrative_classes[global_sub_idx]\n",
    "                    pair = f\"{narr_name}: {sub_name}\"\n",
    "                    if pair not in seen_pairs:\n",
    "                        pairs.append(pair)\n",
    "                        seen_pairs.add(pair)\n",
    "            narratives.append(narr_name)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'narratives': narratives,\n",
    "            'pairs': pairs\n",
    "        }\n",
    "\n",
    "    def _compute_metrics_coarse_fine(self, predictions, dataset):\n",
    "        gold_coarse_all = []\n",
    "        gold_fine_all = []\n",
    "        pred_coarse_all = []\n",
    "        pred_fine_all = []\n",
    "\n",
    "        for pred, (_, row) in zip(predictions, dataset.iterrows()):\n",
    "            gold_coarse = row['narratives']\n",
    "            gold_subnarratives = row['subnarratives']\n",
    "            \n",
    "            pred_coarse = pred['narratives']\n",
    "            pred_fine = []\n",
    "            for p in pred['pairs']:\n",
    "                if p == \"Other\":\n",
    "                    pred_fine.append(\"Other\")\n",
    "                else:\n",
    "                    pred_fine.append(p)\n",
    "\n",
    "            gold_fine = []\n",
    "            for gold_nar, gold_sub in zip(gold_coarse, gold_subnarratives):\n",
    "                if gold_nar == \"Other\":\n",
    "                    gold_fine.append(\"Other\")\n",
    "                else:\n",
    "                    gold_fine.append(f\"{gold_nar}: {gold_sub}\")\n",
    "            \n",
    "            gold_coarse_all.append(gold_coarse)\n",
    "            gold_fine_all.append(gold_fine)\n",
    "            pred_coarse_all.append(pred_coarse)\n",
    "            pred_fine_all.append(pred_fine)\n",
    "\n",
    "        f1_coarse_mean, coarse_std = self._evaluate_multi_label(gold_coarse_all, pred_coarse_all, self.classes_coarse)\n",
    "        f1_fine_mean, fine_std = self._evaluate_multi_label(gold_fine_all, pred_fine_all, self.classes_fine)\n",
    "        \n",
    "        gold_coarse_flat = []\n",
    "        pred_coarse_flat = []\n",
    "        for g_labels, p_labels in zip(gold_coarse_all, pred_coarse_all):\n",
    "            g_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            p_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            \n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    g_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    p_onehot[self.classes_coarse.index(lab)] = 1\n",
    "                    \n",
    "            gold_coarse_flat.append(g_onehot)\n",
    "            pred_coarse_flat.append(p_onehot)\n",
    "            \n",
    "        gold_coarse_flat = np.array(gold_coarse_flat)\n",
    "        pred_coarse_flat = np.array(pred_coarse_flat)\n",
    "        \n",
    "        report_coarse = metrics.classification_report(\n",
    "                gold_coarse_flat, pred_coarse_flat, \n",
    "                target_names=self.classes_coarse, \n",
    "                zero_division=0\n",
    "        )\n",
    "        \n",
    "        gold_fine_flat = []\n",
    "        pred_fine_flat = []\n",
    "        for g_labels, p_labels in zip(gold_fine_all, pred_fine_all):\n",
    "            g_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            p_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            \n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    g_onehot[self.classes_fine.index(lab)] = 1\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    p_onehot[self.classes_fine.index(lab)] = 1\n",
    "                    \n",
    "            gold_fine_flat.append(g_onehot)\n",
    "            pred_fine_flat.append(p_onehot)\n",
    "            \n",
    "        gold_fine_flat = np.array(gold_fine_flat)\n",
    "        pred_fine_flat = np.array(pred_fine_flat)\n",
    "        \n",
    "        precision_fine = metrics.precision_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        recall_fine = metrics.recall_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        samples_f1_fine = metrics.f1_score(gold_fine_flat, pred_fine_flat, average='samples', zero_division=0)\n",
    "        \n",
    "        return f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, samples_f1_fine\n",
    "\n",
    "    def _evaluate_multi_label(self, gold, predicted, class_list):\n",
    "        f1_scores = []\n",
    "        for g_labels, p_labels in zip(gold, predicted):\n",
    "            g_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            p_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            \n",
    "            for lab in g_labels:\n",
    "                if lab in class_list:\n",
    "                    g_onehot[class_list.index(lab)] = 1\n",
    "            for lab in p_labels:\n",
    "                if lab in class_list:\n",
    "                    p_onehot[class_list.index(lab)] = 1\n",
    "                    \n",
    "            f1_doc = metrics.f1_score(g_onehot, p_onehot, zero_division=0)\n",
    "            f1_scores.append(f1_doc)\n",
    "            \n",
    "        return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "    def _save_predictions(self, best_results, filepath):\n",
    "        predictions = best_results['predictions']\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                line = (f\"{pred['article_id']}\\t\"\n",
    "                       f\"{';'.join(pred['narratives'])}\\t\"\n",
    "                       f\"{';'.join(pred['pairs'])}\\n\")\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f5029ac-2fc2-409e-b014-a493458cfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71e91a-9e25-492a-8828-bc6a249310d7",
   "metadata": {},
   "source": [
    "As we train on multiple languages in sequence, the last language (our target) needs some kind of special care. \n",
    "Our goal is to make sure the model performs best on our target language, without losing what it learned from previous languages. When we reach the target language, we make two key changes:\n",
    "- We increase the patience parameter to train the model more carefully on the target language.\n",
    "- We also lower the learning rate, because the model has already learned some patterns from other languages, we want smaller, more precise updates for the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cbdc283-3c7c-46c5-bb41-cb607b4837b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinualLearningModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class,\n",
    "        model_params,\n",
    "        dataset_train=dataset_train,\n",
    "        train_embeddings=train_embeddings,\n",
    "        dataset_val=dataset_val,\n",
    "        val_embeddings=val_embeddings,\n",
    "        language_order=['RU', 'BG', 'HI', 'PT', 'EN'],\n",
    "        learning_rate=0.001,\n",
    "        target=\"EN\",\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.dataset_train = dataset_train\n",
    "        self.train_embeddings = train_embeddings\n",
    "        self.dataset_val = dataset_val\n",
    "        self.val_embeddings = val_embeddings\n",
    "        self.language_order = language_order\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.target = target\n",
    "        self.y_val_nar = self.dataset_val['narratives_encoded'].tolist()\n",
    "        self.y_val_sub_heads = self.dataset_val['aggregated_subnarratives'].tolist()\n",
    "        \n",
    "\n",
    "    def _prepare_language_data(self, language):\n",
    "        language_mask = self.dataset_train[\"language\"] == language\n",
    "        train_data = self.dataset_train[language_mask].copy()\n",
    "        train_emb = self.train_embeddings[language_mask]\n",
    "        y_train_nar = torch.tensor(train_data['narratives_encoded'].tolist(), dtype=torch.float32).to(self.device)\n",
    "        y_train_sub_heads = train_data['aggregated_subnarratives'].tolist()\n",
    "        train_emb = torch.tensor(train_emb, dtype=torch.float32).to(self.device)\n",
    "        return train_data, train_emb, y_train_nar, y_train_sub_heads\n",
    "\n",
    "    def _setup_loss_function(self, y_train_nar, y_train_sub_heads, language):\n",
    "        class_weights_nar = compute_class_weights(y_train_nar)\n",
    "        narrative_criterion = WeightedBCELoss(class_weights_nar)\n",
    "        \n",
    "        sub_criterion_dict = {}\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            local_weights = compute_class_weights(torch.tensor([h[narr_idx] for h in y_train_sub_heads]))\n",
    "            sub_criterion = WeightedBCELoss(local_weights)\n",
    "            sub_criterion_dict[str(narr_idx)] = sub_criterion\n",
    "            \n",
    "        return MultiHeadLoss(narrative_criterion, sub_criterion_dict)\n",
    "\n",
    "    def train(self, epochs_per_language=100, patience=10):\n",
    "        self.model = self.model_class(**self.model_params).to(self.device)\n",
    "\n",
    "        for lang_idx, language in enumerate(self.language_order):\n",
    "            print(f\"\\nTraining on {language} data...\")\n",
    "            \n",
    "            if language == self.target:\n",
    "                patience = patience * 2\n",
    "                learning_rate = self.learning_rate * 0.2\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, \n",
    "                    mode='min', \n",
    "                    factor=0.5,\n",
    "                    patience=8,\n",
    "                    min_lr=2e-5,\n",
    "                    threshold=1e-4\n",
    "                )\n",
    "            else:\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=5\n",
    "                )\n",
    "            \n",
    "            train_data, train_emb, y_train_nar, y_train_sub_heads = self._prepare_language_data(language)\n",
    "            loss_fn = self._setup_loss_function(y_train_nar, y_train_sub_heads, language)\n",
    "            val_emb_tensor = torch.tensor(self.val_embeddings, dtype=torch.float32).to(self.device)\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            best_model_state = None\n",
    "\n",
    "            for epoch in range(epochs_per_language):\n",
    "                self.model.train()\n",
    "                train_narr_probs, train_sub_probs_dict = self.model(train_emb)\n",
    "                train_loss = loss_fn(\n",
    "                    train_narr_probs,\n",
    "                    train_sub_probs_dict,\n",
    "                    y_train_nar,\n",
    "                    y_train_sub_heads\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                                    \n",
    "                optimizer.step()\n",
    "\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_narr_probs, val_sub_probs_dict = self.model(val_emb_tensor)\n",
    "                    val_loss = loss_fn(\n",
    "                        val_narr_probs,\n",
    "                        val_sub_probs_dict,\n",
    "                        torch.tensor(self.y_val_nar, dtype=torch.float32).to(self.device),\n",
    "                        self.y_val_sub_heads\n",
    "                    )\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs_per_language}, \"\n",
    "                      f\"Train Loss: {train_loss.item():.4f}, \"\n",
    "                      f\"Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step(val_loss)\n",
    "                    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered for {language}\")\n",
    "                    break\n",
    "\n",
    "            if best_model_state:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate_final(self, save_predictions=True):\n",
    "        evaluator = MultiHeadEvaluator(device=self.device)\n",
    "        val_emb_tensor = torch.tensor(self.val_embeddings, dtype=torch.float32).to(self.device)\n",
    "        _ = evaluator.evaluate(\n",
    "            self.model,\n",
    "            val_emb_tensor,\n",
    "            self.dataset_val,\n",
    "            save=save_predictions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "849c21ca-b972-41c0-8d18-47048dd66eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {\n",
    "    'PT': ['RU', 'HI', 'BG', 'EN', 'PT'],\n",
    "    'EN': ['RU', 'BG', 'PT', 'HI', 'EN']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e582f89-6a54-42f9-b63e-218de500cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_order=['RU', 'BG', 'PT', 'HI', 'EN']\n",
    "target=language_order[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64f57653-fd1a-4cae-9576-35f1ff023fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size': train_embeddings.shape[1],\n",
    "    'hidden_size': 2048,\n",
    "    'dropout_rate': 0.4\n",
    "}\n",
    "\n",
    "cl_model = ContinualLearningModel(\n",
    "    model_class=MultiTaskClassifierMultiHead,\n",
    "    model_params=model_params,\n",
    "    language_order=language_order,\n",
    "    target=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39f528c1-e47f-4e41-bf74-46e049801217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on RU data...\n",
      "Epoch 1/100, Train Loss: 0.7067, Val Loss: 0.8710\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.4126, Val Loss: 0.8672\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.2842, Val Loss: 0.8689\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.2238, Val Loss: 0.8752\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.1906, Val Loss: 0.8844\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.1682, Val Loss: 0.8951\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.1495, Val Loss: 0.9069\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.1330, Val Loss: 0.9182\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.1184, Val Loss: 0.9241\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.1144, Val Loss: 0.9300\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.1088, Val Loss: 0.9362\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.1017, Val Loss: 0.9435\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.0960, Val Loss: 0.9523\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.0907, Val Loss: 0.9627\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.0865, Val Loss: 0.9723\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.0835, Val Loss: 0.9831\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.0818, Val Loss: 0.9950\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for RU\n",
      "\n",
      "Training on BG data...\n",
      "Epoch 1/100, Train Loss: 3.1307, Val Loss: 2.7006\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 1.5798, Val Loss: 2.4855\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.8328, Val Loss: 2.3949\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5654, Val Loss: 2.3612\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4753, Val Loss: 2.3518\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4437, Val Loss: 2.3548\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4218, Val Loss: 2.3658\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.4074, Val Loss: 2.3819\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.3910, Val Loss: 2.4040\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.3710, Val Loss: 2.4355\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.3521, Val Loss: 2.4792\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.3322, Val Loss: 2.4846\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.3203, Val Loss: 2.5037\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.3066, Val Loss: 2.5394\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 15/100, Train Loss: 0.2966, Val Loss: 2.5919\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 16/100, Train Loss: 0.2830, Val Loss: 2.6599\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 17/100, Train Loss: 0.2769, Val Loss: 2.7452\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 18/100, Train Loss: 0.2679, Val Loss: 2.7914\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 19/100, Train Loss: 0.2616, Val Loss: 2.8515\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 20/100, Train Loss: 0.2605, Val Loss: 2.9263\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for BG\n",
      "\n",
      "Training on PT data...\n",
      "Epoch 1/100, Train Loss: 0.9502, Val Loss: 1.5107\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.5652, Val Loss: 1.5479\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.4889, Val Loss: 1.6301\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.4340, Val Loss: 1.7289\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.3903, Val Loss: 1.8300\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.3640, Val Loss: 1.9645\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3437, Val Loss: 2.1439\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.3119, Val Loss: 2.1300\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.2985, Val Loss: 2.1138\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.2821, Val Loss: 2.1083\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.2679, Val Loss: 2.1139\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.2605, Val Loss: 2.1328\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.2487, Val Loss: 2.1679\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 14/100, Train Loss: 0.2419, Val Loss: 2.1210\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.2368, Val Loss: 2.0936\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.2310, Val Loss: 2.0822\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for PT\n",
      "\n",
      "Training on HI data...\n",
      "Epoch 1/100, Train Loss: 1.6809, Val Loss: 7.0285\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.8847, Val Loss: 9.2733\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.6674, Val Loss: 11.8303\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5608, Val Loss: 14.2024\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4856, Val Loss: 16.1537\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4376, Val Loss: 17.3550\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4006, Val Loss: 18.0888\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.3614, Val Loss: 16.8367\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.3456, Val Loss: 15.9759\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.3310, Val Loss: 15.6090\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.3103, Val Loss: 15.7150\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.2980, Val Loss: 16.1761\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.2841, Val Loss: 16.9624\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 14/100, Train Loss: 0.2750, Val Loss: 17.4730\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.2694, Val Loss: 18.2395\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.2629, Val Loss: 19.1936\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for HI\n",
      "\n",
      "Training on EN data...\n",
      "Epoch 1/100, Train Loss: 1.2047, Val Loss: 2.1026\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 2/100, Train Loss: 1.0695, Val Loss: 1.8069\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 3/100, Train Loss: 0.9807, Val Loss: 1.5961\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 4/100, Train Loss: 0.8852, Val Loss: 1.4518\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 5/100, Train Loss: 0.8211, Val Loss: 1.3576\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 6/100, Train Loss: 0.7558, Val Loss: 1.2974\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 7/100, Train Loss: 0.6968, Val Loss: 1.2593\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 8/100, Train Loss: 0.6566, Val Loss: 1.2386\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 9/100, Train Loss: 0.6246, Val Loss: 1.2285\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 10/100, Train Loss: 0.5863, Val Loss: 1.2260\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 11/100, Train Loss: 0.5546, Val Loss: 1.2280\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 12/100, Train Loss: 0.5348, Val Loss: 1.2321\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 13/100, Train Loss: 0.5106, Val Loss: 1.2355\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 14/100, Train Loss: 0.4994, Val Loss: 1.2385\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 15/100, Train Loss: 0.4765, Val Loss: 1.2379\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 16/100, Train Loss: 0.4622, Val Loss: 1.2332\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 17/100, Train Loss: 0.4473, Val Loss: 1.2255\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 18/100, Train Loss: 0.4375, Val Loss: 1.2162\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 19/100, Train Loss: 0.4183, Val Loss: 1.2082\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 20/100, Train Loss: 0.4057, Val Loss: 1.1994\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 21/100, Train Loss: 0.3998, Val Loss: 1.1930\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 22/100, Train Loss: 0.3881, Val Loss: 1.1883\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 23/100, Train Loss: 0.3762, Val Loss: 1.1863\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 24/100, Train Loss: 0.3672, Val Loss: 1.1842\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 25/100, Train Loss: 0.3596, Val Loss: 1.1819\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 26/100, Train Loss: 0.3514, Val Loss: 1.1806\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 27/100, Train Loss: 0.3422, Val Loss: 1.1771\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 28/100, Train Loss: 0.3374, Val Loss: 1.1735\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 29/100, Train Loss: 0.3332, Val Loss: 1.1696\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 30/100, Train Loss: 0.3237, Val Loss: 1.1642\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 31/100, Train Loss: 0.3176, Val Loss: 1.1582\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 32/100, Train Loss: 0.3108, Val Loss: 1.1503\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 33/100, Train Loss: 0.3061, Val Loss: 1.1391\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 34/100, Train Loss: 0.2996, Val Loss: 1.1289\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 35/100, Train Loss: 0.2944, Val Loss: 1.1175\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 36/100, Train Loss: 0.2916, Val Loss: 1.1070\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 37/100, Train Loss: 0.2865, Val Loss: 1.0961\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 38/100, Train Loss: 0.2827, Val Loss: 1.0834\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 39/100, Train Loss: 0.2780, Val Loss: 1.0690\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 40/100, Train Loss: 0.2751, Val Loss: 1.0537\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 41/100, Train Loss: 0.2699, Val Loss: 1.0363\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 42/100, Train Loss: 0.2648, Val Loss: 1.0206\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 43/100, Train Loss: 0.2609, Val Loss: 1.0054\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 44/100, Train Loss: 0.2578, Val Loss: 0.9929\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 45/100, Train Loss: 0.2559, Val Loss: 0.9821\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 46/100, Train Loss: 0.2510, Val Loss: 0.9739\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 47/100, Train Loss: 0.2484, Val Loss: 0.9689\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 48/100, Train Loss: 0.2469, Val Loss: 0.9648\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 49/100, Train Loss: 0.2413, Val Loss: 0.9619\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 50/100, Train Loss: 0.2381, Val Loss: 0.9597\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 51/100, Train Loss: 0.2365, Val Loss: 0.9574\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 52/100, Train Loss: 0.2318, Val Loss: 0.9559\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 53/100, Train Loss: 0.2285, Val Loss: 0.9561\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 54/100, Train Loss: 0.2269, Val Loss: 0.9548\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 55/100, Train Loss: 0.2251, Val Loss: 0.9520\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 56/100, Train Loss: 0.2202, Val Loss: 0.9491\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 57/100, Train Loss: 0.2192, Val Loss: 0.9470\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 58/100, Train Loss: 0.2147, Val Loss: 0.9468\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 59/100, Train Loss: 0.2112, Val Loss: 0.9478\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 60/100, Train Loss: 0.2088, Val Loss: 0.9505\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 61/100, Train Loss: 0.2095, Val Loss: 0.9547\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 62/100, Train Loss: 0.2062, Val Loss: 0.9606\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 63/100, Train Loss: 0.2028, Val Loss: 0.9665\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 64/100, Train Loss: 0.2011, Val Loss: 0.9731\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 65/100, Train Loss: 0.1993, Val Loss: 0.9810\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 66/100, Train Loss: 0.1963, Val Loss: 0.9913\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 67/100, Train Loss: 0.1954, Val Loss: 1.0013\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 68/100, Train Loss: 0.1923, Val Loss: 1.0030\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 69/100, Train Loss: 0.1910, Val Loss: 1.0045\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 70/100, Train Loss: 0.1890, Val Loss: 1.0054\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 71/100, Train Loss: 0.1883, Val Loss: 1.0074\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 72/100, Train Loss: 0.1872, Val Loss: 1.0097\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 73/100, Train Loss: 0.1880, Val Loss: 1.0120\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 74/100, Train Loss: 0.1869, Val Loss: 1.0139\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 75/100, Train Loss: 0.1838, Val Loss: 1.0163\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 76/100, Train Loss: 0.1833, Val Loss: 1.0195\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 77/100, Train Loss: 0.1835, Val Loss: 1.0205\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 78/100, Train Loss: 0.1825, Val Loss: 1.0216\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 79/100, Train Loss: 0.1822, Val Loss: 1.0231\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 80/100, Train Loss: 0.1806, Val Loss: 1.0249\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 81/100, Train Loss: 0.1807, Val Loss: 1.0263\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 82/100, Train Loss: 0.1803, Val Loss: 1.0276\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 83/100, Train Loss: 0.1794, Val Loss: 1.0294\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 84/100, Train Loss: 0.1784, Val Loss: 1.0315\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 85/100, Train Loss: 0.1782, Val Loss: 1.0336\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 86/100, Train Loss: 0.1778, Val Loss: 1.0345\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 87/100, Train Loss: 0.1784, Val Loss: 1.0353\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 88/100, Train Loss: 0.1793, Val Loss: 1.0360\n",
      "Current Learning Rate: 0.000025\n",
      "Early stopping triggered for EN\n"
     ]
    }
   ],
   "source": [
    "model = cl_model.train(epochs_per_language=100, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6834c3ab-39f1-4105-9d5d-9d5d9e6f23f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.55\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.524\n",
      "F1 st. dev. coarse: 0.352\n",
      "Fine-F1: 0.375\n",
      "F1 st. dev. fine: 0.352\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.114\n",
      "Recall: 0.263\n",
      "F1 Samples: 0.375\n"
     ]
    }
   ],
   "source": [
    "cl_model.evaluate_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da931e-ca57-4ed9-bb02-3f3f91b4695d",
   "metadata": {},
   "source": [
    "If we change the order of the languages being trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4409aa38-c6f3-4088-83a2-e17ac59b9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_order=['RU', 'HI', 'PT', 'BG', 'EN']\n",
    "target=language_order[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74ab6b2a-021c-4cc2-91ba-9829c29bbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_model = ContinualLearningModel(\n",
    "    model_class=MultiTaskClassifierMultiHead,\n",
    "    model_params=model_params,\n",
    "    language_order=language_order,\n",
    "    target=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "614777df-64e0-408c-ba20-f2da6c6dd8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on RU data...\n",
      "Epoch 1/100, Train Loss: 0.7104, Val Loss: 0.8775\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.4176, Val Loss: 0.8726\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.2896, Val Loss: 0.8728\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.2272, Val Loss: 0.8775\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.1906, Val Loss: 0.8856\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.1708, Val Loss: 0.8956\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.1517, Val Loss: 0.9060\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.1356, Val Loss: 0.9174\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.1229, Val Loss: 0.9244\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.1169, Val Loss: 0.9318\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.1112, Val Loss: 0.9398\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.1035, Val Loss: 0.9489\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.0985, Val Loss: 0.9594\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.0927, Val Loss: 0.9716\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.0888, Val Loss: 0.9824\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.0853, Val Loss: 0.9943\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.0831, Val Loss: 1.0075\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for RU\n",
      "\n",
      "Training on HI data...\n",
      "Epoch 1/100, Train Loss: 3.8097, Val Loss: 6.1408\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 2.2381, Val Loss: 5.8020\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 1.1195, Val Loss: 5.6394\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.6701, Val Loss: 5.5928\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5023, Val Loss: 5.5843\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4479, Val Loss: 5.5925\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4195, Val Loss: 5.6103\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.4027, Val Loss: 5.6423\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.3935, Val Loss: 5.7101\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.3757, Val Loss: 5.8254\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.3531, Val Loss: 6.0142\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.3285, Val Loss: 6.1971\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.3160, Val Loss: 6.4938\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.3055, Val Loss: 6.9176\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 15/100, Train Loss: 0.2922, Val Loss: 7.4876\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 16/100, Train Loss: 0.2808, Val Loss: 8.1989\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 17/100, Train Loss: 0.2700, Val Loss: 9.0652\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 18/100, Train Loss: 0.2621, Val Loss: 9.9400\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 19/100, Train Loss: 0.2580, Val Loss: 10.9313\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 20/100, Train Loss: 0.2542, Val Loss: 12.0269\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for HI\n",
      "\n",
      "Training on PT data...\n",
      "Epoch 1/100, Train Loss: 1.1767, Val Loss: 2.7393\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.6779, Val Loss: 2.6052\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.5365, Val Loss: 2.7517\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.4608, Val Loss: 3.0139\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.4185, Val Loss: 3.3465\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.3798, Val Loss: 3.7243\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.3546, Val Loss: 4.0982\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.3338, Val Loss: 4.4020\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.3167, Val Loss: 4.2257\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.3028, Val Loss: 4.0297\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.2947, Val Loss: 3.8225\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.2786, Val Loss: 3.6301\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.2738, Val Loss: 3.4575\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 14/100, Train Loss: 0.2665, Val Loss: 3.3026\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.2598, Val Loss: 3.0768\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.2535, Val Loss: 2.8873\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 17/100, Train Loss: 0.2494, Val Loss: 2.7323\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for PT\n",
      "\n",
      "Training on BG data...\n",
      "Epoch 1/100, Train Loss: 1.2013, Val Loss: 5.0186\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Train Loss: 0.8108, Val Loss: 7.1337\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Train Loss: 0.6466, Val Loss: 9.4737\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Train Loss: 0.5956, Val Loss: 11.5626\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Train Loss: 0.5381, Val Loss: 13.1916\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Train Loss: 0.4966, Val Loss: 14.2672\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.4585, Val Loss: 14.8643\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 8/100, Train Loss: 0.4262, Val Loss: 13.5538\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 9/100, Train Loss: 0.4076, Val Loss: 12.1823\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 10/100, Train Loss: 0.3916, Val Loss: 10.9083\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 11/100, Train Loss: 0.3782, Val Loss: 9.7650\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 12/100, Train Loss: 0.3657, Val Loss: 8.7783\n",
      "Current Learning Rate: 0.000500\n",
      "Epoch 13/100, Train Loss: 0.3582, Val Loss: 7.9842\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 14/100, Train Loss: 0.3521, Val Loss: 7.2102\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 15/100, Train Loss: 0.3467, Val Loss: 6.6173\n",
      "Current Learning Rate: 0.000250\n",
      "Epoch 16/100, Train Loss: 0.3431, Val Loss: 6.1752\n",
      "Current Learning Rate: 0.000250\n",
      "Early stopping triggered for BG\n",
      "\n",
      "Training on EN data...\n",
      "Epoch 1/100, Train Loss: 1.1765, Val Loss: 1.8360\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 2/100, Train Loss: 1.0537, Val Loss: 1.4508\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 3/100, Train Loss: 0.9625, Val Loss: 1.2253\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 4/100, Train Loss: 0.8900, Val Loss: 1.1082\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 5/100, Train Loss: 0.8358, Val Loss: 1.0531\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 6/100, Train Loss: 0.7901, Val Loss: 1.0339\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 7/100, Train Loss: 0.7354, Val Loss: 1.0338\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 8/100, Train Loss: 0.7048, Val Loss: 1.0448\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 9/100, Train Loss: 0.6602, Val Loss: 1.0614\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 10/100, Train Loss: 0.6264, Val Loss: 1.0825\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 11/100, Train Loss: 0.5941, Val Loss: 1.1056\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 12/100, Train Loss: 0.5584, Val Loss: 1.1303\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 13/100, Train Loss: 0.5327, Val Loss: 1.1550\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 14/100, Train Loss: 0.5085, Val Loss: 1.1816\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 15/100, Train Loss: 0.4850, Val Loss: 1.2092\n",
      "Current Learning Rate: 0.000200\n",
      "Epoch 16/100, Train Loss: 0.4670, Val Loss: 1.2369\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 17/100, Train Loss: 0.4512, Val Loss: 1.2143\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 18/100, Train Loss: 0.4460, Val Loss: 1.1952\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 19/100, Train Loss: 0.4396, Val Loss: 1.1796\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 20/100, Train Loss: 0.4346, Val Loss: 1.1671\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 21/100, Train Loss: 0.4273, Val Loss: 1.1568\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 22/100, Train Loss: 0.4198, Val Loss: 1.1486\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 23/100, Train Loss: 0.4152, Val Loss: 1.1425\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 24/100, Train Loss: 0.4158, Val Loss: 1.1377\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 25/100, Train Loss: 0.4046, Val Loss: 1.1338\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 26/100, Train Loss: 0.4031, Val Loss: 1.1119\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 27/100, Train Loss: 0.3999, Val Loss: 1.0930\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 28/100, Train Loss: 0.3973, Val Loss: 1.0769\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 29/100, Train Loss: 0.3921, Val Loss: 1.0632\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 30/100, Train Loss: 0.3933, Val Loss: 1.0519\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 31/100, Train Loss: 0.3890, Val Loss: 1.0424\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 32/100, Train Loss: 0.3837, Val Loss: 1.0347\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 33/100, Train Loss: 0.3792, Val Loss: 1.0284\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 34/100, Train Loss: 0.3799, Val Loss: 1.0231\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 35/100, Train Loss: 0.3775, Val Loss: 1.0187\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 36/100, Train Loss: 0.3803, Val Loss: 1.0153\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 37/100, Train Loss: 0.3731, Val Loss: 1.0127\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 38/100, Train Loss: 0.3743, Val Loss: 1.0108\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 39/100, Train Loss: 0.3699, Val Loss: 1.0093\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 40/100, Train Loss: 0.3673, Val Loss: 1.0082\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 41/100, Train Loss: 0.3635, Val Loss: 1.0074\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 42/100, Train Loss: 0.3650, Val Loss: 1.0069\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 43/100, Train Loss: 0.3625, Val Loss: 1.0068\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 44/100, Train Loss: 0.3585, Val Loss: 1.0072\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 45/100, Train Loss: 0.3556, Val Loss: 1.0075\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 46/100, Train Loss: 0.3547, Val Loss: 1.0079\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 47/100, Train Loss: 0.3536, Val Loss: 1.0084\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 48/100, Train Loss: 0.3524, Val Loss: 1.0094\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 49/100, Train Loss: 0.3497, Val Loss: 1.0106\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 50/100, Train Loss: 0.3498, Val Loss: 1.0119\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 51/100, Train Loss: 0.3454, Val Loss: 1.0130\n",
      "Current Learning Rate: 0.000050\n",
      "Epoch 52/100, Train Loss: 0.3450, Val Loss: 1.0140\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 53/100, Train Loss: 0.3424, Val Loss: 1.0097\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 54/100, Train Loss: 0.3439, Val Loss: 1.0060\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 55/100, Train Loss: 0.3393, Val Loss: 1.0025\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 56/100, Train Loss: 0.3420, Val Loss: 0.9995\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 57/100, Train Loss: 0.3406, Val Loss: 0.9969\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 58/100, Train Loss: 0.3416, Val Loss: 0.9945\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 59/100, Train Loss: 0.3395, Val Loss: 0.9923\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 60/100, Train Loss: 0.3396, Val Loss: 0.9905\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 61/100, Train Loss: 0.3388, Val Loss: 0.9891\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 62/100, Train Loss: 0.3384, Val Loss: 0.9880\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 63/100, Train Loss: 0.3348, Val Loss: 0.9872\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 64/100, Train Loss: 0.3361, Val Loss: 0.9866\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 65/100, Train Loss: 0.3369, Val Loss: 0.9861\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 66/100, Train Loss: 0.3343, Val Loss: 0.9856\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 67/100, Train Loss: 0.3319, Val Loss: 0.9852\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 68/100, Train Loss: 0.3333, Val Loss: 0.9848\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 69/100, Train Loss: 0.3356, Val Loss: 0.9845\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 70/100, Train Loss: 0.3314, Val Loss: 0.9840\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 71/100, Train Loss: 0.3300, Val Loss: 0.9836\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 72/100, Train Loss: 0.3300, Val Loss: 0.9836\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 73/100, Train Loss: 0.3291, Val Loss: 0.9837\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 74/100, Train Loss: 0.3274, Val Loss: 0.9839\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 75/100, Train Loss: 0.3279, Val Loss: 0.9840\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 76/100, Train Loss: 0.3286, Val Loss: 0.9841\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 77/100, Train Loss: 0.3281, Val Loss: 0.9845\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 78/100, Train Loss: 0.3271, Val Loss: 0.9848\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 79/100, Train Loss: 0.3265, Val Loss: 0.9850\n",
      "Current Learning Rate: 0.000025\n",
      "Epoch 80/100, Train Loss: 0.3259, Val Loss: 0.9851\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 81/100, Train Loss: 0.3212, Val Loss: 0.9846\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 82/100, Train Loss: 0.3241, Val Loss: 0.9842\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 83/100, Train Loss: 0.3226, Val Loss: 0.9838\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 84/100, Train Loss: 0.3231, Val Loss: 0.9833\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 85/100, Train Loss: 0.3210, Val Loss: 0.9830\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 86/100, Train Loss: 0.3227, Val Loss: 0.9830\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 87/100, Train Loss: 0.3203, Val Loss: 0.9830\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 88/100, Train Loss: 0.3207, Val Loss: 0.9831\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 89/100, Train Loss: 0.3203, Val Loss: 0.9832\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 90/100, Train Loss: 0.3208, Val Loss: 0.9833\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 91/100, Train Loss: 0.3204, Val Loss: 0.9835\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 92/100, Train Loss: 0.3181, Val Loss: 0.9840\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 93/100, Train Loss: 0.3148, Val Loss: 0.9845\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 94/100, Train Loss: 0.3147, Val Loss: 0.9852\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 95/100, Train Loss: 0.3177, Val Loss: 0.9857\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 96/100, Train Loss: 0.3180, Val Loss: 0.9863\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 97/100, Train Loss: 0.3146, Val Loss: 0.9869\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 98/100, Train Loss: 0.3160, Val Loss: 0.9874\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 99/100, Train Loss: 0.3128, Val Loss: 0.9880\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 100/100, Train Loss: 0.3128, Val Loss: 0.9886\n",
      "Current Learning Rate: 0.000020\n"
     ]
    }
   ],
   "source": [
    "model = cl_model.train(epochs_per_language=100, patience=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277fc9b-c9fc-469a-a8df-291b10cf6d39",
   "metadata": {},
   "source": [
    "The results are poor compared to the first language order.\n",
    "\n",
    "That could be because, the jump from Russian directly to Hindi could be too drastic as these languages have very different structures, and we might try to get very good at it without having the right \"prerequisites\"\n",
    "\n",
    "- In the first instance, however, we have a somewhat more smooth transition, Russian and Bulgarian are both Slavic languages, then the model gets to know different patterns from Portuguese and maybe harder ones from Hindi before moving on to the final target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f8be43a-35f1-4018-b765-61c256e69ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.35\n",
      "Subnarrative threshold: 0.45\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.432\n",
      "F1 st. dev. coarse: 0.314\n",
      "Fine-F1: 0.260\n",
      "F1 st. dev. fine: 0.283\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.114\n",
      "Recall: 0.332\n",
      "F1 Samples: 0.260\n"
     ]
    }
   ],
   "source": [
    "cl_model.evaluate_final()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
