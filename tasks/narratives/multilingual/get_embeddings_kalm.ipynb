{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317b480a-984e-4925-a5b8-11f957ee80c9",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Baseline Classification -- Multilingual\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb696a-1499-44de-a3b7-68549c63885b",
   "metadata": {},
   "source": [
    "## 1. Getting embeddings for the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0dc0d-a7a3-4e6d-b294-1623eeee4d8e",
   "metadata": {},
   "source": [
    "We load our dataframe, to be used for getting the embeddings from the article context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda87a96-5c18-436c-bd4f-9c2e9a22624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93eca595-5171-4ccc-b89e-4cb87e0aa3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e10b46-d163-4afb-b0ba-da84c133171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838a13a-b9ea-47c6-a136-4f57caf392a9",
   "metadata": {},
   "source": [
    "We make sure that everything is ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c060e19e-c960-48ef-bf97-b1f2f69dd519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \n",
       "0  [The West are the aggressors, Other, The West ...  \n",
       "1       [The West is weak, Other, The EU is divided]  \n",
       "2     [Western media is an instrument of propaganda]  \n",
       "3  [Ukraine is a puppet of the West, Discrediting...  \n",
       "4    [Russia is a guarantor of peace and prosperity]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5290784a-7d9b-4d08-b377-f15a1a3eaa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e032647-8c41-4240-a333-044ec384d60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f0936-a973-4576-83e4-a8444621053c",
   "metadata": {},
   "source": [
    "We are going to be using the [KaLM-embedding-multilingual-mini-instruct-v1.5](https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5) model for generating embeddings. \n",
    "\n",
    "There are several embedding models available out there, as listed in the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), and after testing multiple options, this one has shown to provide the best performance. \n",
    "\n",
    "Notice also, that we're using a maximum sequence length of 512 for the tokenizer. \n",
    "While we could use a longer sequence to tokenize the entire article, splitting the article into smaller chunks and processing them individually yielded better results during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e090cfc1-da0c-44a5-9323-ba171cca44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "kalm = \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\"\n",
    "kalm_model = SentenceTransformer(kalm)\n",
    "kalm_max_length = 512 # recommended by the model\n",
    "kalm_model.max_seq_length = kalm_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce859e-653c-4923-8277-87b71d09b07b",
   "metadata": {},
   "source": [
    "Move to GPU if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a135e1-6fdb-488b-b2e5-8826675409f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: Qwen2Model \n",
       "  (1): Pooling({'word_embedding_dimension': 896, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "kalm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c54c08-63d9-4062-92ed-7ec6cb28075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"This is a news article about politics.\",  # English\n",
    "    \"यह राजनीति के बारे में एक समाचार लेख है।\",  # Hindi translation\n",
    "    \"Este é um artigo de notícias sobre política.\",  # Portuguese translation\n",
    "    \"Това е новинарска статия за политика.\",  # Bulgarian translation\n",
    "    \"Это новость о политике.\",  # Russian translation\n",
    "    \"The weather was nice today.\"  # Unrelated sentence\n",
    "]\n",
    "\n",
    "embeddings = kalm_model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced3772f-adbc-4c85-ae78-411f971b0136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PARA>UN chief Warns of global economic crisis at world economic forum in Davos</PARA>\\n\\nUnited Nations secretary-general Antonio Guterres offered a grim outlook on the global economic during the World Economic Forum’s 2023 summit in Davos, Switzerland, warning of a possible worldwide economic downturn. citing the “perfect storm” of the Russia–Ukraine war, high food prices, and high energy costs, Guterres said that there will be “huge economic consequences” around the world. he did not predict, however, when the slowdown would start. Guterres noted that economies outside the United States, europe, and asia would face worsening economic conditions. developing countries in africa, south america, and much of asia are seeing rising interest rates, he noted. “interest rates are going up extremely in the global south. countries are close to default. they have no resources because they couldn’t print money like the United States did, like europe did during the covid-19 [pandemic],” the UN chief told Bartiromo. “they have no access to concessional funding because many of them are middle-income countries. look at small island developing states, the caribbean islands.” some of those countries, he added, “lived on tourism” and “tourism has ended for two years, but as they are middle-income countries, they had no debt to live, they had no concessional funding.” Guterres said that the United Nations is proposing a global stimulus plan to address economic woes, and claimed that economies should transition from oil and gas to other energy sources. “we need to do that in a just way,” he said, adding that “this transition needs to be well-managed.” meanwhile, the west and China need to have “serious” negotiations amid the dim global economic outlook, said the UN chief. other warningswhile inflation has eased at the consumer and producer level in the United States in recent months, a number of economists and business leaders say that americans should prepare for a recession starting in 2023. The Federal Reserve has raised interest rates to their highest levels in decades in order to cool high inflation that reached more than 9 percent in june 2022, though it has since dropped to below 7 percent. “some economists argue that the strength of the labor market—as well as household balance sheets—will keep the economy strong enough to avoid a recession,” wrote Lakshman Achuthan and Anirvan Banerji, co-founders of the Economic Cycle Research Institute, for CNN’s website. “we disagree,” they wrote, saying that “it remains our expectation that the U.S. economy will enter a recession this year.” “i’m confident that Microsoft will emerge from this stronger and more competitive,” Microsoft ceo Satya Nadella told employees in a memo on company’s website on wednesday.\\n\\n<PARA>other tech firms, such as Amazon, Meta, Alphabet, Salesforce, and Twitter, have announced similar moves in recent weeks. Microsoft, based in Redmond, Washington, had 221,000 full-time employees as of june 30, 2022, according to government filings.</PARA>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 7\n",
    "english_article = dataset_train[dataset_train['language'] == 'EN'].iloc[row].content\n",
    "english_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c69a2b-6fc9-4737-b6d2-149da2c8a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sections(content):\n",
    "    parts = re.split(r'<PARA>|</PARA>', content)\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        return parts[0], \"\", \"\"\n",
    "    elif len(parts) == 2:\n",
    "        return parts[0], parts[1], \"\"\n",
    "    else:\n",
    "        header = parts[0]\n",
    "        footer = parts[-1]\n",
    "        body = \" \".join(parts[1:-1])\n",
    "        return header, body, footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0823cf4d-34c7-4acb-ae66-2af41612c172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header:  UN chief Warns of global economic crisis at world economic forum in Davos\n",
      "\n",
      "\n",
      "\n",
      "Body:  United Nations secretary-general Antonio Guterres offered a grim outlook on the global economic during the World Economic Forum’s 2023 summit in Davos, Switzerland, warning of a possible worldwide economic downturn. citing the “perfect storm” of the Russia–Ukraine war, high food prices, and high energy costs, Guterres said that there will be “huge economic consequences” around the world. he did not predict, however, when the slowdown would start. Guterres noted that economies outside the United States, europe, and asia would face worsening economic conditions. developing countries in africa, south america, and much of asia are seeing rising interest rates, he noted. “interest rates are going up extremely in the global south. countries are close to default. they have no resources because they couldn’t print money like the United States did, like europe did during the covid-19 [pandemic],” the UN chief told Bartiromo. “they have no access to concessional funding because many of them are middle-income countries. look at small island developing states, the caribbean islands.” some of those countries, he added, “lived on tourism” and “tourism has ended for two years, but as they are middle-income countries, they had no debt to live, they had no concessional funding.” Guterres said that the United Nations is proposing a global stimulus plan to address economic woes, and claimed that economies should transition from oil and gas to other energy sources. “we need to do that in a just way,” he said, adding that “this transition needs to be well-managed.” meanwhile, the west and China need to have “serious” negotiations amid the dim global economic outlook, said the UN chief. other warningswhile inflation has eased at the consumer and producer level in the United States in recent months, a number of economists and business leaders say that americans should prepare for a recession starting in 2023. The Federal Reserve has raised interest rates to their highest levels in decades in order to cool high inflation that reached more than 9 percent in june 2022, though it has since dropped to below 7 percent. “some economists argue that the strength of the labor market—as well as household balance sheets—will keep the economy strong enough to avoid a recession,” wrote Lakshman Achuthan and Anirvan Banerji, co-founders of the Economic Cycle Research Institute, for CNN’s website. “we disagree,” they wrote, saying that “it remains our expectation that the U.S. economy will enter a recession this year.” “i’m confident that Microsoft will emerge from this stronger and more competitive,” Microsoft ceo Satya Nadella told employees in a memo on company’s website on wednesday.\n",
      "\n",
      "\n",
      "\n",
      "Footer:  other tech firms, such as Amazon, Meta, Alphabet, Salesforce, and Twitter, have announced similar moves in recent weeks. Microsoft, based in Redmond, Washington, had 221,000 full-time employees as of june 30, 2022, according to government filings.\n"
     ]
    }
   ],
   "source": [
    "header, body, footer = split_into_sections(english_article)\n",
    "print(\"Header: \", header)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Body: \", body)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Footer: \", footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a0f7d-26f4-49a6-b8d4-eb713c57b589",
   "metadata": {},
   "source": [
    "Here, `EmbeddingUtils` will help us generate embeddings for text. The main task here is to handle long paragraphs that might exceed the model's token limit. If a paragraph is too long, it is split into smaller chunks, each of which is encoded separately. \n",
    "\n",
    "Once we have embeddings for all chunks, they are combined using `sum` aggregation strtategy (proved to be the best) to get a single vector that represents the entire paragraph. This approach ensures we can handle long texts while preserving the meaningful content in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a40837-0793-43c3-a401-a33df693eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class EmbeddingUtils:\n",
    "    def __init__(self, model, tokenizer, max_length, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "    def _split_long_paragraph_and_get_embeddings(self, text, encode_fn, strategy=\"sum\"):\n",
    "        \"\"\"\n",
    "        Splits a long paragraph into chunks if it exceeds max_length in tokens,\n",
    "        calls the provided encoding function on each chunk, then aggregates.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        # We'll do a naive approach: check token length, if too big -> chunk it.\n",
    "        while True:\n",
    "            # Tokenize\n",
    "            tokens = self.tokenizer(\n",
    "                text, truncation=False, return_tensors=\"pt\", add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            # Ensure token tensors are on the correct device\n",
    "            tokens = {key: value.to(self.device) for key, value in tokens.items()}\n",
    "            num_tokens = tokens[\"input_ids\"].shape[1]\n",
    "\n",
    "            # If it fits in one chunk, just encode and break\n",
    "            if num_tokens <= self.max_length:\n",
    "                emb = encode_fn(text)\n",
    "                embeddings.append(emb)\n",
    "                break\n",
    "            else:\n",
    "                # If it doesn't fit, let's do a naive split ~ in half by chars\n",
    "                split_index = len(text) // 2\n",
    "                chunk = text[:split_index]\n",
    "                emb = encode_fn(chunk)\n",
    "                embeddings.append(emb)\n",
    "\n",
    "                text = text[split_index:].strip()\n",
    "\n",
    "        aggregated_emb = self._aggregate_embeddings(embeddings, strategy=strategy)\n",
    "        return aggregated_emb\n",
    "\n",
    "    def _aggregate_embeddings(self, embedding_list, strategy=\"sum\"):\n",
    "        \"\"\"\n",
    "        Combine multiple chunk embeddings into a single vector.\n",
    "        \"\"\"\n",
    "        if not embedding_list:\n",
    "            print('[WARNING] Embedding list was empty')\n",
    "            return None\n",
    "\n",
    "        # Stack them on the same device\n",
    "        stacked = torch.stack([emb.to(self.device) for emb in embedding_list], dim=0)\n",
    "\n",
    "        if strategy == \"mean\":\n",
    "            agg = stacked.mean(dim=0)\n",
    "        elif strategy == \"sum\":\n",
    "            agg = stacked.sum(dim=0)\n",
    "        elif strategy == \"concat\":\n",
    "            agg = torch.cat(embedding_list, dim=0)\n",
    "        elif strategy == \"rms\":\n",
    "            squares = stacked**2\n",
    "            agg = torch.sqrt(squares.mean(dim=0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "        return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a4483-cff6-4b21-ba61-ced32f084838",
   "metadata": {},
   "source": [
    "We also define the `KALMEmbeddingProcessor` class, which is responsible for generating embeddings for a given text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5a9c567-2943-4814-939d-f899e5289cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KALMEmbeddingProcessor:\n",
    "    def __init__(self, model, max_length=512, device='cpu'):\n",
    "        self.model = model\n",
    "        self.model.max_seq_length = max_length\n",
    "        self.device = device\n",
    "        self.instruction = \"Produce an embedding useful for detecting relevant war- or climate-related narratives from a taxonomy.\"\n",
    "        self.utils = EmbeddingUtils(self.model, self.model.tokenizer, self.model.max_seq_length, self.device)\n",
    "        print(f\"Max length is set to {self.model.max_seq_length}.\")\n",
    "        print(\"Using device\", device)\n",
    "\n",
    "    def _encode(self, sentence):\n",
    "        text_to_encode = f\"Instruct: {self.instruction}\\nQuery: {sentence}\"\n",
    "\n",
    "        embedding = self.model.encode(\n",
    "            text_to_encode,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=False,\n",
    "            show_progress_bar=False,\n",
    "            device=self.device\n",
    "        )\n",
    "        return embedding\n",
    "\n",
    "    def get_embeddings(self, content, strategy=\"mean\"):\n",
    "        \"\"\"\n",
    "        Main method that splits into header, body, footer, applies chunking,\n",
    "        and aggregates into a single doc embedding.\n",
    "        \"\"\"\n",
    "        header, body, footer = split_into_sections(content)\n",
    "\n",
    "        section_embs = []\n",
    "\n",
    "        # 1) Header\n",
    "        if header:\n",
    "            emb = self.utils._split_long_paragraph_and_get_embeddings(header, self._encode, strategy=\"sum\")\n",
    "            if emb is not None:\n",
    "                section_embs.append(emb)\n",
    "\n",
    "        # 2) Body\n",
    "        if body:\n",
    "            emb = self.utils._split_long_paragraph_and_get_embeddings(body, self._encode, strategy=\"sum\")\n",
    "            if emb is not None:\n",
    "                section_embs.append(emb)\n",
    "\n",
    "        # 3) Footer\n",
    "        if footer:\n",
    "            emb = self.utils._split_long_paragraph_and_get_embeddings(footer, self._encode, strategy=\"sum\")\n",
    "            if emb is not None:\n",
    "                section_embs.append(emb)\n",
    "\n",
    "        if not header and not body and not footer:\n",
    "            print(\"[WARNING] Empty article or no sections found\")\n",
    "            return None\n",
    "\n",
    "        final_emb = self.utils._aggregate_embeddings(section_embs, strategy=strategy)\n",
    "        if final_emb is None:\n",
    "            print(\"[WARNING] Failed to aggregate embeddings\")\n",
    "            return None\n",
    "\n",
    "        final_emb_np = final_emb.detach().cpu().numpy()\n",
    "        return final_emb_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e7241a-5602-454a-a027-1910831eb063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is set to 512.\n",
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "processor_kalm = KALMEmbeddingProcessor(model=kalm_model, max_length=kalm_max_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "861a58af-384d-4319-960f-a2bf89cedd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = base_save_folder_dir\n",
    "embedding_train_file_name = 'embeddings_train_kalm.npy'\n",
    "embeddings_train_full_path = embeddings_dir + 'Embeddings/' + embedding_train_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5303c56b-be05-4bad-a18d-55c8fb3c7821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../saved/Embeddings/embeddings_train_kalm.npy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d57e0bd-066b-4374-b917-6f0edc27d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_val_file_name = 'embeddings_val_kalm.npy'\n",
    "embeddings_val_full_path = embeddings_dir + 'Embeddings/' + embedding_val_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3603becc-516c-4af4-80af-b7c38191cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../saved/Embeddings/embeddings_val_kalm.npy'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_val_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e2b2f5e-4eff-47bb-8687-d0b710057ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def are_embeddings_saved(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        return True\n",
    "    print(\"Embeddings not computed, computing..\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62e58ae3-7306-43eb-a58a-491e6f4cc636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings not computed, computing..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_embeddings_saved(embeddings_val_full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac9e23-a2d6-4b46-8425-42d390decf5a",
   "metadata": {},
   "source": [
    "Then, we pre-compute and then save the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e7b5afa-9ed3-4dfa-b972-e992d006c6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76748d5f-2db7-485d-9415-2ec5c15a2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings not computed, computing..\n",
      "Embeddings saved to ../saved/Embeddings/embeddings_train_kalm.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precompute_embeddings(dataset, model, file_path):\n",
    "    embeddings = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        content = row['content']\n",
    "        embedding = processor_kalm.get_embeddings(content, strategy=\"sum\")\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    np.save(file_path, embeddings_array)\n",
    "    print(f\"Embeddings saved to {file_path}\")\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "if not are_embeddings_saved(embeddings_train_full_path): precompute_embeddings(dataset_train, processor_kalm, embeddings_train_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c422f22b-f0b5-4b36-91f9-81c1fc5dcaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings not computed, computing..\n",
      "Embeddings saved to ../saved/Embeddings/embeddings_val_kalm.npy\n"
     ]
    }
   ],
   "source": [
    "if not are_embeddings_saved(embeddings_val_full_path): precompute_embeddings(dataset_val, processor_kalm, embeddings_val_full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
