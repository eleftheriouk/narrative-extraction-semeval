{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852c56be-3ebf-4f40-bf4e-53898a78ec63",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03220268-ecc3-4bf0-a8fb-cdf6413a32c7",
   "metadata": {},
   "source": [
    "## Loss Weighting by Language\n",
    "\n",
    "As of now,  we trained a model in 5 different languages just so that we can face the problem of having limited data. Our final submission is going to be in one of those languages. \n",
    "This is our current target right now, to make our model somewhat focus on a specified language.\n",
    "\n",
    "One way to account for that is to add an extra penalty in our current loss. We know the language of each training sample, so we can double or triple the loss for that sample, in a way to tell the model to pay more attention to it.\n",
    "\n",
    "This can help improve performance in a target language, especially when having limited data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73c4c6-7246-4b25-b2c6-7a317f5d25da",
   "metadata": {},
   "source": [
    "We go ahead and do the boring stuff again by loading our pre-saved components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c0bbce-4fbd-4523-9d4d-ef46b14fef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"../../\"\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b419f7b3-706c-44e4-b9ee-20e72537b577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 1, 0, 0], [0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 1, 0, 0], [0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 0, 1, 0, 0], [0, 0, 1], [0, 0, 0, 1], [0,...  \n",
       "1  [[0, 0, 1, 0, 0], [0, 0, 1], [0, 0, 0, 1], [0,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b611b9-f983-4724-9d7d-d46ff3688713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd076469-7d41-447d-8b2a-01ffc403b0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "BG    401\n",
       "PT    400\n",
       "EN    399\n",
       "HI    366\n",
       "RU    215\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91929ad6-26e8-4adb-b814-d2f7b5653a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bc9d85-cdc2-4520-9c2b-ac8dc9213849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URW: Discrediting Ukraine': ['Discrediting Ukrainian nation and society',\n",
       "  'Discrediting Ukrainian government and officials and policies',\n",
       "  'Discrediting Ukrainian military',\n",
       "  'Ukraine is associated with nazism',\n",
       "  'Ukraine is a hub for criminal activities',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Other',\n",
       "  'Rewriting Ukraine’s history',\n",
       "  'Ukraine is a puppet of the West'],\n",
       " 'URW: Discrediting the West, Diplomacy': ['The West is overreacting',\n",
       "  'The West is weak',\n",
       "  'The EU is divided',\n",
       "  'Other',\n",
       "  'West is tired of Ukraine',\n",
       "  'Diplomacy does/will not work',\n",
       "  'The West does not care about Ukraine, only about its interests'],\n",
       " 'URW: Praise of Russia': ['Praise of Russian military might',\n",
       "  'Praise of Russian President Vladimir Putin',\n",
       "  'Russian invasion has strong national support',\n",
       "  'Russia is a guarantor of peace and prosperity',\n",
       "  'Other',\n",
       "  'Russia has international support from a number of countries and people'],\n",
       " 'URW: Russia is the Victim': ['UA is anti-RU extremists',\n",
       "  'The West is russophobic',\n",
       "  'Russia actions in Ukraine are only self-defence',\n",
       "  'Other'],\n",
       " 'URW: Distrust towards Media': ['Ukrainian media cannot be trusted',\n",
       "  'Western media is an instrument of propaganda',\n",
       "  'Other'],\n",
       " 'URW: Amplifying war-related fears': ['By continuing the war we risk WWIII',\n",
       "  'There is a real possibility that nuclear weapons will be employed',\n",
       "  'Other',\n",
       "  'Russia will also attack other countries',\n",
       "  'NATO should/will directly intervene'],\n",
       " 'URW: Blaming the war on others rather than the invader': ['The West are the aggressors',\n",
       "  'Ukraine is the aggressor',\n",
       "  'Other'],\n",
       " 'URW: Overpraising the West': ['The West has the strongest international support',\n",
       "  'NATO will destroy Russia',\n",
       "  'The West belongs in the right side of history',\n",
       "  'Other'],\n",
       " 'URW: Speculating war outcomes': ['Russian army will lose all the occupied territories',\n",
       "  'Russian army is collapsing',\n",
       "  'Other',\n",
       "  'Ukrainian army is collapsing'],\n",
       " 'URW: Hidden plots by secret schemes of powerful groups': ['Other'],\n",
       " 'Other': ['Other'],\n",
       " 'URW: Negative Consequences for the West': ['Sanctions imposed by Western countries will backfire',\n",
       "  'The conflict will increase the Ukrainian refugee flows to Europe',\n",
       "  'Other'],\n",
       " 'CC: Amplifying Climate Fears': ['Amplifying existing fears of global warming',\n",
       "  'Whatever we do it is already too late',\n",
       "  'Other',\n",
       "  'Doomsday scenarios for humans',\n",
       "  'Earth will be uninhabitable soon'],\n",
       " 'CC: Criticism of institutions and authorities': ['Criticism of international entities',\n",
       "  'Criticism of national governments',\n",
       "  'Criticism of political organizations and figures',\n",
       "  'Criticism of the EU',\n",
       "  'Other'],\n",
       " 'CC: Criticism of climate movement': ['Climate movement is alarmist',\n",
       "  'Climate movement is corrupt',\n",
       "  'Ad hominem attacks on key activists',\n",
       "  'Other'],\n",
       " 'CC: Downplaying climate change': ['Ice is not melting',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Climate cycles are natural',\n",
       "  'Temperature increase does not have significant impact',\n",
       "  'Other',\n",
       "  'CO2 concentrations are too small to have an impact',\n",
       "  'Human activities do not impact climate change',\n",
       "  'Sea levels are not rising',\n",
       "  'Humans and nature will adapt to the changes'],\n",
       " 'CC: Criticism of climate policies': ['Climate policies are ineffective',\n",
       "  'Climate policies are only for profit',\n",
       "  'Climate policies have negative impact on the economy',\n",
       "  'Other'],\n",
       " 'CC: Questioning the measurements and science': ['Scientific community is unreliable',\n",
       "  'Methodologies/metrics used are unreliable/faulty',\n",
       "  'Other',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Data shows no temperature increase'],\n",
       " 'CC: Hidden plots by secret schemes of powerful groups': ['Climate agenda has hidden motives',\n",
       "  'Blaming global elites',\n",
       "  'Other'],\n",
       " 'CC: Climate change is beneficial': ['Temperature increase is beneficial',\n",
       "  'CO2 is beneficial',\n",
       "  'Other'],\n",
       " 'CC: Controversy about green technologies': ['Renewable energy is costly',\n",
       "  'Renewable energy is dangerous',\n",
       "  'Renewable energy is unreliable',\n",
       "  'Other'],\n",
       " 'CC: Green policies are geopolitical instruments': ['Climate-related international relations are abusive/exploitative',\n",
       "  'Green activities are a form of neo-colonialism',\n",
       "  'Other']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024abfe7-45eb-49d5-82d7-5d19262c1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ed1cc-3faf-4fbe-bc62-912ad4e29a46",
   "metadata": {},
   "source": [
    "We will be using `Stella` embeddings, as they have proved quite better than `KaLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5a3905-3ebe-4e0f-82de-b86ef2b1494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_train_kalm.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "train_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34d7b4f-af40-48e9-bed1-87c97b8a0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f91fbc-c03c-41df-a9de-97e28e1f9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_val_kalm.npy')\n",
    "\n",
    "val_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721ab153-b4d6-4dba-970d-66b1136f18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_and_embeddings(dataset, embeddings, condition_fn):\n",
    "    filtered_indices = dataset.index[dataset.apply(condition_fn, axis=1)].tolist()\n",
    "    \n",
    "    filtered_dataset = dataset.loc[filtered_indices]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "\n",
    "    return filtered_dataset, filtered_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8901a79f-c8ed-4dee-87f6-6042cb8f9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, val_embeddings = filter_dataset_and_embeddings(\n",
    "    dataset_val,\n",
    "    val_embeddings, \n",
    "    lambda row: row[\"language\"] == \"EN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145aee8-f5ec-4241-ab2f-b85513af4e95",
   "metadata": {},
   "source": [
    "Also, notice that we are getting validation data for English, since that is our target.\n",
    "\n",
    "However, we also have a some-what large amount of data hanging there, that we could take advantage in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f73db6-f69a-43fc-ba14-9079a72144ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val_non_en, val_embeddings_non_en = filter_dataset_and_embeddings(\n",
    "    dataset_val,\n",
    "    val_embeddings,\n",
    "    lambda row: row[\"language\"] != \"EN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cea1329-20c0-4c3e-ba13-1a8e02bf7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_shuffling(data, embeddings, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    shuffled_indices = np.arange(len(data))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "    embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    return data, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03616b-db42-4988-b259-387063cb8762",
   "metadata": {},
   "source": [
    "For training, we concat the non-English validation data with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6ee9ad-b4c5-423c-96fa-c1495b25ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.concat([\n",
    "    dataset_train,  # All language training\n",
    "    dataset_val_non_en  # Adding non-English validation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c83a32-1b53-45fa-ab24-69c381214fe9",
   "metadata": {},
   "source": [
    "We do the same for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0720e35b-9efe-4a9e-a76e-15bfde4c888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.concatenate([\n",
    "    train_embeddings,\n",
    "    val_embeddings_non_en\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9468f4-4223-4b51-896c-5d43b23d3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, train_embeddings = custom_shuffling(dataset_train, train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e39118c-3fb5-4f06-8809-520391c012f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, val_embeddings = custom_shuffling(dataset_val, val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020b2582-5702-44ae-9731-3dd5cbbee0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a09452c4-fbe0-42fe-9a5c-55cdd7d5ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{13: [22, 20, 21, 66, 64, 50, 33, 39, 65], 14: [58, 60, 53, 33, 71, 19, 56], 19: [35, 34, 46, 42, 33, 41], 20: [63, 59, 40, 33], 15: [69, 72, 33], 11: [3, 62, 33, 43, 31], 12: [54, 67, 33], 18: [57, 32, 55, 33], 21: [45, 44, 33, 68], 16: [33], 10: [33], 17: [47, 61, 33], 0: [1, 73, 33, 23, 24], 5: [14, 15, 16, 17, 33], 3: [8, 9, 0, 33], 6: [29, 70, 7, 51, 33, 4, 27, 49, 28], 4: [10, 11, 12, 33], 9: [48, 30, 33, 26, 18], 8: [6, 2, 33], 1: [52, 5, 33], 2: [36, 37, 38, 33], 7: [13, 25, 33]}\n"
     ]
    }
   ],
   "source": [
    "narrative_to_sub_map = {}\n",
    "narrative_classes = list(mlb_narratives.classes_)\n",
    "subnarrative_classes = list(mlb_subnarratives.classes_)\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    narrative_idx = narrative_classes.index(narrative)\n",
    "    subnarrative_indices = [subnarrative_classes.index(sub) for sub in subnarratives]\n",
    "    narrative_to_sub_map[narrative_idx] = subnarrative_indices\n",
    "\n",
    "print(narrative_to_sub_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f98f1d4-13c4-40c6-8926-cf87c94f9e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_order = sorted(narrative_to_sub_map.keys())\n",
    "narrative_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb9a13a1-917f-47ca-b979-d21b3b7bf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e69f8235-7d2f-475e-9876-5166a3ced91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410f575d-85e8-4d53-ba94-6b7e48b8a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63fa1546-578e-49a2-ac03-2b06c1d2fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 1024,\n",
    "    'dropout': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7664898e-69c8-4f74-99f3-9fabf247a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "y_val_nar = dataset_val['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n",
    "y_val_sub_nar = dataset_val['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74494f26-ae72-4a72-9ec9-bfab3d9b67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32)\n",
    "\n",
    "y_val_nar = torch.tensor(y_val_nar, dtype=torch.float32)\n",
    "y_val_sub_nar = torch.tensor(y_val_sub_nar, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46fcaf5e-dee1-488d-8c79-f7c2f3d618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89bc1f13-5799-400f-b126-5cfe6fdc4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)\n",
    "\n",
    "class_weights_sub_nar = compute_class_weights(y_val_sub_nar)\n",
    "class_weights_nar = compute_class_weights(y_val_nar)\n",
    "narrative_criterion = WeightedBCELoss(class_weights_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc839d6b-f588-436a-b5f0-464093ce986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_criterion_dict = {}\n",
    "\n",
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    local_weights = [ class_weights_sub_nar[sub_i] for sub_i in sub_indices ]\n",
    "\n",
    "    sub_criterion = WeightedBCELoss(local_weights)\n",
    "    sub_criterion_dict[str(narr_idx)] = sub_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c44cc-cfb6-4c8f-80fe-df360cf0a659",
   "metadata": {},
   "source": [
    "We will also select the MultiHeadConcat Model, since this is the one appearing to do the best for our Fine-F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e92f161-370b-409e-bea0-4dc59a28d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassifierMultiHeadConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout']\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2 + 1, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            conditioned_input = torch.cat((shared_out, narr_probs[:, int(narr_idx)].unsqueeze(1)), dim=1)\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50b83196-b7fc-46ce-9e90-3cad23e1bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskClassifierMultiHeadConcat(\n",
    "    input_size=input_size,\n",
    "    hidden_size=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615aed4d-898f-43e7-a100-e8e6198c9add",
   "metadata": {},
   "source": [
    "We add the extra penalty for english samples\n",
    "\n",
    "* In the forwarding step, we check if the sample is an english one, and if it is we apply the extra weight upon the loss to essentially tell them model to pay more attention to those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db85e674-51d2-4c47-9983-41410a48e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAwareMultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=0.8, sub_weight=0.5,\n",
    "                 english_weight=3.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        self.english_weight = english_weight\n",
    "\n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads, is_english):\n",
    "        is_english = is_english.to(narr_probs.device)\n",
    "        sample_weights = torch.where(is_english == 1, self.english_weight, 1.0)\n",
    "        \n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        narr_loss = (narr_loss * sample_weights.unsqueeze(1)).mean()\n",
    "\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "\n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_batch_loss = sub_loss_func(sub_probs, y_sub_tensor)\n",
    "            sub_loss += (sub_batch_loss * sample_weights).mean()\n",
    "\n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.abs(sub_probs * (1 - narr_pred)) + \\\n",
    "                             narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            condition_term = (condition_term * sample_weights.unsqueeze(1)).mean()\n",
    "            condition_loss += condition_term\n",
    "\n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + self.sub_weight * sub_loss + \\\n",
    "                     self.condition_weight * condition_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "818232af-9ada-48e2-bffc-a8c0b170c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_english_train = torch.tensor([1 if lang == 'EN' else 0 for lang in dataset_train['language']], \n",
    "                              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcdeba57-1029-407e-8f6c-f8f245e8bb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_english_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdb1e2a9-7369-449b-9195-1c30ef431fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_aware_loss = LanguageAwareMultiHeadLoss(\n",
    "    narrative_criterion=narrative_criterion,\n",
    "    sub_criterion_dict=sub_criterion_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f8b9c35-d86e-4a69-8bc1-616c9f9406dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn=language_aware_loss,\n",
    "    train_embeddings=train_embeddings_tensor,\n",
    "    y_train_nar=y_train_nar,\n",
    "    y_train_sub_heads=y_train_sub_heads,\n",
    "    val_embeddings=val_embeddings_tensor,\n",
    "    y_val_nar=y_val_nar,\n",
    "    y_val_sub_heads=y_val_sub_heads,\n",
    "    is_english_train=is_english_train, \n",
    "    patience=5,\n",
    "    num_epochs=100,\n",
    "    scheduler=None,\n",
    "    min_delta=0.001\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "\n",
    "        train_loss = loss_fn(\n",
    "            train_narr_probs, \n",
    "            train_sub_probs_dict, \n",
    "            y_train_nar, \n",
    "            y_train_sub_heads,\n",
    "            is_english_train\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            is_english_val = torch.ones(len(val_embeddings), device=val_embeddings.device)\n",
    "            val_loss = loss_fn(\n",
    "                val_narr_probs, \n",
    "                val_sub_probs_dict, \n",
    "                y_val_nar, \n",
    "                y_val_sub_heads,\n",
    "                is_english_val\n",
    "            )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {train_loss.item():.4f}, \"\n",
    "              f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if val_loss.item() < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not significantly improve for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f52b84b-04bf-411b-9835-59e027f833e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def initialize_and_train_model(\n",
    "    model,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=10,\n",
    "    use_scheduler=True,\n",
    "    scheduler_patience=3,\n",
    "    loss_fn=language_aware_loss,\n",
    "    num_subnarratives=len(mlb_subnarratives.classes_),\n",
    "    device='cpu'\n",
    "):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=scheduler_patience)\n",
    "\n",
    "    trained_model = train_with_multihead(\n",
    "                                    model=model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    patience=patience\n",
    "                                )\n",
    "    return trained_model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87eff7a5-59f7-4d8e-9d82-5adae872b486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.5139, Validation Loss: 3.0056\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Training Loss: 0.9622, Validation Loss: 2.9546\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Training Loss: 0.7996, Validation Loss: 2.9209\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Training Loss: 0.7270, Validation Loss: 2.8920\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Training Loss: 0.6822, Validation Loss: 2.8605\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Training Loss: 0.6502, Validation Loss: 2.8236\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Training Loss: 0.6230, Validation Loss: 2.7816\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Training Loss: 0.5999, Validation Loss: 2.7377\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Training Loss: 0.5765, Validation Loss: 2.6946\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Training Loss: 0.5600, Validation Loss: 2.6541\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Training Loss: 0.5464, Validation Loss: 2.6152\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Training Loss: 0.5340, Validation Loss: 2.5759\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Training Loss: 0.5236, Validation Loss: 2.5341\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Training Loss: 0.5133, Validation Loss: 2.4899\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Training Loss: 0.5032, Validation Loss: 2.4430\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Training Loss: 0.4918, Validation Loss: 2.3948\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Training Loss: 0.4821, Validation Loss: 2.3470\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Training Loss: 0.4732, Validation Loss: 2.3019\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Training Loss: 0.4640, Validation Loss: 2.2605\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Training Loss: 0.4559, Validation Loss: 2.2230\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Training Loss: 0.4486, Validation Loss: 2.1887\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Training Loss: 0.4412, Validation Loss: 2.1565\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Training Loss: 0.4338, Validation Loss: 2.1247\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Training Loss: 0.4259, Validation Loss: 2.0934\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Training Loss: 0.4202, Validation Loss: 2.0630\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Training Loss: 0.4130, Validation Loss: 2.0346\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Training Loss: 0.4054, Validation Loss: 2.0090\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Training Loss: 0.3991, Validation Loss: 1.9862\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Training Loss: 0.3924, Validation Loss: 1.9657\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Training Loss: 0.3872, Validation Loss: 1.9467\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Training Loss: 0.3812, Validation Loss: 1.9292\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Training Loss: 0.3747, Validation Loss: 1.9166\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Training Loss: 0.3695, Validation Loss: 1.9091\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Training Loss: 0.3644, Validation Loss: 1.9051\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Training Loss: 0.3573, Validation Loss: 1.9041\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Training Loss: 0.3529, Validation Loss: 1.9054\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 37/100, Training Loss: 0.3465, Validation Loss: 1.9089\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 38/100, Training Loss: 0.3402, Validation Loss: 1.9097\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 39/100, Training Loss: 0.3363, Validation Loss: 1.9080\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Epoch 40/100, Training Loss: 0.3312, Validation Loss: 1.9056\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "trained_model, optimizer, scheduler = initialize_and_train_model(\n",
    "    model,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0221d707-9320-4c0f-b25e-98e109cb9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_classes = sorted(narrative_to_subnarratives.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "550fa66e-d415-461d-9377-df45a0cbdf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears',\n",
       " 'CC: Climate change is beneficial',\n",
       " 'CC: Controversy about green technologies',\n",
       " 'CC: Criticism of climate movement',\n",
       " 'CC: Criticism of climate policies',\n",
       " 'CC: Criticism of institutions and authorities',\n",
       " 'CC: Downplaying climate change',\n",
       " 'CC: Green policies are geopolitical instruments',\n",
       " 'CC: Hidden plots by secret schemes of powerful groups',\n",
       " 'CC: Questioning the measurements and science',\n",
       " 'Other',\n",
       " 'URW: Amplifying war-related fears',\n",
       " 'URW: Blaming the war on others rather than the invader',\n",
       " 'URW: Discrediting Ukraine',\n",
       " 'URW: Discrediting the West, Diplomacy',\n",
       " 'URW: Distrust towards Media',\n",
       " 'URW: Hidden plots by secret schemes of powerful groups',\n",
       " 'URW: Negative Consequences for the West',\n",
       " 'URW: Overpraising the West',\n",
       " 'URW: Praise of Russia',\n",
       " 'URW: Russia is the Victim',\n",
       " 'URW: Speculating war outcomes']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "713c2c13-cbd4-4c5b-8008-922dd00bc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_label_set = set()\n",
    "\n",
    "for narrative, subnarratives in narrative_to_subnarratives.items():\n",
    "    if narrative == \"Other\":\n",
    "        fine_label_set.add(\"Other\")\n",
    "    else:\n",
    "        for sub in subnarratives:\n",
    "            if sub == \"Other\":\n",
    "                fine_label_set.add(f\"{narrative}: Other\")\n",
    "            else:\n",
    "                fine_label_set.add(f\"{narrative}: {sub}\")\n",
    "\n",
    "fine_classes = sorted(fine_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "435caf0b-cbbe-4130-8782-28a408d3c9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears: Amplifying existing fears of global warming',\n",
       " 'CC: Amplifying Climate Fears: Doomsday scenarios for humans',\n",
       " 'CC: Amplifying Climate Fears: Earth will be uninhabitable soon',\n",
       " 'CC: Amplifying Climate Fears: Other',\n",
       " 'CC: Amplifying Climate Fears: Whatever we do it is already too late',\n",
       " 'CC: Climate change is beneficial: CO2 is beneficial',\n",
       " 'CC: Climate change is beneficial: Other',\n",
       " 'CC: Climate change is beneficial: Temperature increase is beneficial',\n",
       " 'CC: Controversy about green technologies: Other',\n",
       " 'CC: Controversy about green technologies: Renewable energy is costly',\n",
       " 'CC: Controversy about green technologies: Renewable energy is dangerous',\n",
       " 'CC: Controversy about green technologies: Renewable energy is unreliable',\n",
       " 'CC: Criticism of climate movement: Ad hominem attacks on key activists',\n",
       " 'CC: Criticism of climate movement: Climate movement is alarmist',\n",
       " 'CC: Criticism of climate movement: Climate movement is corrupt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "087519a2-98bb-425b-b836-8b093854684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from sklearn import metrics\n",
    "\n",
    "class MultiHeadEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes_coarse=coarse_classes,\n",
    "        classes_fine=fine_classes,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu',\n",
    "        output_dir='../../../submissions',\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.narrative_classes = list(narrative_classes)\n",
    "        self.subnarrative_classes = list(subnarrative_classes)\n",
    "        \n",
    "        self.classes_coarse = classes_coarse\n",
    "        self.classes_fine = classes_fine\n",
    "\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings=val_embeddings_tensor,\n",
    "        dataset=dataset_val,\n",
    "        thresholds=None,\n",
    "        save=False,\n",
    "        std_weight=0.6,\n",
    "        lower_thres=0.1,\n",
    "        upper_thres=0.55\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(lower_thres, upper_thres, 0.05)    \n",
    "        embeddings = embeddings.to(self.device)\n",
    "    \n",
    "        best_results = {\n",
    "            'best_coarse_f1': -1,\n",
    "            'best_coarse_std': float('inf'),\n",
    "            'best_fine_f1': -1,\n",
    "            'best_fine_std': float('inf'),\n",
    "            'narr_threshold': 0,\n",
    "            'sub_threshold': 0,\n",
    "            'predictions': None,\n",
    "            'best_combined_score': -float('inf')\n",
    "        }\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "    \n",
    "        for narr_threshold in thresholds:\n",
    "            for sub_threshold in thresholds:\n",
    "                predictions = []\n",
    "                for sample_idx, row in dataset.iterrows():\n",
    "                    pred = self._make_prediction(\n",
    "                        row['article_id'],\n",
    "                        sample_idx,\n",
    "                        narr_probs,\n",
    "                        sub_probs_dict,\n",
    "                        narr_threshold,\n",
    "                        sub_threshold\n",
    "                    )\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                f1_coarse_mean, coarse_std, f1_fine_mean, fine_std = self._compute_metrics_coarse_fine(\n",
    "                    predictions,\n",
    "                    dataset\n",
    "                )\n",
    "                \n",
    "                combined_score = f1_fine_mean - (std_weight * coarse_std)\n",
    "                \n",
    "                if combined_score > best_results['best_combined_score']:\n",
    "                    best_results.update({\n",
    "                        'best_coarse_f1': f1_coarse_mean,\n",
    "                        'best_coarse_std': coarse_std,\n",
    "                        'best_fine_f1': f1_fine_mean,\n",
    "                        'best_fine_std': fine_std,\n",
    "                        'narr_threshold': narr_threshold,\n",
    "                        'sub_threshold': sub_threshold,\n",
    "                        'predictions': predictions,\n",
    "                        'best_combined_score': combined_score\n",
    "                    })\n",
    "    \n",
    "        print(\"\\nBest thresholds found:\")\n",
    "        print(f\"Narrative threshold: {best_results['narr_threshold']:.2f}\")\n",
    "        print(f\"Subnarrative threshold: {best_results['sub_threshold']:.2f}\")\n",
    "        print('\\n')\n",
    "        print(f\"Coarse-F1: {best_results['best_coarse_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. coarse: {best_results['best_coarse_std']:.3f}\")\n",
    "        print(f\"Fine-F1: {best_results['best_fine_f1']:.3f} \")\n",
    "        print(f\"F1 st. dev. fine: {best_results['best_fine_std']:.3f}\")\n",
    "\n",
    "        if save:\n",
    "            self._save_predictions(best_results, os.path.join(self.output_dir, 'submission.txt'))\n",
    "        \n",
    "        return best_results\n",
    "\n",
    "    def _make_prediction(self, article_id, sample_idx, narr_probs, sub_probs_dict, narr_threshold, sub_threshold):\n",
    "        other_idx = self.narrative_classes.index(\"Other\")\n",
    "        # Find all narratives >= narr_threshold \n",
    "        # (except 'Other', this is going to be used as a fallback)\n",
    "        active_narratives = [\n",
    "            (n_idx, prob)\n",
    "            for n_idx, prob in enumerate(narr_probs[sample_idx])\n",
    "            if n_idx != other_idx and prob >= narr_threshold\n",
    "        ]\n",
    "        \n",
    "        if not active_narratives:\n",
    "            return {\n",
    "                'article_id': article_id,\n",
    "                'narratives': [\"Other\"],\n",
    "                'pairs': [\"Other\"]\n",
    "            }\n",
    "        \n",
    "        narratives = []\n",
    "        pairs = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        active_narratives.sort(key=lambda x: x[1], reverse=True)\n",
    "        for narr_idx, narr_prob in active_narratives:\n",
    "            narr_name = self.narrative_classes[narr_idx]\n",
    "            \n",
    "            sub_probs = sub_probs_dict[str(narr_idx)][sample_idx]\n",
    "            active_subnarratives = [\n",
    "                (local_idx, s_prob)\n",
    "                for local_idx, s_prob in enumerate(sub_probs)\n",
    "                if s_prob >= sub_threshold\n",
    "            ]\n",
    "            active_subnarratives.sort(key=lambda x: x[1], reverse=True)\n",
    "            # Fallback, when no subnarrative active, we put other.\n",
    "            if not active_subnarratives:\n",
    "                pairs.append(f\"{narr_name}: Other\")\n",
    "            else:\n",
    "                for local_idx, _ in active_subnarratives:\n",
    "                    global_sub_idx = self.narrative_to_sub_map[narr_idx][local_idx]\n",
    "                    sub_name = self.subnarrative_classes[global_sub_idx]\n",
    "                    pair = f\"{narr_name}: {sub_name}\"\n",
    "                    if pair not in seen_pairs:\n",
    "                        pairs.append(pair)\n",
    "                        seen_pairs.add(pair)\n",
    "\n",
    "            narratives.append(narr_name)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'narratives': narratives,\n",
    "            'pairs': pairs\n",
    "        }\n",
    "\n",
    "    def _compute_metrics_coarse_fine(self, predictions, dataset):\n",
    "        \"\"\"\n",
    "        Mimics the official scorere used by the challenge.\n",
    "        \"\"\"\n",
    "        gold_coarse_all = []\n",
    "        gold_fine_all = []\n",
    "        pred_coarse_all = []\n",
    "        pred_fine_all = []\n",
    "\n",
    "        for pred, (_, row) in zip(predictions, dataset.iterrows()):\n",
    "            gold_coarse = row['narratives']\n",
    "            gold_subnarratives = row['subnarratives']\n",
    "            \n",
    "            pred_coarse = pred['narratives']\n",
    "            pred_fine = []\n",
    "            for p in pred['pairs']:\n",
    "                # If previously we predicted a \"Other\",\n",
    "                # we output \"Other Other\" as the narrative and subnarrative.\n",
    "                if p == \"Other\":\n",
    "                    pred_fine.append(\"Other\")\n",
    "                else:\n",
    "                    # Takes the whole nar : sub pair.\n",
    "                    pred_fine.append(p)\n",
    "\n",
    "            gold_fine = []\n",
    "            for gold_nar, gold_sub in zip(gold_coarse, gold_subnarratives):\n",
    "                # We do the same for truths.\n",
    "                if gold_nar == \"Other\":\n",
    "                    gold_fine.append(\"Other\")\n",
    "                else:\n",
    "                    gold_fine.append(f\"{gold_nar}: {gold_sub}\")\n",
    "            \n",
    "            gold_coarse_all.append(gold_coarse)\n",
    "            gold_fine_all.append(gold_fine)\n",
    "            pred_coarse_all.append(pred_coarse)\n",
    "            pred_fine_all.append(pred_fine)\n",
    "\n",
    "        f1_coarse_mean, coarse_std = self._evaluate_multi_label(gold_coarse_all, pred_coarse_all, self.classes_coarse)\n",
    "        f1_fine_mean, fine_std = self._evaluate_multi_label(gold_fine_all, pred_fine_all, self.classes_fine)\n",
    "\n",
    "        return f1_coarse_mean, coarse_std, f1_fine_mean, fine_std\n",
    "\n",
    "    def _evaluate_multi_label(self, gold, predicted, class_list):\n",
    "        \"\"\"\n",
    "        Mimics the official f1-score calculation used by the challenge.\n",
    "        \"\"\"\n",
    "        f1_scores = []\n",
    "        for g_labels, p_labels in zip(gold, predicted):\n",
    "            g_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in class_list:\n",
    "                    g_onehot[class_list.index(lab)] = 1\n",
    "                    \n",
    "            p_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in class_list:\n",
    "                    p_onehot[class_list.index(lab)] = 1\n",
    "\n",
    "            f1_doc = metrics.f1_score(g_onehot, p_onehot, zero_division=0)\n",
    "            f1_scores.append(f1_doc)\n",
    "        \n",
    "        return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "    def _save_predictions(self, best_results, filepath):\n",
    "        predictions = best_results['predictions']\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                line = (f\"{pred['article_id']}\\t\"\n",
    "                        f\"{';'.join(pred['narratives'])}\\t\"\n",
    "                        f\"{';'.join(pred['pairs'])}\\n\")\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebe71798-314a-40ce-8724-506db53bcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultiHeadEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d1c49c5-eaf8-47ec-874a-e056b1a22de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.50\n",
      "Subnarrative threshold: 0.25\n",
      "\n",
      "\n",
      "Coarse-F1: 0.478\n",
      "F1 st. dev. coarse: 0.385\n",
      "Fine-F1: 0.322 \n",
      "F1 st. dev. fine: 0.312\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.evaluate(\n",
    "    model=trained_model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
