{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852c56be-3ebf-4f40-bf4e-53898a78ec63",
   "metadata": {},
   "source": [
    "# Semeval 2025 Task 10\n",
    "### Subtask 2: Narrative Classification\n",
    "\n",
    "Given a news article and a [two-level taxonomy of narrative labels](https://propaganda.math.unipd.it/semeval2025task10/NARRATIVE-TAXONOMIES.pdf) (where each narrative is subdivided into subnarratives) from a particular domain, assign to the article all the appropriate subnarrative labels. This is a multi-label multi-class document classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865e2b23-473a-4597-b1e6-07f9188eb7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560c281b-63af-4939-b1f2-a1784c25263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "if random_state:\n",
    "    print('[WARNING] Setting random state')\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state) \n",
    "    random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03220268-ecc3-4bf0-a8fb-cdf6413a32c7",
   "metadata": {},
   "source": [
    "## Loss Weighting by Language\n",
    "\n",
    "As of now,  we trained a model in 5 different languages just so that we can face the problem of having limited data. Our final submission is going to be in one of those languages. \n",
    "This is our current target right now, to make our model somewhat focus on a specified language.\n",
    "\n",
    "One way to account for that is to add an extra penalty in our current loss. We know the language of each training sample, so we can double or triple the loss for that sample, in a way to tell the model to pay more attention to it.\n",
    "\n",
    "This can help improve performance in a target language, especially when having limited data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73c4c6-7246-4b25-b2c6-7a317f5d25da",
   "metadata": {},
   "source": [
    "We go ahead and do the boring stuff again by loading our pre-saved components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c0bbce-4fbd-4523-9d4d-ef46b14fef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"../../\"\n",
    "base_save_folder_dir = '../saved/'\n",
    "dataset_folder = os.path.join(base_save_folder_dir, 'Dataset')\n",
    "\n",
    "with open(os.path.join(dataset_folder, 'dataset_train_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b419f7b3-706c-44e4-b9ee-20e72537b577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "      <th>narratives</th>\n",
       "      <th>subnarratives</th>\n",
       "      <th>narratives_encoded</th>\n",
       "      <th>subnarratives_encoded</th>\n",
       "      <th>aggregated_subnarratives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1161.txt</td>\n",
       "      <td>&lt;PARA&gt;в ближайшие два месяца сша будут стремит...</td>\n",
       "      <td>[URW: Blaming the war on others rather than th...</td>\n",
       "      <td>[The West are the aggressors, Other, The West ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1175.txt</td>\n",
       "      <td>&lt;PARA&gt;в ес испугались последствий популярности...</td>\n",
       "      <td>[URW: Discrediting the West, Diplomacy, URW: D...</td>\n",
       "      <td>[The West is weak, Other, The EU is divided]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1149.txt</td>\n",
       "      <td>&lt;PARA&gt;возможность признания аллы пугачевой ино...</td>\n",
       "      <td>[URW: Distrust towards Media]</td>\n",
       "      <td>[Western media is an instrument of propaganda]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1015.txt</td>\n",
       "      <td>&lt;PARA&gt;азаров рассказал о смене риторики киева ...</td>\n",
       "      <td>[URW: Discrediting Ukraine, URW: Discrediting ...</td>\n",
       "      <td>[Ukraine is a puppet of the West, Discrediting...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>RU-URW-1001.txt</td>\n",
       "      <td>&lt;PARA&gt;в россиянах проснулась массовая любовь к...</td>\n",
       "      <td>[URW: Praise of Russia]</td>\n",
       "      <td>[Russia is a guarantor of peace and prosperity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       article_id  \\\n",
       "0       RU  RU-URW-1161.txt   \n",
       "1       RU  RU-URW-1175.txt   \n",
       "2       RU  RU-URW-1149.txt   \n",
       "3       RU  RU-URW-1015.txt   \n",
       "4       RU  RU-URW-1001.txt   \n",
       "\n",
       "                                             content  \\\n",
       "0  <PARA>в ближайшие два месяца сша будут стремит...   \n",
       "1  <PARA>в ес испугались последствий популярности...   \n",
       "2  <PARA>возможность признания аллы пугачевой ино...   \n",
       "3  <PARA>азаров рассказал о смене риторики киева ...   \n",
       "4  <PARA>в россиянах проснулась массовая любовь к...   \n",
       "\n",
       "                                          narratives  \\\n",
       "0  [URW: Blaming the war on others rather than th...   \n",
       "1  [URW: Discrediting the West, Diplomacy, URW: D...   \n",
       "2                      [URW: Distrust towards Media]   \n",
       "3  [URW: Discrediting Ukraine, URW: Discrediting ...   \n",
       "4                            [URW: Praise of Russia]   \n",
       "\n",
       "                                       subnarratives  \\\n",
       "0  [The West are the aggressors, Other, The West ...   \n",
       "1       [The West is weak, Other, The EU is divided]   \n",
       "2     [Western media is an instrument of propaganda]   \n",
       "3  [Ukraine is a puppet of the West, Discrediting...   \n",
       "4    [Russia is a guarantor of peace and prosperity]   \n",
       "\n",
       "                                  narratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               subnarratives_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                            aggregated_subnarratives  \n",
       "0  [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...  \n",
       "1  [[0, 1, 0, 0, 0], [1, 0, 0], [1, 0, 0, 0], [1,...  \n",
       "2  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "3  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  \n",
       "4  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b611b9-f983-4724-9d7d-d46ff3688713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd076469-7d41-447d-8b2a-01ffc403b0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "BG    401\n",
       "PT    400\n",
       "EN    399\n",
       "HI    366\n",
       "RU    215\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91929ad6-26e8-4adb-b814-d2f7b5653a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives.pkl'), 'rb') as f:\n",
    "    narrative_to_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c80ede-623e-4e21-8f75-fc02e337c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'narrative_to_subnarratives_map.pkl'), 'rb') as f:\n",
    "    narrative_to_sub_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0e6b0b-d49e-4211-8bba-bbf58516a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(misc_folder, 'coarse_classes.pkl'), 'rb') as f:\n",
    "    coarse_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'fine_classes.pkl'), 'rb') as f:\n",
    "    fine_classes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(misc_folder, 'narrative_order.pkl'), 'rb') as f:\n",
    "    narrative_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6bc9d85-cdc2-4520-9c2b-ac8dc9213849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URW: Discrediting Ukraine': ['Discrediting Ukrainian government and officials and policies',\n",
       "  'Discrediting Ukrainian nation and society',\n",
       "  'Other',\n",
       "  'Ukraine is associated with nazism',\n",
       "  'Ukraine is a puppet of the West',\n",
       "  'Rewriting Ukraine’s history',\n",
       "  'Situation in Ukraine is hopeless',\n",
       "  'Discrediting Ukrainian military',\n",
       "  'Ukraine is a hub for criminal activities'],\n",
       " 'URW: Discrediting the West, Diplomacy': ['Diplomacy does/will not work',\n",
       "  'The EU is divided',\n",
       "  'West is tired of Ukraine',\n",
       "  'Other',\n",
       "  'The West does not care about Ukraine, only about its interests',\n",
       "  'The West is overreacting',\n",
       "  'The West is weak'],\n",
       " 'URW: Praise of Russia': ['Praise of Russian President Vladimir Putin',\n",
       "  'Russia has international support from a number of countries and people',\n",
       "  'Russia is a guarantor of peace and prosperity',\n",
       "  'Other',\n",
       "  'Russian invasion has strong national support',\n",
       "  'Praise of Russian military might'],\n",
       " 'URW: Russia is the Victim': ['Other',\n",
       "  'The West is russophobic',\n",
       "  'UA is anti-RU extremists',\n",
       "  'Russia actions in Ukraine are only self-defence'],\n",
       " 'URW: Distrust towards Media': ['Other',\n",
       "  'Ukrainian media cannot be trusted',\n",
       "  'Western media is an instrument of propaganda'],\n",
       " 'URW: Amplifying war-related fears': ['Russia will also attack other countries',\n",
       "  'Other',\n",
       "  'There is a real possibility that nuclear weapons will be employed',\n",
       "  'NATO should/will directly intervene',\n",
       "  'By continuing the war we risk WWIII'],\n",
       " 'URW: Blaming the war on others rather than the invader': ['Other',\n",
       "  'Ukraine is the aggressor',\n",
       "  'The West are the aggressors'],\n",
       " 'URW: Overpraising the West': ['Other',\n",
       "  'The West belongs in the right side of history',\n",
       "  'The West has the strongest international support',\n",
       "  'NATO will destroy Russia'],\n",
       " 'URW: Speculating war outcomes': ['Other',\n",
       "  'Russian army is collapsing',\n",
       "  'Russian army will lose all the occupied territories',\n",
       "  'Ukrainian army is collapsing'],\n",
       " 'URW: Hidden plots by secret schemes of powerful groups': ['Other'],\n",
       " 'Other': ['Other'],\n",
       " 'URW: Negative Consequences for the West': ['Other',\n",
       "  'Sanctions imposed by Western countries will backfire',\n",
       "  'The conflict will increase the Ukrainian refugee flows to Europe'],\n",
       " 'CC: Amplifying Climate Fears': ['Amplifying existing fears of global warming',\n",
       "  'Other',\n",
       "  'Earth will be uninhabitable soon',\n",
       "  'Doomsday scenarios for humans',\n",
       "  'Whatever we do it is already too late'],\n",
       " 'CC: Criticism of institutions and authorities': ['Other',\n",
       "  'Criticism of international entities',\n",
       "  'Criticism of the EU',\n",
       "  'Criticism of national governments',\n",
       "  'Criticism of political organizations and figures'],\n",
       " 'CC: Criticism of climate movement': ['Other',\n",
       "  'Climate movement is alarmist',\n",
       "  'Ad hominem attacks on key activists',\n",
       "  'Climate movement is corrupt'],\n",
       " 'CC: Downplaying climate change': ['Temperature increase does not have significant impact',\n",
       "  'Other',\n",
       "  'Human activities do not impact climate change',\n",
       "  'Sea levels are not rising',\n",
       "  'Climate cycles are natural',\n",
       "  'CO2 concentrations are too small to have an impact',\n",
       "  'Weather suggests the trend is global cooling',\n",
       "  'Ice is not melting',\n",
       "  'Humans and nature will adapt to the changes'],\n",
       " 'CC: Criticism of climate policies': ['Other',\n",
       "  'Climate policies are only for profit',\n",
       "  'Climate policies have negative impact on the economy',\n",
       "  'Climate policies are ineffective'],\n",
       " 'CC: Questioning the measurements and science': ['Data shows no temperature increase',\n",
       "  'Greenhouse effect/carbon dioxide do not drive climate change',\n",
       "  'Other',\n",
       "  'Methodologies/metrics used are unreliable/faulty',\n",
       "  'Scientific community is unreliable'],\n",
       " 'CC: Hidden plots by secret schemes of powerful groups': ['Other',\n",
       "  'Climate agenda has hidden motives',\n",
       "  'Blaming global elites'],\n",
       " 'CC: Climate change is beneficial': ['Other',\n",
       "  'CO2 is beneficial',\n",
       "  'Temperature increase is beneficial'],\n",
       " 'CC: Controversy about green technologies': ['Other',\n",
       "  'Renewable energy is costly',\n",
       "  'Renewable energy is unreliable',\n",
       "  'Renewable energy is dangerous'],\n",
       " 'CC: Green policies are geopolitical instruments': ['Other',\n",
       "  'Green activities are a form of neo-colonialism',\n",
       "  'Climate-related international relations are abusive/exploitative']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_to_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024abfe7-45eb-49d5-82d7-5d19262c1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_folder = os.path.join(base_save_folder_dir, 'LabelEncoders')\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_narratives.pkl'), 'rb') as f:\n",
    "    mlb_narratives = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(label_encoder_folder, 'mlb_subnarratives.pkl'), 'rb') as f:\n",
    "    mlb_subnarratives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ed1cc-3faf-4fbe-bc62-912ad4e29a46",
   "metadata": {},
   "source": [
    "We will be using `Stella` embeddings, as they have proved quite better than `KaLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5a3905-3ebe-4e0f-82de-b86ef2b1494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_train_kalm.npy')\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "train_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34d7b4f-af40-48e9-bed1-87c97b8a0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_folder, 'dataset_val_cleaned.pkl'), 'rb') as f:\n",
    "    dataset_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54f91fbc-c03c-41df-a9de-97e28e1f9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_folder = os.path.join(base_save_folder_dir, 'Embeddings/embeddings_val_kalm.npy')\n",
    "\n",
    "val_embeddings = load_embeddings(embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "721ab153-b4d6-4dba-970d-66b1136f18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_and_embeddings(dataset, embeddings, condition_fn):\n",
    "    filtered_indices = dataset.index[dataset.apply(condition_fn, axis=1)].tolist()\n",
    "    \n",
    "    filtered_dataset = dataset.loc[filtered_indices]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "\n",
    "    return filtered_dataset, filtered_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8901a79f-c8ed-4dee-87f6-6042cb8f9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, val_embeddings = filter_dataset_and_embeddings(\n",
    "    dataset_val,\n",
    "    val_embeddings, \n",
    "    lambda row: row[\"language\"] == \"EN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cea1329-20c0-4c3e-ba13-1a8e02bf7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_shuffling(data, embeddings):\n",
    "    shuffled_indices = np.arange(len(data))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    data = data.iloc[shuffled_indices].reset_index(drop=True)\n",
    "    embeddings = embeddings[shuffled_indices]\n",
    "\n",
    "    return data, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f9468f4-4223-4b51-896c-5d43b23d3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, train_embeddings = custom_shuffling(dataset_train, train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e39118c-3fb5-4f06-8809-520391c012f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, val_embeddings = custom_shuffling(dataset_val, val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020b2582-5702-44ae-9731-3dd5cbbee0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_folder = os.path.join(base_save_folder_dir, 'Misc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb9a13a1-917f-47ca-b979-d21b3b7bf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sub_heads = dataset_train['aggregated_subnarratives'].to_numpy()\n",
    "y_val_sub_heads = dataset_val['aggregated_subnarratives'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d6cab6-b1bc-41ed-9740-04d4d7d2efe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "prefer_cpu=True\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() and not prefer_cpu\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e69f8235-7d2f-475e-9876-5166a3ced91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410f575d-85e8-4d53-ba94-6b7e48b8a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n"
     ]
    }
   ],
   "source": [
    "input_size = train_embeddings_tensor.shape[1]\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63fa1546-578e-49a2-ac03-2b06c1d2fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "    'lr': 0.001,\n",
    "    'hidden_size': 1024,\n",
    "    'dropout': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7664898e-69c8-4f74-99f3-9fabf247a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = dataset_train['narratives_encoded'].tolist()\n",
    "y_val_nar = dataset_val['narratives_encoded'].tolist()\n",
    "\n",
    "y_train_sub_nar = dataset_train['subnarratives_encoded'].tolist()\n",
    "y_val_sub_nar = dataset_val['subnarratives_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74494f26-ae72-4a72-9ec9-bfab3d9b67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nar = torch.tensor(y_train_nar, dtype=torch.float32).to(device)\n",
    "y_train_sub_nar = torch.tensor(y_train_sub_nar, dtype=torch.float32).to(device)\n",
    "\n",
    "y_val_nar = torch.tensor(y_val_nar, dtype=torch.float32).to(device)\n",
    "y_val_sub_nar = torch.tensor(y_val_sub_nar, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46fcaf5e-dee1-488d-8c79-f7c2f3d618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89bc1f13-5799-400f-b126-5cfe6fdc4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    total_samples = y_train.shape[0]\n",
    "    class_weights = []\n",
    "    for label in range(y_train.shape[1]):\n",
    "        pos_count = y_train[:, label].sum().item()\n",
    "        neg_count = total_samples - pos_count\n",
    "        pos_weight = total_samples / (2 * pos_count) if pos_count > 0 else 0\n",
    "        neg_weight = total_samples / (2 * neg_count) if neg_count > 0 else 0\n",
    "        class_weights.append((pos_weight, neg_weight))\n",
    "    return class_weights\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, probs, targets):\n",
    "        bce_loss = 0\n",
    "        epsilon = 1e-7\n",
    "        for i, (pos_weight, neg_weight) in enumerate(self.class_weights):\n",
    "            prob = probs[:, i]\n",
    "            bce = -pos_weight * targets[:, i] * torch.log(prob + epsilon) - \\\n",
    "                  neg_weight * (1 - targets[:, i]) * torch.log(1 - prob + epsilon)\n",
    "            bce_loss += bce.mean()\n",
    "        return bce_loss / len(self.class_weights)\n",
    "\n",
    "class_weights_sub_nar = compute_class_weights(y_val_sub_nar)\n",
    "class_weights_nar = compute_class_weights(y_val_nar)\n",
    "narrative_criterion = WeightedBCELoss(class_weights_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc839d6b-f588-436a-b5f0-464093ce986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_criterion_dict = {}\n",
    "\n",
    "for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "    local_weights = [ class_weights_sub_nar[sub_i] for sub_i in sub_indices ]\n",
    "\n",
    "    sub_criterion = WeightedBCELoss(local_weights)\n",
    "    sub_criterion_dict[str(narr_idx)] = sub_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c44cc-cfb6-4c8f-80fe-df360cf0a659",
   "metadata": {},
   "source": [
    "We will also select the MultiHeadConcat Model, since this is the one appearing to do the best for our Fine-F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e92f161-370b-409e-bea0-4dc59a28d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassifierMultiHeadConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_narratives=len(mlb_narratives.classes_),\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        dropout_rate=network_params['dropout']\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.narrative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, num_narratives),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.subnarrative_heads = nn.ModuleDict()\n",
    "        for narr_idx, sub_indices in narrative_to_sub_map.items():\n",
    "            num_subs_for_this_narr = len(sub_indices)\n",
    "            self.subnarrative_heads[str(narr_idx)] = nn.Sequential(\n",
    "                nn.Linear(hidden_size * 2 + 1, num_subs_for_this_narr),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared_layer(x)\n",
    "\n",
    "        narr_probs = self.narrative_head(shared_out)\n",
    "\n",
    "        sub_probs_dict = {}\n",
    "        for narr_idx, head in self.subnarrative_heads.items():\n",
    "            conditioned_input = torch.cat((shared_out, narr_probs[:, int(narr_idx)].unsqueeze(1)), dim=1)\n",
    "            sub_probs_dict[narr_idx] = head(conditioned_input)\n",
    "\n",
    "        return narr_probs, sub_probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50b83196-b7fc-46ce-9e90-3cad23e1bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskClassifierMultiHeadConcat(\n",
    "    input_size=input_size,\n",
    "    hidden_size=2048\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615aed4d-898f-43e7-a100-e8e6198c9add",
   "metadata": {},
   "source": [
    "We add the extra penalty for english samples\n",
    "\n",
    "* In the forwarding step, we check if the sample is an english one, and if it is we apply the extra weight upon the loss to essentially tell them model to pay more attention to those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db85e674-51d2-4c47-9983-41410a48e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAwareMultiHeadLoss(nn.Module):\n",
    "    def __init__(self, narrative_criterion, sub_criterion_dict, \n",
    "                 condition_weight=0.3, sub_weight=0.3,\n",
    "                 english_weight=3.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.narrative_criterion = narrative_criterion\n",
    "        self.sub_criterion_dict = sub_criterion_dict\n",
    "        self.condition_weight = condition_weight\n",
    "        self.sub_weight = sub_weight\n",
    "        self.english_weight = english_weight\n",
    "\n",
    "    def forward(self, narr_probs, sub_probs_dict, y_narr, y_sub_heads, is_english):\n",
    "        is_english = is_english.to(narr_probs.device)\n",
    "        sample_weights = torch.where(is_english == 1, self.english_weight, 1.0)\n",
    "        \n",
    "        narr_loss = self.narrative_criterion(narr_probs, y_narr)\n",
    "        narr_loss = (narr_loss * sample_weights.unsqueeze(1)).mean()\n",
    "\n",
    "        sub_loss = 0.0\n",
    "        condition_loss = 0.0\n",
    "\n",
    "        for narr_idx_str, sub_probs in sub_probs_dict.items():\n",
    "            narr_idx = int(narr_idx_str)\n",
    "            y_sub = [row[narr_idx] for row in y_sub_heads]\n",
    "            y_sub_tensor = torch.tensor(y_sub, dtype=torch.float32, device=sub_probs.device)\n",
    "            sub_loss_func = self.sub_criterion_dict[narr_idx_str]\n",
    "            sub_batch_loss = sub_loss_func(sub_probs, y_sub_tensor)\n",
    "            sub_loss += (sub_batch_loss * sample_weights).mean()\n",
    "\n",
    "            narr_pred = narr_probs[:, narr_idx].unsqueeze(1)\n",
    "            condition_term = torch.abs(sub_probs * (1 - narr_pred)) + \\\n",
    "                             narr_pred * torch.abs(sub_probs - y_sub_tensor.unsqueeze(1))\n",
    "            condition_term = (condition_term * sample_weights.unsqueeze(1)).mean()\n",
    "            condition_loss += condition_term\n",
    "\n",
    "        sub_loss = sub_loss / len(sub_probs_dict)\n",
    "        condition_loss = condition_loss / len(sub_probs_dict)\n",
    "        total_loss = (1 - self.sub_weight) * narr_loss + self.sub_weight * sub_loss + \\\n",
    "                     self.condition_weight * condition_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226ec39-ae18-470a-a0b8-560edb58216f",
   "metadata": {},
   "source": [
    "We find the instances of our dataset that are English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "818232af-9ada-48e2-bffc-a8c0b170c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_english_train = torch.tensor([1 if lang == 'EN' else 0 for lang in dataset_train['language']], \n",
    "                              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcdeba57-1029-407e-8f6c-f8f245e8bb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_english_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdb1e2a9-7369-449b-9195-1c30ef431fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_aware_loss = LanguageAwareMultiHeadLoss(\n",
    "    narrative_criterion=narrative_criterion,\n",
    "    sub_criterion_dict=sub_criterion_dict,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f8b9c35-d86e-4a69-8bc1-616c9f9406dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_multihead(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn=language_aware_loss,\n",
    "    train_embeddings=train_embeddings_tensor,\n",
    "    y_train_nar=y_train_nar,\n",
    "    y_train_sub_heads=y_train_sub_heads,\n",
    "    val_embeddings=val_embeddings_tensor,\n",
    "    y_val_nar=y_val_nar,\n",
    "    y_val_sub_heads=y_val_sub_heads,\n",
    "    is_english_train=is_english_train, \n",
    "    patience=10,\n",
    "    num_epochs=100,\n",
    "    scheduler=None,\n",
    "    min_delta=0.001\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_narr_probs, train_sub_probs_dict = model(train_embeddings)\n",
    "\n",
    "        train_loss = loss_fn(\n",
    "            train_narr_probs, \n",
    "            train_sub_probs_dict, \n",
    "            y_train_nar, \n",
    "            y_train_sub_heads,\n",
    "            is_english_train\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_narr_probs, val_sub_probs_dict = model(val_embeddings)\n",
    "            is_english_val = torch.ones(len(val_embeddings), device=val_embeddings.device)\n",
    "            val_loss = loss_fn(\n",
    "                val_narr_probs, \n",
    "                val_sub_probs_dict, \n",
    "                y_val_nar, \n",
    "                y_val_sub_heads,\n",
    "                is_english_val\n",
    "            )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {train_loss.item():.4f}, \"\n",
    "              f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if val_loss.item() < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not significantly improve for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f52b84b-04bf-411b-9835-59e027f833e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def initialize_and_train_model(\n",
    "    model,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=10,\n",
    "    use_scheduler=True,\n",
    "    scheduler_patience=3,\n",
    "    loss_fn=language_aware_loss,\n",
    "    num_subnarratives=len(mlb_subnarratives.classes_),\n",
    "    device=device\n",
    "):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=scheduler_patience)\n",
    "\n",
    "    trained_model = train_with_multihead(\n",
    "                                    model=model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    patience=patience\n",
    "                                ).to(device)\n",
    "    return trained_model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87eff7a5-59f7-4d8e-9d82-5adae872b486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.1729, Validation Loss: 2.3293\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 2/100, Training Loss: 0.7802, Validation Loss: 2.2946\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 3/100, Training Loss: 0.6632, Validation Loss: 2.2670\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 4/100, Training Loss: 0.6027, Validation Loss: 2.2429\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 5/100, Training Loss: 0.5603, Validation Loss: 2.2185\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 6/100, Training Loss: 0.5223, Validation Loss: 2.1925\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 7/100, Training Loss: 0.4911, Validation Loss: 2.1644\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 8/100, Training Loss: 0.4688, Validation Loss: 2.1351\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 9/100, Training Loss: 0.4497, Validation Loss: 2.1059\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 10/100, Training Loss: 0.4376, Validation Loss: 2.0767\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 11/100, Training Loss: 0.4234, Validation Loss: 2.0476\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 12/100, Training Loss: 0.4092, Validation Loss: 2.0180\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 13/100, Training Loss: 0.3974, Validation Loss: 1.9884\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 14/100, Training Loss: 0.3862, Validation Loss: 1.9593\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 15/100, Training Loss: 0.3773, Validation Loss: 1.9297\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 16/100, Training Loss: 0.3690, Validation Loss: 1.8999\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 17/100, Training Loss: 0.3599, Validation Loss: 1.8708\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 18/100, Training Loss: 0.3511, Validation Loss: 1.8437\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 19/100, Training Loss: 0.3434, Validation Loss: 1.8183\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 20/100, Training Loss: 0.3351, Validation Loss: 1.7942\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 21/100, Training Loss: 0.3289, Validation Loss: 1.7706\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 22/100, Training Loss: 0.3219, Validation Loss: 1.7476\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 23/100, Training Loss: 0.3138, Validation Loss: 1.7262\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 24/100, Training Loss: 0.3070, Validation Loss: 1.7070\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 25/100, Training Loss: 0.2997, Validation Loss: 1.6903\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 26/100, Training Loss: 0.2934, Validation Loss: 1.6761\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 27/100, Training Loss: 0.2872, Validation Loss: 1.6633\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 28/100, Training Loss: 0.2809, Validation Loss: 1.6526\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 29/100, Training Loss: 0.2758, Validation Loss: 1.6436\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 30/100, Training Loss: 0.2702, Validation Loss: 1.6357\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 31/100, Training Loss: 0.2644, Validation Loss: 1.6301\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 32/100, Training Loss: 0.2589, Validation Loss: 1.6249\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 33/100, Training Loss: 0.2537, Validation Loss: 1.6210\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 34/100, Training Loss: 0.2481, Validation Loss: 1.6166\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 35/100, Training Loss: 0.2428, Validation Loss: 1.6145\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 36/100, Training Loss: 0.2383, Validation Loss: 1.6145\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 37/100, Training Loss: 0.2329, Validation Loss: 1.6158\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 38/100, Training Loss: 0.2280, Validation Loss: 1.6138\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 39/100, Training Loss: 0.2236, Validation Loss: 1.6125\n",
      "Current Learning Rate: 0.001000\n",
      "Epoch 40/100, Training Loss: 0.2185, Validation Loss: 1.6217\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 1 epoch(s).\n",
      "Epoch 41/100, Training Loss: 0.2145, Validation Loss: 1.6492\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 2 epoch(s).\n",
      "Epoch 42/100, Training Loss: 0.2106, Validation Loss: 1.6851\n",
      "Current Learning Rate: 0.001000\n",
      "Validation loss did not significantly improve for 3 epoch(s).\n",
      "Epoch 43/100, Training Loss: 0.2065, Validation Loss: 1.7126\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 4 epoch(s).\n",
      "Epoch 44/100, Training Loss: 0.2011, Validation Loss: 1.7191\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 5 epoch(s).\n",
      "Epoch 45/100, Training Loss: 0.2000, Validation Loss: 1.7245\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 6 epoch(s).\n",
      "Epoch 46/100, Training Loss: 0.1972, Validation Loss: 1.7324\n",
      "Current Learning Rate: 0.000500\n",
      "Validation loss did not significantly improve for 7 epoch(s).\n",
      "Epoch 47/100, Training Loss: 0.1955, Validation Loss: 1.7449\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 8 epoch(s).\n",
      "Epoch 48/100, Training Loss: 0.1921, Validation Loss: 1.7541\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 9 epoch(s).\n",
      "Epoch 49/100, Training Loss: 0.1914, Validation Loss: 1.7630\n",
      "Current Learning Rate: 0.000250\n",
      "Validation loss did not significantly improve for 10 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "trained_model, _, _ = initialize_and_train_model(\n",
    "    model,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "550fa66e-d415-461d-9377-df45a0cbdf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears',\n",
       " 'CC: Climate change is beneficial',\n",
       " 'CC: Controversy about green technologies',\n",
       " 'CC: Criticism of climate movement',\n",
       " 'CC: Criticism of climate policies',\n",
       " 'CC: Criticism of institutions and authorities',\n",
       " 'CC: Downplaying climate change',\n",
       " 'CC: Green policies are geopolitical instruments',\n",
       " 'CC: Hidden plots by secret schemes of powerful groups',\n",
       " 'CC: Questioning the measurements and science',\n",
       " 'Other',\n",
       " 'URW: Amplifying war-related fears',\n",
       " 'URW: Blaming the war on others rather than the invader',\n",
       " 'URW: Discrediting Ukraine',\n",
       " 'URW: Discrediting the West, Diplomacy',\n",
       " 'URW: Distrust towards Media',\n",
       " 'URW: Hidden plots by secret schemes of powerful groups',\n",
       " 'URW: Negative Consequences for the West',\n",
       " 'URW: Overpraising the West',\n",
       " 'URW: Praise of Russia',\n",
       " 'URW: Russia is the Victim',\n",
       " 'URW: Speculating war outcomes']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "435caf0b-cbbe-4130-8782-28a408d3c9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC: Amplifying Climate Fears: Amplifying existing fears of global warming',\n",
       " 'CC: Amplifying Climate Fears: Doomsday scenarios for humans',\n",
       " 'CC: Amplifying Climate Fears: Earth will be uninhabitable soon',\n",
       " 'CC: Amplifying Climate Fears: Other',\n",
       " 'CC: Amplifying Climate Fears: Whatever we do it is already too late',\n",
       " 'CC: Climate change is beneficial: CO2 is beneficial',\n",
       " 'CC: Climate change is beneficial: Other',\n",
       " 'CC: Climate change is beneficial: Temperature increase is beneficial',\n",
       " 'CC: Controversy about green technologies: Other',\n",
       " 'CC: Controversy about green technologies: Renewable energy is costly',\n",
       " 'CC: Controversy about green technologies: Renewable energy is dangerous',\n",
       " 'CC: Controversy about green technologies: Renewable energy is unreliable',\n",
       " 'CC: Criticism of climate movement: Ad hominem attacks on key activists',\n",
       " 'CC: Criticism of climate movement: Climate movement is alarmist',\n",
       " 'CC: Criticism of climate movement: Climate movement is corrupt']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "087519a2-98bb-425b-b836-8b093854684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "class MultiHeadEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes_coarse=coarse_classes,\n",
    "        classes_fine=fine_classes,\n",
    "        narrative_to_sub_map=narrative_to_sub_map,\n",
    "        narrative_order=narrative_order,\n",
    "        narrative_classes=mlb_narratives.classes_,\n",
    "        subnarrative_classes=mlb_subnarratives.classes_,\n",
    "        device='cpu',\n",
    "        output_dir='../../../submissions',\n",
    "    ):\n",
    "        self.narrative_to_sub_map = narrative_to_sub_map\n",
    "        self.narrative_order = narrative_order\n",
    "        self.narrative_classes = list(narrative_classes)\n",
    "        self.subnarrative_classes = list(subnarrative_classes)\n",
    "        \n",
    "        self.classes_coarse = classes_coarse\n",
    "        self.classes_fine = classes_fine\n",
    "\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        embeddings=val_embeddings_tensor,\n",
    "        dataset=dataset_val,\n",
    "        thresholds=None,\n",
    "        save=False,\n",
    "        std_weight=0.4,\n",
    "        lower_thres=0.1,\n",
    "        upper_thres=0.60\n",
    "    ):\n",
    "        if thresholds is None:\n",
    "            thresholds = np.arange(lower_thres, upper_thres, 0.05)    \n",
    "        embeddings = embeddings.to(self.device)\n",
    "    \n",
    "        best_results = {\n",
    "            'best_coarse_f1': -1,\n",
    "            'best_coarse_std': float('inf'),\n",
    "            'best_fine_f1': -1,\n",
    "            'best_fine_std': float('inf'),\n",
    "            'narr_threshold': 0,\n",
    "            'sub_threshold': 0,\n",
    "            'predictions': None,\n",
    "            'best_combined_score': -float('inf'),\n",
    "            'coarse_classification_report': None,\n",
    "            'fine_precision': None,\n",
    "            'fine_recall': None,\n",
    "            'samples_f1_fine': None,\n",
    "        }\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            narr_probs, sub_probs_dict = model(embeddings)\n",
    "            narr_probs = narr_probs.cpu().numpy()\n",
    "            sub_probs_dict = {k: v.cpu().numpy() for k, v in sub_probs_dict.items()}\n",
    "    \n",
    "        for narr_threshold in thresholds:\n",
    "            for sub_threshold in thresholds:\n",
    "                predictions = []\n",
    "                for sample_idx, row in dataset.iterrows():\n",
    "                    pred = self._make_prediction(\n",
    "                        row['article_id'],\n",
    "                        sample_idx,\n",
    "                        narr_probs,\n",
    "                        sub_probs_dict,\n",
    "                        narr_threshold,\n",
    "                        sub_threshold\n",
    "                    )\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, samples_f1_fine = self._compute_metrics_coarse_fine(predictions, dataset)\n",
    "                \n",
    "                combined_score = f1_fine_mean - (std_weight * coarse_std)\n",
    "                \n",
    "                if combined_score > best_results['best_combined_score']:\n",
    "                    best_results.update({\n",
    "                        'best_coarse_f1': f1_coarse_mean,\n",
    "                        'best_coarse_std': coarse_std,\n",
    "                        'best_fine_f1': f1_fine_mean,\n",
    "                        'best_fine_std': fine_std,\n",
    "                        'narr_threshold': narr_threshold,\n",
    "                        'sub_threshold': sub_threshold,\n",
    "                        'predictions': predictions,\n",
    "                        'best_combined_score': combined_score,\n",
    "                        'coarse_classification_report': report_coarse,\n",
    "                        'fine_precision': precision_fine,\n",
    "                        'fine_recall': recall_fine,\n",
    "                        'samples_f1_fine': samples_f1_fine,\n",
    "                    })\n",
    "    \n",
    "        print(\"\\nBest thresholds found:\")\n",
    "        print(f\"Narrative threshold: {best_results['narr_threshold']:.2f}\")\n",
    "        print(f\"Subnarrative threshold: {best_results['sub_threshold']:.2f}\")\n",
    "        print('\\nCompetition Values')\n",
    "        print(f\"Coarse-F1: {best_results['best_coarse_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. coarse: {best_results['best_coarse_std']:.3f}\")\n",
    "        print(f\"Fine-F1: {best_results['best_fine_f1']:.3f}\")\n",
    "        print(f\"F1 st. dev. fine: {best_results['best_fine_std']:.3f}\")\n",
    "        print(\"\\nCoarse Classification Report:\")\n",
    "        print(best_results['coarse_classification_report'])\n",
    "        print(\"\\nFine Metrics:\")\n",
    "        print(\"Precision: {:.3f}\".format(best_results['fine_precision']))\n",
    "        print(\"Recall: {:.3f}\".format(best_results['fine_recall']))\n",
    "        print(\"F1 Samples: {:.3f}\".format(best_results['samples_f1_fine']))\n",
    "\n",
    "        if save:\n",
    "            self._save_predictions(best_results, os.path.join(self.output_dir, 'submission.txt'))\n",
    "        \n",
    "        return best_results\n",
    "\n",
    "    def _make_prediction(self, article_id, sample_idx, narr_probs, sub_probs_dict, narr_threshold, sub_threshold):\n",
    "        other_idx = self.narrative_classes.index(\"Other\")\n",
    "        active_narratives = [\n",
    "            (n_idx, prob)\n",
    "            for n_idx, prob in enumerate(narr_probs[sample_idx])\n",
    "            if n_idx != other_idx and prob >= narr_threshold\n",
    "        ]\n",
    "        # Fallback, If no active narrartive, output \"Other\" for both\n",
    "        # narrative and subnarratives.\n",
    "        if not active_narratives:\n",
    "            return {\n",
    "                'article_id': article_id,\n",
    "                'narratives': [\"Other\"],\n",
    "                'pairs': [\"Other\"]\n",
    "            }\n",
    "        \n",
    "        narratives = []\n",
    "        pairs = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        active_narratives.sort(key=lambda x: x[1], reverse=True)\n",
    "        for narr_idx, _ in active_narratives:\n",
    "            narr_name = self.narrative_classes[narr_idx]\n",
    "            \n",
    "            sub_probs = sub_probs_dict[str(narr_idx)][sample_idx]\n",
    "            # FInd active subnarratives based on the cur threshold\n",
    "            active_subnarratives = [\n",
    "                (local_idx, s_prob)\n",
    "                for local_idx, s_prob in enumerate(sub_probs)\n",
    "                if s_prob >= sub_threshold\n",
    "            ]\n",
    "            # If no active subnarrative, output the predicted Narrative, with Other\n",
    "            # as a pair.\n",
    "            active_subnarratives.sort(key=lambda x: x[1], reverse=True)\n",
    "            if not active_subnarratives:\n",
    "                pairs.append(f\"{narr_name}: Other\")\n",
    "            else:\n",
    "                for local_idx, _ in active_subnarratives:\n",
    "                    global_sub_idx = self.narrative_to_sub_map[narr_idx][local_idx]\n",
    "                    sub_name = self.subnarrative_classes[global_sub_idx]\n",
    "                    pair = f\"{narr_name}: {sub_name}\"\n",
    "                    if pair not in seen_pairs:\n",
    "                        pairs.append(pair)\n",
    "                        seen_pairs.add(pair)\n",
    "            narratives.append(narr_name)\n",
    "        \n",
    "        return {\n",
    "            'article_id': article_id,\n",
    "            'narratives': narratives,\n",
    "            'pairs': pairs\n",
    "        }\n",
    "\n",
    "    def _compute_metrics_coarse_fine(self, predictions, dataset):\n",
    "        \"\"\"\n",
    "        Evaluates the problem predictions with the gold.\n",
    "        Mimics the challenge evaluation function.\n",
    "        \"\"\"\n",
    "        gold_coarse_all = []\n",
    "        gold_fine_all = []\n",
    "        pred_coarse_all = []\n",
    "        pred_fine_all = []\n",
    "\n",
    "        for pred, (_, row) in zip(predictions, dataset.iterrows()):\n",
    "            gold_coarse = row['narratives']\n",
    "            gold_subnarratives = row['subnarratives']\n",
    "            \n",
    "            pred_coarse = pred['narratives']\n",
    "            pred_fine = []\n",
    "            for p in pred['pairs']:\n",
    "                if p == \"Other\":\n",
    "                    pred_fine.append(\"Other\")\n",
    "                else:\n",
    "                    pred_fine.append(p)\n",
    "\n",
    "            gold_fine = []\n",
    "            for gold_nar, gold_sub in zip(gold_coarse, gold_subnarratives):\n",
    "                if gold_nar == \"Other\":\n",
    "                    gold_fine.append(\"Other\")\n",
    "                else:\n",
    "                    gold_fine.append(f\"{gold_nar}: {gold_sub}\")\n",
    "            \n",
    "            gold_coarse_all.append(gold_coarse)\n",
    "            gold_fine_all.append(gold_fine)\n",
    "            pred_coarse_all.append(pred_coarse)\n",
    "            pred_fine_all.append(pred_fine)\n",
    "\n",
    "        f1_coarse_mean, coarse_std = self._evaluate_multi_label(gold_coarse_all, pred_coarse_all, self.classes_coarse)\n",
    "        f1_fine_mean, fine_std = self._evaluate_multi_label(gold_fine_all, pred_fine_all, self.classes_fine)\n",
    "        \n",
    "        gold_coarse_flat = []\n",
    "        pred_coarse_flat = []\n",
    "        for g_labels, p_labels in zip(gold_coarse_all, pred_coarse_all):\n",
    "            g_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    g_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            p_onehot = np.zeros(len(self.classes_coarse), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_coarse:\n",
    "                    p_onehot[self.classes_coarse.index(lab)] = 1\n",
    "            gold_coarse_flat.append(g_onehot)\n",
    "            pred_coarse_flat.append(p_onehot)\n",
    "        gold_coarse_flat = np.array(gold_coarse_flat)\n",
    "        pred_coarse_flat = np.array(pred_coarse_flat)\n",
    "        report_coarse = metrics.classification_report(\n",
    "            gold_coarse_flat, pred_coarse_flat, target_names=self.classes_coarse, zero_division=0\n",
    "        )\n",
    "        \n",
    "        gold_fine_flat = []\n",
    "        pred_fine_flat = []\n",
    "        for g_labels, p_labels in zip(gold_fine_all, pred_fine_all):\n",
    "            g_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    g_onehot[self.classes_fine.index(lab)] = 1\n",
    "            p_onehot = np.zeros(len(self.classes_fine), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in self.classes_fine:\n",
    "                    p_onehot[self.classes_fine.index(lab)] = 1\n",
    "            gold_fine_flat.append(g_onehot)\n",
    "            pred_fine_flat.append(p_onehot)\n",
    "        gold_fine_flat = np.array(gold_fine_flat)\n",
    "        pred_fine_flat = np.array(pred_fine_flat)\n",
    "        \n",
    "        precision_fine = metrics.precision_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        recall_fine = metrics.recall_score(gold_fine_flat, pred_fine_flat, average='macro', zero_division=0)\n",
    "        samples_f1_fine = metrics.f1_score(\n",
    "            gold_fine_flat, \n",
    "            pred_fine_flat, \n",
    "            average='samples',\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        return f1_coarse_mean, coarse_std, f1_fine_mean, fine_std, report_coarse, precision_fine, recall_fine, samples_f1_fine\n",
    "\n",
    "    def _evaluate_multi_label(self, gold, predicted, class_list):\n",
    "        \"\"\"\n",
    "        Evaluates the predicted, with the gold and returns the mean and std f1 scores.\n",
    "        Mimics the challenge evaluation function.\n",
    "        \"\"\"\n",
    "        f1_scores = []\n",
    "        for g_labels, p_labels in zip(gold, predicted):\n",
    "            g_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in g_labels:\n",
    "                if lab in class_list:\n",
    "                    g_onehot[class_list.index(lab)] = 1\n",
    "                    \n",
    "            p_onehot = np.zeros(len(class_list), dtype=int)\n",
    "            for lab in p_labels:\n",
    "                if lab in class_list:\n",
    "                    p_onehot[class_list.index(lab)] = 1\n",
    "\n",
    "            f1_doc = metrics.f1_score(g_onehot, p_onehot, zero_division=0)\n",
    "            f1_scores.append(f1_doc)\n",
    "        \n",
    "        return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "    def _save_predictions(self, best_results, filepath):\n",
    "        predictions = best_results['predictions']\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                line = (f\"{pred['article_id']}\\t\"\n",
    "                        f\"{';'.join(pred['narratives'])}\\t\"\n",
    "                        f\"{';'.join(pred['pairs'])}\\n\")\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebe71798-314a-40ce-8724-506db53bcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultiHeadEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d1c49c5-eaf8-47ec-874a-e056b1a22de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best thresholds found:\n",
      "Narrative threshold: 0.45\n",
      "Subnarrative threshold: 0.20\n",
      "\n",
      "Competition Values\n",
      "Coarse-F1: 0.459\n",
      "F1 st. dev. coarse: 0.374\n",
      "Fine-F1: 0.311\n",
      "F1 st. dev. fine: 0.312\n",
      "\n",
      "Coarse Classification Report:\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "                          CC: Amplifying Climate Fears       0.00      0.00      0.00         0\n",
      "                      CC: Climate change is beneficial       0.00      0.00      0.00         1\n",
      "              CC: Controversy about green technologies       0.33      0.50      0.40         2\n",
      "                     CC: Criticism of climate movement       0.60      0.75      0.67         8\n",
      "                     CC: Criticism of climate policies       0.20      0.67      0.31         3\n",
      "         CC: Criticism of institutions and authorities       0.41      0.88      0.56         8\n",
      "                        CC: Downplaying climate change       0.00      0.00      0.00         2\n",
      "       CC: Green policies are geopolitical instruments       0.00      0.00      0.00         3\n",
      " CC: Hidden plots by secret schemes of powerful groups       0.17      0.25      0.20         4\n",
      "          CC: Questioning the measurements and science       1.00      0.50      0.67         4\n",
      "                                                 Other       0.67      0.36      0.47        11\n",
      "                     URW: Amplifying war-related fears       0.50      1.00      0.67         3\n",
      "URW: Blaming the war on others rather than the invader       0.50      0.50      0.50         6\n",
      "                             URW: Discrediting Ukraine       0.55      0.86      0.67         7\n",
      "                 URW: Discrediting the West, Diplomacy       0.82      1.00      0.90         9\n",
      "                           URW: Distrust towards Media       0.00      0.00      0.00         4\n",
      "URW: Hidden plots by secret schemes of powerful groups       0.00      0.00      0.00         0\n",
      "               URW: Negative Consequences for the West       0.00      0.00      0.00         1\n",
      "                            URW: Overpraising the West       0.00      0.00      0.00         1\n",
      "                                 URW: Praise of Russia       0.29      1.00      0.44         2\n",
      "                             URW: Russia is the Victim       0.33      0.50      0.40         2\n",
      "                         URW: Speculating war outcomes       0.75      0.75      0.75         4\n",
      "\n",
      "                                             micro avg       0.47      0.59      0.52        85\n",
      "                                             macro avg       0.32      0.43      0.35        85\n",
      "                                          weighted avg       0.49      0.59      0.50        85\n",
      "                                           samples avg       0.46      0.52      0.46        85\n",
      "\n",
      "\n",
      "Fine Metrics:\n",
      "Precision: 0.123\n",
      "Recall: 0.345\n",
      "F1 Samples: 0.311\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.evaluate(\n",
    "    model=trained_model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
